[{"uri":"https://beforelights.github.io/AWS-Worklog/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: “AI-Driven Development Life Cycle: Reimagining Software Engineering” Event Objectives The event aimed to provide a deep understanding of the transformative impact of generative AI on software development. Key objectives included:\nIntroducing the AI-Driven Development Life Cycle (AI-DLC) framework and its foundational principles. Demonstrating the capabilities of Kiro and Amazon Q Developer as tools to support AI-driven development. Exploring how generative AI can enhance productivity and streamline the software engineering process. Speakers The session featured insights from two distinguished speakers:\nToan Huynh – Specialist SA, PACE My Nguyen – Senior Prototyping Architect, Amazon Web Services - ASEAN Key Highlights The event focused on the AI-DLC framework, which reimagines software development by positioning AI as a collaborative partner. Key concepts included:\nAI-DLC Core Concept: The framework emphasizes a human-centric approach where AI acts as a collaborator to augment developer capabilities. This results in accelerated development cycles, reducing timelines from weeks or months to mere hours or days.\nAI-DLC Workflow: The iterative workflow alternates between AI-driven tasks (e.g., creating plans, implementing solutions, seeking clarifications) and human-driven tasks (e.g., providing clarifications, validating solutions). AI only executes solutions after human validation, ensuring quality and alignment.\nAI-DLC Stages: The lifecycle is divided into three distinct stages:\nInception: Establishing context, elaborating user intent through user stories, and planning tasks using Units of Work. Construction: Domain modeling, code generation, testing, and deploying infrastructure as code (IaC) with automated tests. Operation: Managing production deployments and addressing incidents. Challenges Addressed by AI-DLC:\nScaling AI development for complex projects. Enhancing control and collaboration with AI agents. Maintaining code quality from proof-of-concept to production. Deep Dive: Kiro - The AI IDE for Prototype to Production The session included a detailed demonstration of Kiro, an AI-first Integrated Development Environment (IDE) designed to support the AI-DLC framework. Key features of Kiro include:\nSpec-Driven Development: Kiro transforms high-level prompts (e.g., \u0026ldquo;Build a Slack-like chat application\u0026rdquo;) into structured artifacts such as:\nrequirements.md for clear requirements. design.md for system design. tasks.md for discrete, actionable tasks. This structured approach ensures traceability and shifts development from intuition-based coding to a systematic process. Agentic Workflows: Kiro’s AI agents autonomously execute tasks while keeping developers in control. Key features include:\nImplementation Plan: Kiro generates detailed plans with tasks and sub-tasks linked to specific requirements for validation. Agent Hooks: These hooks trigger AI agents to perform tasks like generating documentation, writing unit tests, or optimizing code performance in the background. Key Takeaways AI-Driven Production Readiness: Kiro ensures production-ready code by generating detailed design documents (e.g., data flow diagrams, API contracts) and unit tests before implementation begins. Human Control via Artifacts: Developers maintain oversight by validating and refining artifacts (requirements, designs, task plans) before AI agents execute the implementation. Applying Insights to Work The event provided actionable insights for integrating AI-driven tools into my workflow:\nAdopting AI Coding Assistants: Tools like Amazon Q Developer can automate repetitive tasks, enabling me to focus on high-value activities. Prioritizing Human-Centric Tasks: By delegating routine tasks to AI, I can dedicate more time to creative and strategic activities such as domain modeling and architectural design. Event Experience Attending the AI-Driven Development Life Cycle: Reimagining Software Engineering event was an eye-opening experience. The session highlighted how generative AI is evolving from a coding assistant to a central orchestrator of the development process. The structured approach of AI-DLC, combined with tools like Kiro, demonstrated the potential to revolutionize software engineering.\nThe live demonstration of Kiro was particularly impactful, showcasing how a simple text prompt could be transformed into a comprehensive, executable development plan. This approach not only accelerates development but also ensures traceability and maintainability.\nLessons Learned The structured, human-validated approach of AI-DLC addresses critical challenges in AI-driven development, including scalability, control, and code quality. Generative AI, when integrated thoughtfully, can significantly enhance productivity while maintaining high standards of quality. Event Photos "},{"uri":"https://beforelights.github.io/AWS-Worklog/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Mai Quốc Anh\nPhone Number: 0931823911\nEmail: maiquocanh2608@gmail.com\nUniversity: FPT University HCM\nMajor: Software Engineering\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 12/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://beforelights.github.io/AWS-Worklog/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect with FCJ members and mentors. Find out what working in an office is like. Install Linux, learn how to properly use Linux. Learn the basics of AWS, console and CLI. Complete first and second module. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon - Reviewed and acknowledged the official internship rules and guidelines. - Created an AWS account based on the materials. (Viewed account ID, Updated account credentials, Created an alias) - Acknowledged the steps required to close my account - Understanding on MFA for accounts (Chose Virtual MFA for ease of use) - Understood and created Groups and Users - Read the Account authentication support (I didn\u0026rsquo;t need to do this part since everything went well) - Learnt about the basics of the AWS Management Console - Know about the support cases (types of support cases, how to create one well) - Know about the types of Budgets, the steps to create them and the benefits for each one of them. 08/09/2025 08/09/2025 Setting up an AWS account What is the AWS Management Console? Managing Costs with AWS Budgets Getting Help with AWS Support AWS Service Quotas Compare AWS Support Plans Tue - Installed Hugo and started setting up\n- Learnt the fundamentals about IAM\n+ Create and manage IAM Groups for user organization\n+ Apply IAM Policies for effective permissions\n+ Manage IAM Users through groups for streamlined control\n+ IAM Role for easy permission switching\n- Getting started on Amazon VPC (Virtual Private Cloud)\n+ Revise Subnets, getting to know about its relationship with AZs and best (Availability Zone) and the best practices + Learnt about Route table + Learnt about IGW (Internet Gateway) and NAT Gateway Learnt about Firewall fundamentals (Security Group, Network ACLs, VPC Resource Map) - Practice after learning the fundamentals + Created an EC2 instance, NAT Gateway, used Reachability Analyzer, SSM, CloudWatch 09/09/2025 09/09/2025 AWS Identity and Access Management (IAM) Access Control Amazon VPC and AWS Site-to-Site VPN Workshop Wed - Finished fixing up bugs for Hugo Started with Site to Site VPN + Created a VPC for the VPN, created an EC2 instance Created a Virtual Private Gateway to connect to the endpoints, created a Customer Gateway, Set up route tables and propagation for the VPN connection (EC2 in this case) + Configure the Customer Gateway (I chose Libreswan not knowing Strongswan is recommended and OpenSwan is no longer usable.) + Spend most of the time on debugging. 10/09/2025 10/09/2025 Amazon VPC and AWS Site-to-Site VPN Workshop Thu - Finished debugging the configuration of the Customer Gateway (some commands are not working and the ones that work are in 5.2.7, use systemd instead of service) - Finished the Amazon VPC and AWS Site-to-Site VPN Workshop as a whole - Starting the new Lab (Introduction to Amazon EC2) + Created a Linux VPC, Windows VPC, Security group Linux, Security group Windows + Created the Windows Instance and connected to it. + Created the Linux Instance and connected to it. + Learnt about the fundamentals of EC2, changing instance type, creating snapshots, etc\u0026hellip; + Created custom AMIs with and without sysprep, created instances from the AMIs. + For the How to connect when you lost the keypair part, I created an IAM Role with AmazonSSMFullAccess and updated the EC2 to have that policy, checked SSM but I don\u0026rsquo;t see the instance, spent some time searching and still seeing Managed: false, tested the connection, checked the SSM Agent logs to see if there are errors (no errors), restarted the SSM service inside of the AMI and it worked after that, it is still Managed: false for some reasons + Replace keypair via editing user data is successful for the Linux machine + Installed desktop environment for the Ubuntu AMI and it booted up successfully 11/09/2025 11/09/2025 Amazon VPC and AWS Site-to-Site VPN Workshop Introduction to Amazon EC2 Create an Amazon EC2 AMI using Windows Sysprep Working with SSM Agent on EC2 instances for Windows Server AWS Systems Manager Parameter Store Fri - Setup a LAMP web server + Downloaded Apache, PHP and MariaDB and ran the web server + The server ran as expected with the PHP file + Finished configuring the database + Installed phpMyAdmin, successfully logged into the phpMyAdmin page and created a new database after that - Installed Node.js on Linux using Node Version Manager (nvm) for ease of use, it allows us to switch between different versions - Deployed AWS FCJ Management on the EC2 Linux instance successfully - Installed XAMPP and Node.js on the Windows Instance, have them fully configured and deployed 12/09/2025 12/09/2025 Introduction to Amazon EC2 Week 1 Achievements: Successfully created and secured an AWS Free Tier account with MFA and budgets. Mastered IAM fundamentals: Groups, Users, Policies, and Roles. Built and secured a VPC infrastructure with Subnets, Route Tables, NAT Gateways, Security Groups, and NACLs. Established a functional Site-to-Site VPN connection (Libreswan) to simulate hybrid cloud networking. Performed advanced EC2 operations: Custom AMIs (Sysprep), Keypair recovery via User Data, and SSM Agent debugging. Deployed a LAMP stack and Node.js environment on both Linux and Windows EC2 instances. "},{"uri":"https://beforelights.github.io/AWS-Worklog/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Week 1: Getting familiar with AWS, Account Setup, and Core Services (EC2, VPC, IAM)\nWeek 2: Advanced EC2, IAM Policies, and Static Website Hosting with S3 \u0026amp; CloudFront\nWeek 3: VM Import/Export and High Availability Web App with Auto Scaling \u0026amp; ALB\nWeek 4: CloudWatch Dashboards, Cost Optimization with Lambda, and Database Migration\nWeek 5: Hybrid DNS with Route 53 Resolver, AWS Managed AD, and CLI Practice\nWeek 6: Project Scoping, Cost Estimation, and DevSecOps Research\nWeek 7: Amazon FSx for Windows File Server: Deployment, Performance, and Quotas\nWeek 8: Three.js Fundamentals and Midterm Exam Preparation\nWeek 9: Cost Analysis with Athena/CUR and Serverless Orchestration with Step Functions\nWeek 10: Advanced Step Functions Patterns and Multi-Channel Security Alerting\nWeek 11: Unified Alerting Lambda (Slack/Telegram/SES) and Incident Response Project Management\nWeek 12: S3 Security Dashboard Optimization, React Authentication, and WireGuard VPN\n"},{"uri":"https://beforelights.github.io/AWS-Worklog/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Xây dựng nền tảng phát sóng dữ liệu thị trường sàn giao dịch hiệu suất cao trên AWS bởi Abhishek Sarolia và Avanish Yadav vào ngày 24/09/2025 trong Amazon ElastiCache, Amazon RDS, AWS Direct Connect, AWS Transit Gateway, Best Practices, Customer Solutions, Database, Networking \u0026amp; Content Delivery, RDS for PostgreSQL | Permalink\nĐây là bài đăng hợp tác cùng Abhishek Chawla, Giám đốc Sản phẩm \u0026amp; Công nghệ; Kartik Manimuthu, Giám đốc Kỹ thuật Đám mây; và Digvijay, Giám đốc Kỹ thuật Ứng dụng tại SMC Global Securities Ltd.\nSMC Global Securities Ltd. (SMC), thành lập năm 1990, là công ty dịch vụ tài chính hàng đầu Ấn Độ cung cấp các dịch vụ giao dịch, tư vấn tài sản và phân phối sản phẩm tài chính cho các nhà đầu tư cá nhân, công ty và khách hàng giàu có.\nSMC Global Securities (SMC) đã hợp tác với Amazon Web Services (AWS) để hiện đại hóa hạ tầng giao dịch bán lẻ của họ. Là một phần của sáng kiến này, SMC đã xây dựng hệ thống phân phối dữ liệu tài chính vững chắc trên AWS xử lý hiệu quả luồng dữ liệu thị trường thời gian thực từ các sàn giao dịch lớn của Ấn Độ (NSE, BSE, MCX và NCDEX). Nền tảng này sử dụng công nghệ mạng Multicast để thu thập và phát sóng dữ liệu giá sàn với hiệu suất và khả năng mở rộng cao.\nNhiều khách hàng AWS vận hành hệ thống tài chính lõi trên đám mây thường xử lý dữ liệu unicast từ hạ tầng on-premises. Điều này thường liên quan đến chuyển đổi nguồn dữ liệu multicast thành unicast trước khi truyền tới AWS qua AWS Direct Connect. Tuy nhiên, việc thu nhận trực tiếp dữ liệu multicast ngoại vi trên AWS đã đưa ra thách thức đối với nhiều tổ chức. Bài viết này trình bày cách SMC tiếp cận đổi mới để vượt qua các trở ngại đó. Chúng tôi trình bày cách SMC dùng hỗ trợ multicast tích hợp của AWS Transit Gateway để liên lạc trong môi trường Amazon Virtual Private Cloud (Amazon VPC), kết hợp với Fortinet SD-WAN cho giải pháp overlay. Thiết lập này cho phép SMC thiết lập các đường hầm Generic Routing Encapsulation (GRE) qua Direct Connect, mang nguồn dữ liệu giá multicast từ hệ thống on-premises của họ trực tiếp vào AWS Cloud.\nAWS Transit Gateway là gì AWS Transit Gateway là một bộ trung chuyển mạng giúp kết nối các VPC và mạng on-premises theo kiểu hub-and-spoke. Nó hỗ trợ nhiều loại kết nối như VPC, thiết bị SD-WAN, cổng Direct Connect, kết nối VPN và các Transit Gateway khác. Điều này loại bỏ nhu cầu kết nối điểm-điểm phức tạp. Một chức năng quan trọng là hỗ trợ multicast native, tự động xử lý định tuyến multicast cho các instance gửi dữ liệu đến nhiều instance nhận thuộc cùng một nhóm multicast.\nThách thức SMC bắt đầu hành trình trên AWS với triết lý cốt lõi là mang lại “trải nghiệm giao dịch nhanh và liền mạch” cho người dùng cuối. Việc có một nền tảng phát sóng có hiệu suất cao là chìa khóa cho trải nghiệm đó. Vì nền tảng phát sóng nhận sự kiện từ sàn giao dịch chứng khoán và gửi đến các trader, nó là thành phần quan trọng của ứng dụng giao dịch bán lẻ hiện đại.\nSMC cần một hệ thống có khả năng cung cấp dữ liệu thị trường tài chính gần như thời gian thực cho trader với mức độ chịu mất gói bằng 0, bởi vì ngay cả một gói lỗi cũng có thể hiển thị thông tin thị trường then chốt sai (như đỉnh giá trong 52 tuần). Điều này có thể gây ra thua lỗ tài chính đáng kể cho trader và ảnh hưởng tới niềm tin của khách hàng. Hạ tầng phát sóng on-premises hiện tại của SMC gặp thách thức về khả năng mở rộng và không đáp ứng được yêu cầu hiệu suất của các ứng dụng giao dịch tài chính hiện đại, khiến downtime thường xuyên và giảm hiệu suất.\nTổng quan giải pháp Các tổ chức cần kết nối mạng hybrid để liên kết liền mạch các trung tâm dữ liệu on-premises, địa điểm từ xa và hạ tầng cloud. AWS cung cấp các tùy chọn kết nối cho các kiến trúc hybrid như Direct Connect và Site-to-Site VPN, để kết nối tài nguyên on-premises lên AWS an toàn.\nSMC hoạt động trong lĩnh vực tài chính có quy định khắt khe, nên khi bắt đầu thiết kế nền tảng broadcast tài chính lai trên AWS Cloud, họ xác định bốn mục tiêu chính:\nKhả năng mở rộng Độ tin cậy Hiệu suất Bảo mật Để đạt các mục tiêu này ở quy mô lớn, SMC sử dụng AWS Control Tower để thiết lập workloads trên AWS. AWS Control Tower cung cấp cách rõ ràng và hiệu quả để này và quản trị môi trường AWS đa tài khoản an toàn. Nó tạo ra landing zone dựa trên best-practices và quản trị thông qua các controls từ danh mục có sẵn. Landing zone là nền tảng đa tài khoản theo chuẩn well-architected, tuân theo best-practices của AWS.\nCác cân nhắc thiết kế Đáp ứng yêu cầu từ các sàn truyền thống phải kết thúc kết nối mạng dữ liệu giá tại địa điểm on-premises, SMC đánh giá hai phương án kiến trúc:\nKết thúc nguồn dữ liệu multicast tại trung tâm dữ liệu, chuyển đổi thành unicast và chuyển giải pháp phát sóng hiện tại lên AWS. Cách này nhanh hơn nhưng mang theo các hạn chế giải pháp cũ lên cloud. Kết thúc nguồn dữ liệu multicast tại trung tâm dữ liệu và gửi trực tiếp nguồn multicast lên AWS. Cách này cho phép thiết kế lại hệ broadcast bằng các dịch vụ AWS native. Sau khi cân nhắc, SMC quyết định kiến trúc lại hoàn toàn hệ thống trên AWS. Quyết định chiến lược này dựa vào hai mục tiêu:\nXây dựng kiến trúc sẵn sàng cho tương lai Tối đa hóa hiệu quả với các dịch vụ AWS native Để đạt các mục tiêu, nền tảng tuân thủ một số nguyên tắc cốt lõi:\nƯu tiên tính mới của sự kiện hơn sự đầy đủ\nVì hiệu suất và tính thực tiễn, hệ thống sẽ loại bỏ các sự kiện cũ. Ví dụ, giá giao dịch cuối cùng (LTP) của mã chứng khoán XYZ là 100, 101, 102 lần lượt tại T1, T2, T3, thì hệ thống có thể loại bỏ giá 100 và 101 để hiển thị trực tiếp giá 102. Người dùng chủ yếu quan tâm đến giá hiện tại hơn các giá cũ khi kiểm tra LTP.\nHình 1 – Giá giao dịch cuối cùng\nTuy nhiên, giá trị lịch sử rất là quan trọng cho việc phân tích như đỉnh/thấp trong 12 tuần, 52 tuần, hoặc xem danh sách tổng hợp lệnh.\nHình 2 – Danh sách giá từ cao xuống thấp\nVì vậy, giải pháp cần một chế độ thời gian thực cho dữ liệu giá mới nhất và cơ chế đối chiếu dữ liệu đã loại bỏ/lỗi qua path khác. Để đáp ứng, hệ thống ghi nhận và đối chiếu thời gian thực bằng microservices độc lập và có khả năng mở rộng trên cụm Amazon Elastic Container Service (Amazon ECS), sử dụng dịch vụ AWS native để xử lý:\nAmazon ElastiCache để cache và phản chiếu packet dữ liệu ghi nhận mới nhất, truy xuất nhanh Amazon RDS PostgreSQL cho lưu trữ dài hạn và đối chiếu batch Hình 3 – Ghi lại và đối chiếu giá\nXử lý đa luồng cho throughput cao\nDo khối lượng dữ liệu thị trường lớn, hệ thống xử lý đa luồng trên tất cả các phần. Để hạn chế mất packet do giới hạn băng thông mạng, thiết lập nhiều kết nối TCP ở các port khác nhau trên mỗi instance để truyền dữ liệu song song.\nMột thách thức là đảm bảo thứ tự xử lý đúng. Nếu XYZ được update hai lần một giây—M1 (giá lên 100) và M2 (giá lên 101) —mà M2 xử lý trước M1 sẽ gây lỗi lớn. Để xử lý, hệ thống thêm thành phần thời gian tùy biến (custom chronological component) trước khi dữ liệu trả về giao diện người dùng.\nHình 4 – Sắp xếp dữ liệu theo thứ tự thời gian\nQuản lý kết nối WebSocket ở quy mô lớn\nCông nghệ WebSocket cho phép giao tiếp hai chiều, thời gian thực giữa máy khách và máy chủ, làm cho nó trở nên lý tưởng để phát sóng dữ liệu thị trường đến giao diện web và di động. Không giống như các yêu cầu HTTP truyền thống, các kết nối WebSocket:\nDuy trì kết nối liên tục, giảm latency Giảm tải mạng khi loại bỏ handshake lặp lại Cho phép cập nhật đẩy từ máy chủ (server-push), điều này rất quan trọng đối với việc phân phối dữ liệu thị trường theo thời gian thực Giảm tải máy chủ so với các phương pháp dựa trên thăm dò (polling-based) SMC đã triển khai các kết nối WebSocket chạy trên các cụm ECS để sử dụng những lợi ích này trên các giao diện ứng dụng web và di động của họ. Tuy nhiên, việc vận hành các hệ thống dựa trên WebSocket ở quy mô lớn đặt ra những thách thức:\n1. Thách thức về quản lý kết nối\nKhi thiết lập, các client WebSocket duy trì kết nối với các instance server cụ thể. Điều này làm phức tạp quá trình cân bằng tải truyền thống, đặc biệt khi scaling ECS container vì các kết nối hiện tại không dễ chia lại sang các instance mới do trạng thái của chúng.\n2. Thách thức về “bầy đàn” (thundering herd) của WebSocket\nTrong giờ thị trường, hàng nghìn nhà giao dịch đồng thời cố gắng thiết lập các kết nối WebSocket, đặc biệt là vào thời điểm mở cửa thị trường hoặc sau khi bảo trì hệ thống. Hiệu ứng \u0026ldquo;thundering herd\u0026rdquo; này có thể làm quá tải máy chủ và dẫn đến lỗi kết nối. Hơn nữa, khi các container được thay thế trong các sự kiện mở rộng quy mô, nhiều máy khách cố gắng kết nối lại đồng thời có thể tạo ra căng thẳng tương tự trên hệ thống.\nĐể giải quyết những thách thức này, một chiến lược quản lý kết nối toàn diện đã được triển khai. Điều này bao gồm mở rộng quy mô dựa trên kết nối thông qua các báo động Amazon CloudWatch tùy chỉnh và kết nối lại máy khách theo cấp số nhân (staggered client reconnections) bằng cách sử dụng exponential backoff and jitter. Để có khả năng mở rộng dài hạn, nhóm cũng đánh giá các cách tiếp cận thay thế như quản lý trạng thái bên ngoài bằng cách sử dụng ElastiCache để lưu trữ trạng thái kết nối cho các chuyển đổi máy khách liền mạch giữa các container và mở rộng quy mô dần với các khoảng thời gian được kiểm soát để ngăn chặn sự gia tăng đột biến của kết nối.\nĐộ ổn định sản xuất \u0026amp; quan sát\nĐể đảm bảo độ tin cậy của hệ thống phát sóng, SMC đã triển khai một công cụ tùy chỉnh sử dụng LGTM stack để giám sát và quan sát toàn diện, kết hợp bốn công cụ chính: Loki cho nhật ký, Grafana để trực quan hóa, Tempo để theo dõi và Mimir for metrics. cho các số liệu. Điều này được tích hợp với một hệ thống thông báo và quản lý sự cố tập trung bằng cách sử dụng AWS Systems Manager Incident Manager để nhanh chóng giảm thiểu và phục hồi từ các sự cố ảnh hưởng đến các ứng dụng được lưu trữ trên AWS.\nHình 5 – Trình quản lý sự cố tích hợp\nKiến trúc giải pháp Kiến trúc tham chiếu cấp cao sau đây cho thấy cách SMC triển khai nền tảng phát sóng của họ. AWS Transit Gateway đóng một vai trò quan trọng trong kiến trúc này bằng cách đóng vai trò là bộ định tuyến đám mây giúp hợp lý hóa cấu trúc liên kết mạng, đồng thời cho phép định tuyến multicast trên nhiều VPC. Nó tập trung hóa khả năng kết nối giữa các VPC và mạng on-premises, đồng thời hỗ trợ các kết nối băng thông cao, độ trễ thấp.\nViệc sử dụng AWS Transit Gateway đã cho phép SMC mở rộng mạng của họ khi họ thêm các VPC hoặc địa điểm on-premises mới, thực hiện các chính sách mạng nhất quán trên tất cả các mạng được đính kèm và giảm số lượng điểm kết nối cần thiết để kết nối nhiều VPC và mạng on-premises.\nSMC đã sử dụng Fortinet Firewall để thu nạp dữ liệu multicast qua các đường hầm GRE trong Amazon VPC. Họ đã tích hợp FortiGate SD-WAN Hub của họ với AWS Transit Gateway để thiết lập kết nối an toàn, hiệu suất cao giữa môi trường on-premises và AWS, cho phép phân phối hiệu quả các nguồn cấp dữ liệu thị trường multicast trên nhiều VPC, hợp lý hóa việc quản lý các kiến trúc mạng phức tạp và giảm chi phí vận hành trong việc quản lý mạng đa VPC.\nCách định cấu hình FortiGate SD-WAN nằm ngoài phạm vi thảo luận của bài đăng này, nhưng có thể kiểm tra chi tiết đầy đủ tại đây.\nHình 6 – Kiến trúc hệ thống phát sóng\nSolution Outcome Hệ thống mới được thiết kế của SMC trong AWS hiện có thể xử lý hàng chục nghìn người dùng đồng thời. Nó có thể xử lý hơn 100K tin nhắn mỗi giây từ nhiều sàn giao dịch chứng khoán với thời gian ngừng hoạt động bằng không. AWS cho phép SMC mở rộng quy mô nền tảng của họ để đáp ứng nhu cầu kinh doanh gia tăng trong giờ cao điểm, dẫn đến hiệu suất tốt hơn và độ trễ thấp hơn (cải thiện khoảng 60%) và thu nhỏ quy mô liền mạch sau giờ làm việc để tiết kiệm chi phí.\nHình 7 – Các chỉ số độ trễ hệ thống\nKết luận Hành trình của SMC từ nền tảng phát sóng on-premises sang nền tảng cloud-native là một ví dụ điển hình về cách các tổ chức tài chính có thể vượt qua các giới hạn cơ sở hạ tầng truyền thống thông qua việc áp dụng AWS một cách chiến lược. Sự chuyển đổi đòi hỏi những quyết định kiến trúc táo bạo—chọn tái kiến trúc thay vì di chuyển—và chứng minh rằng với việc lập kế hoạch cẩn thận, các dịch vụ AWS phù hợp và cam kết hiện đại hóa, các công ty dịch vụ tài chính có thể xây dựng cơ sở hạ tầng đáp ứng các yêu cầu khắt khe ngày nay đồng thời mở rộng quy mô cho các cơ hội ngày mai.\nTại SMC Global Securities (và Stoxkart), hành trình chuyển đổi kỹ thuật số của chúng tôi với AWS đã tăng cường đáng kể tính ổn định, hiệu suất và sự linh hoạt trong hoạt động của nền tảng. Việc di chuyển từ on-premises lên đám mây đã loại bỏ nhu cầu mở rộng quy mô, vá lỗi và nâng cấp cơ sở hạ tầng thủ công—giải phóng các nhóm của chúng tôi để tập trung vào đổi mới. Với khả năng kiểm soát, khả năng hiển thị được cải thiện và khả năng tự động mở rộng quy mô trong giờ giao dịch cao điểm, chúng tôi đã xây dựng một hệ sinh thái môi giới kiên cường, sẵn sàng cho tương lai, mang lại trải nghiệm liền mạch cho khách hàng của chúng tôi.\n– Abhishek Chawla (SMC , CTO)\nVề các tác giả﻿ Abhishek Sarolia Abhishek Sarolia là Kiến trúc sư Giải pháp cấp cao tại AWS. Anh làm việc với người dùng AWS để giải quyết các thách thức kinh doanh của họ bằng cách thiết kế các giải pháp an toàn, hiệu suất cao và khả năng mở rộng sử dụng các công nghệ điện toán đám mây mới nhất. Anh có niềm đam mê với công nghệ và thích xây dựng, thử nghiệm các dự án về AI/ML và IoT. Ngoài công việc, anh yêu thích du lịch, đọc sách phi hư cấu và dành thời gian cho gia đình.\nAbhishek Chawla (Khách mời) Abhishek Chawla là Giám đốc Sản phẩm và Công nghệ Tập đoàn (CPTO) tại SMC Global Securities, nơi anh lãnh đạo chiến lược công nghệ và sản phẩm cho nhiều đơn vị kinh doanh như môi giới dịch vụ đầy đủ (SMC), môi giới môi giới chiết khấu (Stoxkart), quản lý tài sản và phân phối. Với gần hai thập kỷ kinh nghiệm trong các lĩnh vực fintech, edtech, chăm sóc sức khỏe, và du lịch, Abhishek đã thúc đẩy những chuyển đổi kỹ thuật số quy mô lớn bằng cách hiện đại hóa hạ tầng, xây dựng nền tảng cloud-native và mở rộng các nhóm kỹ thuật hiệu suất cao. Anh làm việc chặt chẽ với các CEO để đảm bảo các sáng kiến công nghệ phù hợp với mục tiêu kinh doanh, tạo ra tác động đo lường được thông qua đổi mới, hiệu quả và lấy khách hàng làm trọng tâm.\nKartik Manimuthu (Khách mời) Kartik Manimuthu là Giám đốc Kỹ thuật Đám mây tại SMC Global Securities. Với niềm đam mê giải quyết các bài toán kinh doanh phức tạp bằng công nghệ, anh dẫn dắt nhóm kỹ thuật thiết kế các giải pháp sáng tạo, vững chắc giúp thúc đẩy chuyển đổi số của công ty. Kartik chuyên về chuyển đổi lên cloud, hiện đại hóa hạ tầng doanh nghiệp và xây dựng nền tảng có thể mở rộng phù hợp với mục tiêu kinh doanh. Ngoài văn phòng, anh thích khám phá công nghệ mới và đọc sách.\nDigvijay (Khách mời) Digvijay là Giám đốc Kỹ thuật tại SMC, một công ty môi giới hàng đầu, nơi anh đã dẫn dắt chiến lược công nghệ và đổi mới sản phẩm trong hai năm qua. Với hơn 10 năm kinh nghiệm trong ngành phần mềm, Digvijay có chuyên môn sâu về xây dựng hệ thống có khả năng mở rộng và lãnh đạo các nhóm kỹ thuật hiệu suất cao. Trước khi gia nhập SMC, Digvijay từng giữ các vị trí kỹ thuật chủ chốt tại Microsoft và Expedia, đóng góp vào các nền tảng lớn phục vụ khách hàng. Quản lý của anh kết hợp nền tảng kỹ thuật vững chắc với hiểu biết sâu về ngành dịch vụ tài chính.\nAvanish Yadav Avanish Yadav là Kiến trúc sư Giải pháp Mạng cấp cao tại AWS. Với niềm đam mê về công nghệ mạng, anh thích sáng tạo và hỗ trợ người dùng giải quyết các vấn đề kỹ thuật phức tạp bằng cách xây dựng các kiến trúc điện toán đám mây an toàn, khả năng mở rộng. Khi không cộng tác với người dùng để cung cấp giải pháp chuyên sâu, anh thường chơi cricket bên ngoài công việc. LinkedIn: /avanish-yadav-93b8a947/.\n"},{"uri":"https://beforelights.github.io/AWS-Worklog/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Optimize security operations with AWS Security Incident Response bởi Kyle Shields và Matt Meck vào ngày 23/09/2005 trong Best Practices, Customer Solutions, Security, Identity, \u0026amp; Compliance, Technical How-to | Permalink | Comments\nCác mối đe dọa bảo mật yêu cầu hành động nhanh chóng, đó là lý do AWS Security Incident Response cung cấp bảo vệ gốc AWS có thể ngay lập tức củng cố tư thế bảo mật của bạn. Giải pháp toàn diện này kết hợp logic tự động phân loại và đánh giá với metadata chu vi bảo mật của bạn để xác định các vấn đề quan trọng, liền mạch bổ sung chuyên môn con người khi cần. Khi Security Incident Response được tích hợp với Amazon GuardDuty và AWS Security Hub trong một môi trường bảo mật hợp nhất, tổ chức có quyền truy cập 24/7 vào AWS Customer Incident Response Team (CIRT) để phát hiện nhanh chóng, phân tích chuyên môn, và ngăn chặn đe dọa hiệu quả—quản lý qua một bảng điều khiển trực quan. Security Incident Response bao gồm trong Amazon Managed Services (AMS), giúp tổ chức áp dụng và vận hành AWS ở quy mô lớn một cách hiệu quả và an toàn.\nTrong bài này, chúng tôi hướng dẫn bạn kích hoạt Security Incident Response và thực hiện bằng chứng khái niệm (POC) để nhanh chóng nâng cao khả năng bảo mật khi nhận lợi ích ngay lập tức. Chúng tôi khám phá chức năng dịch vụ, thiết lập tiêu chí thành công POC, xác định cấu hình, chuẩn bị triển khai, kích hoạt dịch vụ và tối ưu hiệu quả từ ngày đầu tiên, giúp tổ chức bạn xây dựng niềm tin trong suốt vòng đời phản ứng sự cố đồng thời cải thiện thời gian phục hồi.\nHiểu chức năng của Security Incident Response Dịch vụ AWS Security Incident Response cung cấp khả năng phát hiện và phản ứng mối đe dọa toàn diện thông qua quy trình bốn bước hợp lý. Nó bắt đầu bằng việc lấy các phát hiện bảo mật từ GuardDuty và các tích hợp Security Hub được chọn với công cụ bên thứ ba. Dịch vụ tự động phân loại các phát hiện này bằng metadata khách hàng và tình báo đe dọa để xác định hành vi bất thường và hoạt động nghi ngờ. Khi phát hiện mối đe dọa tiềm ẩn, thành viên CIRT chủ động điều tra các trường hợp qua portal khách hàng để xác định đó là tích cực thật hay giả. Với các mối đe dọa được xác nhận, dịch vụ leo thang các phát hiện để hành động ngay lập tức, trong khi với tích cực giả sẽ cập nhật hệ thống phân loại tự động và quy tắc triệt tiêu cho GuardDuty và Security Hub, liên tục cải thiện độ chính xác phát hiện.\nBảo vệ toàn diện với ít điều kiện tiên quyết Security Incident Response mang lại năng lực bảo mật mạnh mẽ nhờ tích hợp liền mạch với cả hệ thống phát hiện và phản ứng sự cố AWS (TDIR) và dịch vụ bảo mật bên thứ ba như CrowdStrike, Lacework, TrendMicro. Giải pháp này cung cấp trung tâm chỉ huy thống nhất để quản lý sự cố đầu-cuối—từ lập kế hoạch, giao tiếp đến giải quyết—khi lấy các phát hiện GuardDuty và tích hợp với nhà cung cấp ngoài qua Security Hub. Với quản lý case an toàn cùng timeline hoạt động bất biến, nó nâng cao đáng kể hoạt động bảo mật khi tăng cường các đội SOC và phản ứng sự cố (IR) bằng khả năng hiển thị tốt hơn cũng như quyền truy cập vào các công cụ và nhân sự đã kiểm chứng từ AWS. CIRT AWS cộng tác với đội ứng phó của bạn trong truy vết và phục hồi, giải phóng nguồn lực có giá trị cho các ưu tiên khác.\nDịch vụ cung cấp giá trị liên tục qua khả năng giám sát và phản ứng chủ động. Nó liên tục giám sát môi trường của bạn bằng các phát hiện từ GuardDuty và Security Hub, với tự động hóa dịch vụ, phân loại và phân tích làm việc âm thầm để cảnh báo bạn chỉ trong trường hợp có mối bận tâm bảo mật thực sự. Bảo vệ này mang lại giá trị tức thì lúc xảy ra sự cố tiềm năng mà không đòi hỏi bạn phải chú ý liên tục.\nBắt đầu rất đơn giản—điều kiện duy nhất là bật AWS Organizations và bảo đảm rằng bạn đã thiết lập Organizations với cấu trúc đơn vị tổ chức (OU) cơ bản gồm các tài khoản thành viên. Nền tảng này vừa cho phép triển khai Security Incident Response, vừa là nền tảng vững chắc cho chiến lược TDIR mạnh ở toàn tổ chức.\nXác định tiêu chí thành công Thiết lập tiêu chí thành công giúp so sánh kết quả POC với mục tiêu kinh doanh. Một số ví dụ:\nChỉ định đội phản ứng sự cố: Xác định và ghi chép thành viên đội nội bộ cũng như nguồn lực ngoài chịu trách nhiệm phản ứng sự cố. Như nhấn mạnh trong AWS Well-Architected Security Pillar, có nhân sự chỉ định sẽ giảm thời gian phân loại và phản ứng khi sự cố xảy ra. Phát triển framework phản ứng sự cố chính thức: Xây dựng kế hoạch phản ứng sự cố toàn diện với playbook chi tiết cùng quy trình tập luyện tabletop thường xuyên. AWS cung cấp thư viện playbook tham khảo trên GitHub. Chạy bài tập tabletop: Xem xét thực hiện mô phỏng thường xuyên để kiểm tra kế hoạch phản ứng sự cố, phát hiện lỗ hổng, xây dựng “bản năng hành động” cho đội trước khi khủng hoảng thực sự xảy ra. AWS cung cấp ngữ cảnh về các loại bài tập tabletop. Xác định các nhà cung cấp bảo mật bên thứ ba hiện có: Xác định nhà cung cấp tích hợp Security Hub đẩy dữ liệu vào Security Incident Response. Đối tác AWS cung cấp phát hiện như tài liệu ở Detect and Analyze. Triển khai GuardDuty: Cấu hình GuardDuty theo thực hành tốt nhất để giám sát—phát hiện đe dọa trên dịch vụ trọng yếu. AWS duy trì thực hành tốt nhất GuardDuty tại AWS Security Services Best Practices for GuardDuty. Kiểm tra tiêu chí thành công của bạn để đảm bảo mục tiêu thực tế với thời gian cũng như ràng buộc đặc thù tổ chức. Ví dụ: Bạn có kiểm soát hoàn toàn cấu hình dịch vụ AWS không? Có nguồn lực có thể dành thời gian thực thi và kiểm tra không? Thời điểm này có thuận tiện để các bên liên quan đánh giá dịch vụ không?\nĐịnh nghĩa cấu hình Security Incident Response Sau khi xác định tiêu chí và thời gian, bạn nên định nghĩa cấu hình Security Incident Response. Một số quyết định quan trọng gồm:\nChọn tài khoản quản trị viên được ủy quyền: Xác định tài khoản làm quản trị viên được ủy quyền (DA). Tài khoản này và AWS Region đã chọn sẽ lưu trữ dịch vụ và portal Security Incident Response. AWS Security Reference Architecture (SRA) đề nghị dùng tài khoản công cụ bảo mật chuyên biệt. Xem các lưu ý nên cân nhắc trước khi chốt DA. Định nghĩa phạm vi tài khoản: Security Incident Response là dịch vụ cấp tổ chức. Mỗi tài khoản ở mọi Region trong tổ chức đều nằm trong bảo hiểm dưới một đăng ký duy nhất. Bảo hiểm tự động điều chỉnh khi thêm/xóa tài khoản, bảo vệ toàn diện AWS footprint. Cấu hình nguồn phát hiện: Xác định các phát hiện đáp ứng nhu cầu tổ chức. Dịch vụ mặc định lấy phát hiện GuardDuty toàn tổ chức và một số loại phát hiện Security Hub từ đối tác bên thứ ba. Đánh giá kế hoạch bảo vệ GuardDuty và phát hiện Security Hub hữu ích nhất cho tư thế bảo mật, khả năng phản ứng sự cố của bạn. Xây dựng framework leo thang: Thiết lập ngưỡng leo thang rõ ràng cho từng loại trường hợp: tự quản lý, có hỗ trợ AWS, chủ động. Ai có quyền xác định gửi loại case nào dựa trên mức độ nghiêm trọng, tác động, yêu cầu nguồn lực. Triển khai chiến lược phân tích: Xác định dùng công cụ phân tích AWS gốc như Amazon Athena, Amazon OpenSearch, và Amazon Detective), hoặc tích hợp SIEM (security information and event management) hiện có. Chức năng này làm giàu phản ứng sự cố với dữ liệu bối cảnh, hiểu biết sâu hơn. Chuẩn bị triển khai Sau khi xác định tiêu chí, cấu hình, bạn xác định các bên liên quan, trạng thái và thời gian mong muốn. Chuẩn bị triển khai bằng cách:\nKế hoạch dự án, timeline: Xây dựng kế hoạch, tiêu chí thành công, phạm vi, mốc chính, thời gian triển khai thực tế. Đề xuất dòng thời gian: Trước khi bật: Cấu hình GuardDuty, các bên thứ ba Security Hub, lên kế hoạch tài nguyên Xin phê duyệt thử nghiệm POC từ đội tài khoản AWS hay đội Dịch vụ Ngày 0 – Bật dịch vụ Tuần 1 – Mở các case CIRT phản ứng Tuần 2 – Kết nối công cụ ITSM Tuần 3 – Tập luyện tabletop Tuần 4 – Rà soát báo cáo CIRT cung cấp Xác định các bên liên quan: CISO, nhóm bảo mật thông tin, SOC, đội phản ứng sự cố, kỹ sư bảo mật, tài chính, pháp lý, tuân thủ, MSSP ngoài, đại diện business unit. Phát triển ma trận RACI: Vẽ biểu đồ RACI xác định vai trò, trách nhiệm, tạo thuận lợi cho trách nhiệm giải trình, kênh giao tiếp phù hợp. Cấu hình quyền truy cập tài khoản quản lý: Đảm bảo ủy quyền để phân quyền quản trị. Xem thêm Permissions required to designate a delegated Security Incident Response administrator account. Thiết lập vai trò IAM/quyền: Dùng AWS Identity and Access Management (IAM) để áp dụng kiểm soát truy cập dựa trên vai trò (kết nối RACI chart), gồm quản lý case, leo thang, vai trò chỉ đọc sử dụng chính sách quản lý AWS. Chi tiết ở AWS Managed Policies Kích hoạt Security Incident Response Khi chuẩn bị xong, bạn đã sẵn sàng bật dịch vụ.\nTruy cập Security Incident Response trong tài khoản quản lý:\nVào tài khoản quản lý tổ chức, truy cập AWS Management Console, tìm Security Incident Response trên thanh tìm kiếm. Chọn Sign up. Đảm bảo đã chọn Use delegated administrator account (khuyên dùng), nhập số tài khoản quản trị vào trường Account ID, chọn Next. Đăng nhập vào tài khoản quản trị được ủy quyền đã cấu hình ở bước 3, tìm Security Incident Response rồi chọn Sign up. Hoàn thành thiết lập ở tài khoản quản trị viên được ủy quyền:\nĐịnh nghĩa chi tiết thành viên: Chọn vùng nhà của bạn dưới Region selection. Đối với Membership name, nhập một tên phù hợp tuân theo tiêu chuẩn đặt tên của tổ chức bạn. Dưới Membership contacts, nhập thông tin Primary và Secondary contact. Thêm Membership tags theo chiến lược gắn thẻ của tổ chức bạn. Chọn Next. Cấu hình quyền cho phản ứng chủ động: Service permissions for proactive response đã được kích hoạt, nhưng bạn có thể vô hiệu hóa tính năng này nếu cần. Chọn By choosing this option… và chọn Next. Xem xét quyền dịch vụ và chọn Next. Xem xét cấu hình thành viên và chi tiết, sau đó chọn Sign up. Vai trò liên kết dịch vụ được tạo với phản ứng chủ động không thể được tạo trong tài khoản quản lý thông qua quy trình lên tàu này. Xem AWS Security Incident Response User Guide để triển khai vai trò liên kết dịch vụ đến tài khoản quản lý. Xem hướng dẫn chi tiết ở video hướng dẫn thiết lập trên YouTube..\nNhiều tổ chức có các quy trình và bộ ứng dụng được thiết lập tốt cho IR và quản lý mối đe dọa bảo mật. Để phù hợp với các thiết lập có sẵn này, AWS đã phát triển các tích hợp với các ứng dụng ITSM và quản lý trường hợp phổ biến. Các bản phát hành ban đầu của chúng tôi cho phép tích hợp hai chiều hoàn toàn với cả Jira và ServiceNow, với nhiều hơn nữa đang trên đường.\nChúng tôi đã cung cấp hướng dẫn đầy đủ quy trình thiết lập tại GitHub.\nTối ưu hóa giá trị từ ngày đầu tiên Ngay sau khi kích hoạt dịch vụ, Security Incident Response bắt đầu thu thập các phát hiện GuardDuty và Security Hub (từ các đối tác bảo mật) của bạn. Các phát hiện của bạn được tự động phân loại và giám sát bằng cách sử dụng logic đánh giá xác định; dựa trên metadata độc đáo của tổ chức bạn và chu vi bảo mật, các mối đe dọa có mức độ ưu tiên cao được leo thang đến trung tâm chỉ huy Security Incident Response của bạn để điều tra ngay lập tức. Trong khi tổ chức của bạn nhận được bảo hiểm 24/7 từ đầu, việc thực hiện các tối ưu hóa được khuyến nghị này sẽ tăng cường đáng kể độ chính xác phát hiện mối đe dọa, giảm tích cực giả, tăng tốc thời gian phản ứng và tăng cường tư thế bảo mật tổng thể của bạn thông qua bảo vệ tùy chỉnh được căn chỉnh với rủi ro kinh doanh cụ thể và yêu cầu tuân thủ của bạn.\nĐể tối đa hóa giá trị ngay lập tức từ Security Incident Response, chúng tôi đề xuất sử dụng khả năng phản ứng của nó bắt đầu từ ngày một. Khi đội của bạn gặp phải hoạt động đáng ngờ hoặc yêu cầu điều tra chuyên môn, bạn có thể tạo một trường hợp được AWS hỗ trợ thông qua cổng thông tin dịch vụ để tương tác trực tiếp với các chuyên gia AWS CIRT. Các chuyên gia bảo mật này một cách hiệu quả mở rộng khả năng của đội bạn, cung cấp kiến thức chuyên môn và hướng dẫn để giúp bạn nhanh chóng hiểu, ngăn chặn và khắc phục các mối quan tâm bảo mật tiềm ẩn. Quyền truy cập theo yêu cầu này vào AWS CIRT có thể giảm thời gian giải quyết trung bình của bạn, giảm thiểu tác động tiềm ẩn và đảm bảo bạn có hỗ trợ chuyên nghiệp ngay cả đối với các kịch bản bảo mật phức tạp có thể khác làm choáng ngợp tài nguyên nội bộ.\nVí dụ truy vấn hỗ trợ phản ứng:\nPhát hiện IP đáng ngờ trong môi trường, thực hiện các API call đa dạng. Bạn hỗ trợ điều tra được không? Tài khoản mới tạo 2 ngày trước, được thông báo qua quy tắc Amazon EventBridge và tích hợp endpoint detection response (EDR), bạn giúp xác định phạm vi, tìm ai tạo? Tạo bằng cách nào? Người dùng AWS Identity and Access Management (IAM) thực hiện API call xuyên Region, tạo resource ở Region không dùng. Giải pháp EDR phát hiện hành vi bất thường trên website production, cho thấy vi phạm tiềm tàng. EDR phát hiện tải web-shell nghi ngờ và hoạt động – cần giúp điều tra, cô lập. Người dùng trái phép thực hiện API ngoài quyền hạn—hỗ trợ phát hiện leo thang quyền. Cần giúp phân tích log bảo mật từ AWS WAF và Amazon Elastic Compute Cloud (Amazon EC2) instances. Có chỉ số compromise hoặc mẫu lạ không? Các bước tiếp theo Nếu bạn quyết định tiến hành với AWS Security Incident Response và triển khai một POC, chúng tôi khuyến nghị các mục hành động sau:\nXác định xem bạn có phê duyệt và ngân sách để sử dụng Security Incident Response không. Các thỏa thuận giá ưu đãi, giảm giá và thử nghiệm dựa trên hiệu suất có sẵn. Cấu hình và triển khai GuardDuty để giúp duy trì bảo hiểm toàn diện và có liên quan trên các tài khoản quản lý và thành viên, dịch vụ quan trọng và workload của bạn. Xác minh rằng các công cụ bảo mật bên thứ ba (như CrowdStrike, Lacework hoặc Trend Micro) được tích hợp đúng cách với Security Hub. Giao tiếp các thay đổi công cụ phản ứng sự cố bảo mật với các đội tổ chức có liên quan. Kết luận Trong bài viết này, chúng tôi đã chỉ cho bạn cách lập kế hoạch và thực hiện một POC AWS Security Incident Response. Bạn đã học cách làm như vậy thông qua các giai đoạn, bao gồm xác định tiêu chí thành công, cấu hình Security Incident Response và xác thực rằng Security Incident Response đáp ứng nhu cầu kinh doanh của bạn.\nLà một khách hàng, hướng dẫn này sẽ giúp bạn chạy một POC thành công với Security Incident Response. Nó hướng dẫn bạn đánh giá giá trị và các yếu tố để xem xét khi quyết định thực hiện các tính năng hiện tại.\nTài nguyên bổ sung Security Incident Response – Getting Started Guide Configuring security tool integrations through Security Hub Managing Security Incident Response events with Amazon EventBridge Amazon GuardDuty best practices AWS Security Hub best practices AWS Security Incident Response Technical Guide (best practices) AWS Managed Services Offering AWS Security Incident Response Blog: The customer’s journey to accelerating the incident Response lifecycle Nếu có phản hồi về bài này, gửi bình luận ở Comments. Nếu có câu hỏi về bài này, hãy liên hệ AWS Support.\nKyle Shields\nKyle là Kiến trúc sư Giải pháp Chuyên gia Bảo mật tập trung phát hiện mối đe dọa và phản ứng sự cố tại AWS. Hiện nay, anh giúp khách hàng doanh nghiệp AWS vận hành Security Incident Response, cải thiện tư thế bảo mật.\nMatt Meck\nMatt là chuyên gia bảo mật WW với 10 năm kinh nghiệm trong công nghệ AI, an ninh mạng, 3 năm tại AWS về Lĩnh vực Detection \u0026amp; Response. Đặt ở NY, yêu thích thiên nhiên, anh dành thời gian chơi bóng đá, trượt tuyết, chinh phục các đỉnh núi mới.\n"},{"uri":"https://beforelights.github.io/AWS-Worklog/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Giải phóng giá trị dữ liệu phi cấu trúc với Amazon Bedrock Data Automation bởi Vani Eswarappa và Rupesh Mishra vào ngày 29/09/2025 trong Amazon Bedrock Knowledge Bases, Amazon Comprehend, Amazon DataZone, Amazon EventBridge, Amazon OpenSearch Service, Amazon Rekognition, Amazon Simple Storage Service (S3), Amazon Textract, AWS Glue, AWS Lambda, AWS Marketplace, Healthcare, Industries | Permalink\nCác tổ chức trong mọi ngành đang gặp khó khăn với sự gia tăng dữ liệu phi cấu trúc—hình ảnh, tài liệu văn bản, PDF, tệp âm thanh \u0026amp; video, và các định dạng chuyên biệt như chuỗi gen. Khác với dữ liệu có cấu trúc, dữ liệu phi cấu trúc thường thiếu tổ chức chuẩn hóa, khiến việc phát hiện, truy cập, và khai phá ý nghĩa từ nguồn dữ liệu giá trị này trở nên khó khăn.a\nTại Amazon Web Services (AWS), chúng tôi ghi nhận những khó khăn tương tự ở các viện nghiên cứu và các tổ chức y tế khi tìm giải pháp lưu trữ, tìm kiếm và khai thác kho dữ liệu phi cấu trúc của mình.\nChúng tôi sẽ khám phá cách giải pháp sử dụng Amazon Bedrock Data Automation, một tính năng của Amazon Bedrock, có thể mang lại:\nGiải pháp tổng thể để quản lý dữ liệu phi cấu trúc Tự động hóa quá trình lưu trữ dữ liệu Cải thiện chất lượng dữ liệu Triển khai chính sách quản trị Truy cập nhanh các sản phẩm dữ liệu giá trị Chúng tôi cũng làm rõ cách AWS và giải pháp đối tác giúp xử lý khử định danh (de-identification) cho dữ liệu phi cấu trúc, bằng cách làm mờ hoặc xóa thông tin nhận diện khỏi văn bản, PDF, hình ảnh và tài liệu khác. Và vì khử định danh đôi khi cần thiết cho phát triển AI, nghiên cứu, chính sách công và các ứng dụng khác.\nThách thức quản lý dữ liệu phi cấu trúc Khách hàng các ngành chia sẻ những thách thức cơ bản liên quan đến tổ chức dữ liệu, tích hợp và khả năng truy cập. Chúng tôi đã nhận diện nhiều khó khăn phổ biến mà tổ chức phải đối mặt với dữ liệu phi cấu trúc:\nLưu trữ phân mảnh: Dữ liệu bị phân tán trên các storage bucket, workspace cục bộ, hệ thống file và các giải pháp lưu trữ khác nhau. Khả năng phát hiện hạn chế: Nếu thiếu metadata phù hợp, việc trích xuất và catalog hóa dữ liệu phi cấu trúc giá trị sẽ làm các tài sản bị ẩn và không được khai thác hết tiềm năng. Tính phức tạp khi tích hợp: Khó kết nối thông tin liên quan giữa các loại dữ liệu đa phương thức. Vấn đề về quản trị: Khi khối lượng dữ liệu tăng, việc đảm bảo lineage, kiểm soát truy cập và governance ngày càng khó khăn. Khử định danh dữ liệu: Việc cân bằng nhu cầu insight từ dữ liệu với yêu cầu nghiêm ngặt về riêng tư \u0026amp; tuân thủ là thách thức lớn ở nhiều ngành, đặc biệt khi cần độ chính xác đáp ứng quy định pháp lý. Amazon Bedrock Data Automation cho catalog hóa dữ liệu phi cấu trúc Amazon Bedrock Data Automation là một tính năng mạnh mẽ của Amazon Bedrock, dùng để tự động phân loại, tìm kiếm, và trích xuất insight từ dữ liệu đa phương thức phi cấu trúc như tài liệu, hình ảnh, audio và video bằng AI. Hãy xem nó đáp ứng những vấn đề then chốt như thế nào.\nBedrock Data Automation giúp chuẩn hóa quá trình trích xuất insight giá trị từ các loại dữ liệu phi cấu trúc như hình ảnh y tế, báo cáo hồ sơ bệnh án (PDF, hình ảnh), tệp audio \u0026amp; video. Dịch vụ này trích xuất metadata kỹ thuật và kinh doanh qua blueprint tùy chỉnh, giúp dữ liệu phi cấu trúc được phát hiện dễ dàng nhờ khả năng tìm kiếm toàn diện. Đối với các tổ chức nghiên cứu/y tế, điều này nghĩa là hình ảnh, bản scan và dữ liệu nghiên cứu có thể catalog hóa tự động và tìm kiếm dễ dàng mà không cần gán tag thủ công.\nKhi tích hợp với các dịch vụ AWS khác (như AWS Glue và Amazon SageMaker Unified Studio), Bedrock Data Automation tạo điều kiện xây dựng giải pháp phân tích tổng thể qua cái nhìn tích hợp cho các tài sản dữ liệu vốn trước đây bị silo hóa.\nFigure 1 – Giải pháp kiến trúc lưu trữ dữ liệu phi cấu trúc cấp cao\nGiải pháp catalog hóa dữ liệu phi cấu trúc: Cách hoạt động Giải pháp sử dụng kiến trúc nhiều tầng. Dưới đây là explanation flow trình bày ở Figure 1.\n1 – Quá trình nạp dữ liệu và khử định danh ban đầu\nDữ liệu từ các nguồn phi cấu trúc, đa phương thức sẽ được chuyển sang Amazon Simple Storage Service (Amazon S3), đóng vai trò điểm ingest dữ liệu chính. AWS cung cấp một danh mục dịch vụ chuyển dữ liệu dùng để truyền dữ liệu từ nhiều nguồn doanh nghiệp phi cấu trúc đến Amazon S3 và các dịch vụ AWS khác.\nĐối với các tổ chức cần dữ liệu đã khử định danh cho các trường hợp sử dụng khác nhau, có thể sử dụng kiến trúc event-driven kết hợp Amazon EventBridge và AWS Lambda. Họ có thể gọi các dịch vụ khử định danh AWS và các giải pháp trên AWS Marketplace để khử định danh dữ liệu.\nTổ chức cũng có thể tự phát triển giải pháp khử định danh trên AWS bằng cách kết hợp nhiều dịch vụ AWS. Có nhiều lựa chọn hỗ trợ như Amazon Textract, Amazon Comprehend, AWS Glue, Amazon Rekognition, Amazon Bedrock và các dịch vụ khác. Dữ liệu đã khử định danh sẽ được lưu vào Amazon S3.\nAWS Marketplace cung cấp một catalog số hóa các giải pháp bên thứ ba đã được xác minh cho các tình huống khử định danh ở nhiều ngành khác nhau. Ví dụ, với ngành healthcare, AWS Marketplace có các giải pháp đối tác cho clinical text deidentification, PDF deidentification và DICOM image and metadata deidentification\u0026hellip; Các giải pháp này có thể triển khai cho các transform jobs theo thời gian thực hoặc batch với các lựa chọn khác nhau trên AWS.\n2 – Data processing pipeline\nAmazon S3 kích hoạt các events qua Amazon EventBridge, đây là pha xử lý tiếp theo. Amazon EventBridge quản lý workflow event-driven và quá trình tự động hóa. EventBridge sẽ gọi Lambda functions để xử lý dữ liệu đã khử định danh và tiếp tục gọi Bedrock Data Automation để xử lý tài liệu, hình ảnh, file audio và video phi cấu trúc.\nBedrock Data Automation cung cấp năng lực AI qua foundation models cùng các tính năng tự động hóa dữ liệu. Nó trích xuất metadata mặc định và metadata tuỳ chỉnh bằng blueprint do khách hàng cung cấp. Dữ liệu xử lý sẽ lưu vào một S3 bucket. Ngoài ra, Bedrock Data Automation có thể tiếp tục kích hoạt EventBridge cho các xử lý downstream với dữ liệu và metadata đã trích xuất.\n3 – Knowledge management\nDữ liệu đã xử lý từ Amazon S3 sẽ chuyển vào Amazon Bedrock Knowledge Bases. Ứng dụng Generative AI và chatbot cung cấp giao diện tương tác ngôn ngữ tự nhiên và phản hồi tự động, giúp dữ liệu trở nên dễ truy cập và hành động cho người dùng cuối.\n4 – Centralized data storage and distribution layer\nChúng tôi sử dụng nhiều tầng lưu trữ để phục vụ các nhu cầu khác nhau của khách hàng. Khi triển khai ứng dụng Generative AI và chatbot, chúng tôi lưu dữ liệu xử lý từ Amazon S3 vào Amazon Bedrock Knowledge Bases. Để xây dựng centralized metadata store, AWS Glue ETL jobs sẽ xử lý dữ liệu incoming từ Bedrock Data Automation, trích xuất metadata và lưu vào một AWS Glue Catalog.\nAmazon DataZone nhập metadata từ AWS Glue Catalog và cung cấp một portal hợp nhất cho người dùng khám phá, chia sẻ, quản trị dữ liệu dù lưu trữ ở đâu. Nó dùng data catalog để cung cấp repository trung tâm cho metadata và ngữ cảnh vận hành của tài sản dữ liệu. Người dùng có thể khám phá, hiểu tài sản từ nhiều nguồn dữ liệu khác nhau. Ngoài ra, chúng tôi sử dụng Amazon DynamoDB làm storage layer cho metadata, tích hợp với Amazon OpenSearch Service.\n5 – Analytics layer\nKhi các tài sản đã nằm trong catalog DataZone, người dùng có thể truy vấn, phân tích dữ liệu đó bằng Amazon Athena hoặc Amazon Redshift Query Editor. Điều này cho phép thực hiện analytics phức tạp quy mô lớn và toàn diện về hoạt động business intelligence. Việc tích hợp Amazon OpenSearch Service với DynamoDB cung cấp năng lực tìm kiếm mạnh mẽ và khả năng phân tích gần real-time.\nBảo mật và tuân thủ Kiến trúc giải pháp này đảm bảo dữ liệu PHI (Protected Health Information) được khử định danh đúng chuẩn trước khi xử lý tiếp. Sử dụng lưu trữ riêng cho dữ liệu gốc và dữ liệu đã khử định danh. Amazon DataZone quản lý governance, kiểm soát truy cập và phân loại để xác minh quản lý dữ liệu và tuân thủ hiệu quả.\nCác bước tiếp theo Hãy chuyển đổi hành trình quản trị dữ liệu của bạn với giải pháp mà chúng tôi vừa trình bày sử dụng tính năng Amazon Bedrock Data Automation.\nChúng tôi khuyến nghị trước khi bắt đầu:\nLập bản đồ landscape dữ liệu phi cấu trúc Định nghĩa rõ tiêu chí thành công cho nhu cầu của tổ chức bạn Bắt đầu pilot tập trung với tài sản dữ liệu giá trị nhất để chứng minh giá trị ngay Triển khai rộng toàn tổ chức dựa trên thành công ban đầu Truy cập GitHub Repository: Guidance for Multimodal Data Processing để bắt đầu ngay hôm nay.\nKết luận Khi dữ liệu phi cấu trúc tiếp tục tăng trưởng mạnh mẽ trong mọi ngành, các tổ chức triển khai giải pháp catalog hóa và quản lý hiệu quả sẽ sở hữu lợi thế cạnh tranh lớn. Tính năng Amazon Bedrock Data Automation là một phương pháp mạnh mẽ, dễ mở rộng, có thể biến vấn đề quản lý dữ liệu phi cấu trúc thành tài sản chiến lược khi triển khai.\nDù bạn là tổ chức nghiên cứu hay healthcare, Bedrock Data Automation của chúng tôi giúp bạn phân loại, tìm kiếm và khai thác insight từ dữ liệu phi cấu trúc phức tạp. Giải pháp đảm bảo governance chuẩn và kiểm soát truy cập đúng, đồng thời unlock toàn bộ tiềm năng tài sản dữ liệu.\nLiên hệ AWS Representative để biết cách chúng tôi có thể giúp tăng tốc doanh nghiệp của bạn.\nĐọc thêm Amazon Bedrock Data Automation: Amazon Bedrock Data Automation to transform unstructured data into actionable insights AWS Glue Documentation Amazon DataZone User Guide Vani Eswarappa Vani Eswarappa là Principal Architect tại AWS, có kinh nghiệm về Containers, AI/ML, Enterprise Architecture. Là một technical leader, Vani làm việc với khách hàng AWS về hành trình cloud nhằm đáp ứng nhu cầu kinh doanh. Khi không làm việc, cô thích dành thời gian bên gia đình và khám phá địa điểm mới ngoài trời.\nRupesh Mishra Rupesh Mishra là Health AI/ML Specialist Solutions Architect tại AWS, với hơn 15 năm kinh nghiệm lâm sàng và kỹ thuật trong ngành healthcare và công nghệ. Anh nỗ lực nâng cao kết quả điều trị thông qua công nghệ và cộng tác cùng khách hàng, đối tác AWS các sáng kiến AI, ML, generative AI. Thời gian rảnh anh thích chơi thể thao, ở bên gia đình, du lịch và khám phá món ăn.\n"},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.11-appendices/5.11.1-cloudtrail-etl/","title":"CloudTrail ETL Code","tags":[],"description":"","content":" import json import boto3 import gzip import re import os from datetime import datetime, timezone s3 = boto3.client(\u0026#34;s3\u0026#34;) firehose= boto3.client(\u0026#34;firehose\u0026#34;) # -------------------------------------------------- # CONFIG # -------------------------------------------------- SOURCE_PREFIX = \u0026#34;exportedlogs/vpc-dns-logs/\u0026#34; FIREHOSE_STREAM_NAME = os.environ.get(\u0026#34;FIREHOSE_STREAM_NAME\u0026#34;) VPC_RE = re.compile(r\u0026#34;/(vpc-[0-9A-Za-z\\-]+)\u0026#34;) ISO_TS_RE = re.compile(r\u0026#34;^\\d{4}-\\d{2}-\\d{2}T\u0026#34;) def read_gz(bucket, key): obj = s3.get_object(Bucket=bucket, Key=key) with gzip.GzipFile(fileobj=obj[\u0026#34;Body\u0026#34;]) as f: return f.read().decode(\u0026#34;utf-8\u0026#34;, errors=\u0026#34;replace\u0026#34;) def flatten_once(d): out = {} for k, v in (d or {}).items(): if isinstance(v, dict): for k2, v2 in v.items(): out[f\u0026#34;{k}_{k2}\u0026#34;] = v2 else: out[k] = v return out def safe_int(x): try: return int(x) except: return None def parse_dns_line(line): raw = line.strip() if not raw: return None json_part = raw prefix_ts = None if ISO_TS_RE.match(raw): try: prefix_ts, rest = raw.split(\u0026#34; \u0026#34;, 1) json_part = rest except: pass if not json_part.startswith(\u0026#34;{\u0026#34;): idx = json_part.find(\u0026#34;{\u0026#34;) if idx != -1: json_part = json_part[idx:] try: obj = json.loads(json_part) except: return None flat = flatten_once(obj) if prefix_ts: flat[\u0026#34;_prefix_ts\u0026#34;] = prefix_ts return flat def lambda_handler(event, context): print(f\u0026#34;Received S3 Event. Records: {len(event.get(\u0026#39;Records\u0026#39;, []))}\u0026#34;) firehose_records = [] for record in event.get(\u0026#34;Records\u0026#34;, []): if \u0026#34;s3\u0026#34; not in record: continue bucket = record[\u0026#34;s3\u0026#34;][\u0026#34;bucket\u0026#34;][\u0026#34;name\u0026#34;] key = record[\u0026#34;s3\u0026#34;][\u0026#34;object\u0026#34;][\u0026#34;key\u0026#34;] if not key.startswith(SOURCE_PREFIX) or not key.endswith(\u0026#34;.gz\u0026#34;): print(f\u0026#34;Skipping file: {key}\u0026#34;) continue print(f\u0026#34;Processing S3 file: {key}\u0026#34;) # Extract VPC ID from file path vpc_id_match = VPC_RE.search(key) vpc_id = vpc_id_match.group(1) if vpc_id_match else \u0026#34;unknown\u0026#34; # Read and process file content content = read_gz(bucket, key) if not content: continue for line in content.splitlines(): r = parse_dns_line(line) if not r: continue # Create flattened JSON record out = { \u0026#34;version\u0026#34;: r.get(\u0026#34;version\u0026#34;), \u0026#34;account_id\u0026#34;: r.get(\u0026#34;account_id\u0026#34;), \u0026#34;region\u0026#34;: r.get(\u0026#34;region\u0026#34;), \u0026#34;vpc_id\u0026#34;: r.get(\u0026#34;vpc_id\u0026#34;, vpc_id), \u0026#34;query_timestamp\u0026#34;: r.get(\u0026#34;query_timestamp\u0026#34;), \u0026#34;query_name\u0026#34;: r.get(\u0026#34;query_name\u0026#34;), \u0026#34;query_type\u0026#34;: r.get(\u0026#34;query_type\u0026#34;), \u0026#34;query_class\u0026#34;: r.get(\u0026#34;query_class\u0026#34;), \u0026#34;rcode\u0026#34;: r.get(\u0026#34;rcode\u0026#34;), \u0026#34;answers\u0026#34;: json.dumps(r.get(\u0026#34;answers\u0026#34;), ensure_ascii=False), \u0026#34;srcaddr\u0026#34;: r.get(\u0026#34;srcaddr\u0026#34;), \u0026#34;srcport\u0026#34;: safe_int(r.get(\u0026#34;srcport\u0026#34;)), \u0026#34;transport\u0026#34;: r.get(\u0026#34;transport\u0026#34;), \u0026#34;srcids_instance\u0026#34;: r.get(\u0026#34;srcids_instance\u0026#34;), \u0026#34;timestamp\u0026#34;: (r.get(\u0026#34;query_timestamp\u0026#34;) or r.get(\u0026#34;timestamp\u0026#34;) or r.get(\u0026#34;_prefix_ts\u0026#34;)) } # Add newline for JSONL format json_row = json.dumps(out, ensure_ascii=False) + \u0026#34;\\n\u0026#34; firehose_records.append({\u0026#39;Data\u0026#39;: json_row}) # Send to Firehose in batches of 500 if firehose_records: total_records = len(firehose_records) print(f\u0026#34;Sending {total_records} records to Firehose...\u0026#34;) batch_size = 500 for i in range(0, total_records, batch_size): batch = firehose_records[i:i + batch_size] try: response = firehose.put_record_batch( DeliveryStreamName=FIREHOSE_STREAM_NAME, Records=batch ) if response[\u0026#39;FailedPutCount\u0026#39;] \u0026gt; 0: print(f\u0026#34;Warning: {response[\u0026#39;FailedPutCount\u0026#39;]} records failed\u0026#34;) except Exception as e: print(f\u0026#34;Firehose error: {e}\u0026#34;) return {\u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;, \u0026#34;total_records\u0026#34;: len(firehose_records)} "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.5-processing-setup/5.5.1-create-kinesis-data-firehose/","title":"Create Kinesis Data Firehose","tags":[],"description":"","content":"Create Kinesis Data Firehose Delivery Streams Create cloudtrail-firehose-stream Open Kinesis Console → Delivery streams → Create delivery stream\nConfigure:\nSource: Direct PUT Destination: Amazon S3 Stream name: cloudtrail-firehose-stream S3 bucket: processed-cloudtrail-logs-ACCOUNT_ID-REGION Prefix: processed-cloudtrail/date=!{timestamp:yyyy-MM-dd}/ Error prefix: processed-cloudtrail/errors/date=!{timestamp:yyyy-MM-dd}/error-type=!{firehose:error-output-type}/ Buffer size: 10 MB Buffer interval: 300 seconds Compression: GZIP IAM role: CloudTrailFirehoseRole Create delivery stream\nCreate vpc-dns-firehose-stream Stream name: vpc-dns-firehose-stream S3 bucket: processed-cloudwatch-logs-ACCOUNT_ID-REGION Prefix: vpc-logs/date=!{timestamp:yyyy-MM-dd}/ Error prefix: vpc-logs/errors/date=!{timestamp:yyyy-MM-dd}/error-type=!{firehose:error-output-type}/ IAM role: CloudWatchFirehoseRole (Same buffer/compression settings as above) Create vpc-flow-firehose-stream Stream name: vpc-flow-firehose-stream S3 bucket: processed-cloudwatch-logs-ACCOUNT_ID-REGION Prefix: eni-flow-logs/date=!{timestamp:yyyy-MM-dd}/ Error prefix: eni-flow-logs/errors/date=!{timestamp:yyyy-MM-dd}/error-type=!{firehose:error-output-type}/ IAM role: CloudWatchFirehoseRole (Same buffer/compression settings as above) "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.3-foundation-setup/5.3.3-create-iam-roles-and-policies/5.3.3.1-create-lambda-excecution-roles/","title":"Create Lambda Execution Roles","tags":[],"description":"","content":"Create CloudTrailETLLambdaServiceRole Open the IAM Console:\nNavigate to https://console.aws.amazon.com/iam/ Or: AWS Management Console → Search for \u0026ldquo;IAM\u0026rdquo; → Click \u0026ldquo;IAM\u0026rdquo; Navigate to Roles:\nIn the left sidebar, click \u0026ldquo;Roles\u0026rdquo; Click \u0026ldquo;Create role\u0026rdquo;\nSelect trusted entity:\nTrusted entity type: Select \u0026ldquo;AWS service\u0026rdquo; Use case: Select \u0026ldquo;Lambda\u0026rdquo; Click \u0026ldquo;Next\u0026rdquo; Add permissions:\nIn the search box, type AWSLambdaBasicExecutionRole Check the box next to \u0026ldquo;AWSLambdaBasicExecutionRole\u0026rdquo; Click \u0026ldquo;Next\u0026rdquo; Name, review, and create:\nRole name: Enter CloudTrailETLLambdaServiceRole Description: Enter Execution role for CloudTrail ETL Lambda function Click \u0026ldquo;Create role\u0026rdquo; Add inline policy:\nAfter creation, you\u0026rsquo;ll be on the role details page Click on the \u0026ldquo;Permissions\u0026rdquo; tab Click \u0026ldquo;Add permissions\u0026rdquo; → \u0026ldquo;Create inline policy\u0026rdquo; Create inline policy:\nClick on the \u0026ldquo;JSON\u0026rdquo; tab Paste the following policy (replace ACCOUNT_ID and REGION): { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;firehose:PutRecord\u0026#34;, \u0026#34;firehose:PutRecordBatch\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:firehose:REGION:ACCOUNT_ID:deliverystream/cloudtrail-firehose-stream\u0026#34; } ] } Click \u0026ldquo;Next\u0026rdquo;\nPolicy name:\nPolicy name: Enter CloudTrailETLPolicy Click \u0026ldquo;Create policy\u0026rdquo; Verify role creation:\nYou should see the role with both managed and inline policies attached Create Remaining Lambda Roles Follow the same process for each role below (steps 3-11):\nGuardDutyETLLambdaServiceRole\nRole name: GuardDutyETLLambdaServiceRole Description: Execution role for GuardDuty ETL Lambda function Managed policy: AWSLambdaBasicExecutionRole Inline policy name: GuardDutyETLPolicy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/*\u0026#34;, \u0026#34;arn:aws:s3:::processed-guardduty-findings-ACCOUNT_ID-REGION/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;kms:Decrypt\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:kms:REGION:ACCOUNT_ID:key/*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;glue:CreatePartition\u0026#34;, \u0026#34;glue:GetPartition\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:glue:REGION:ACCOUNT_ID:catalog\u0026#34;, \u0026#34;arn:aws:glue:REGION:ACCOUNT_ID:database/security_logs\u0026#34;, \u0026#34;arn:aws:glue:REGION:ACCOUNT_ID:table/security_logs/processed_guardduty\u0026#34; ] } ] } CloudWatchETLLambdaServiceRole\nRole name: CloudWatchETLLambdaServiceRole Description: Execution role for VPC DNS logs ETL Lambda Managed policy: AWSLambdaBasicExecutionRole Inline policy name: CloudWatchETLPolicy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;firehose:PutRecord\u0026#34;, \u0026#34;firehose:PutRecordBatch\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:firehose:REGION:ACCOUNT_ID:deliverystream/vpc-dns-firehose-stream\u0026#34; } ] } CloudWatchENIETLLambdaServiceRole\nRole name: CloudWatchENIETLLambdaServiceRole Description: Execution role for VPC Flow logs ETL Lambda Managed policy: AWSLambdaBasicExecutionRole Inline policy name: CloudWatchENIETLPolicy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;firehose:PutRecord\u0026#34;, \u0026#34;firehose:PutRecordBatch\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:firehose:REGION:ACCOUNT_ID:deliverystream/vpc-flow-firehose-stream\u0026#34; } ] } CloudWatchExportLambdaServiceRole\nRole name: CloudWatchExportLambdaServiceRole Description: Execution role for CloudWatch log export Lambda Managed policy: AWSLambdaBasicExecutionRole Inline policy name: CloudWatchExportPolicy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateExportTask\u0026#34;, \u0026#34;logs:DescribeExportTasks\u0026#34;, \u0026#34;s3:PutObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:logs:REGION:ACCOUNT_ID:log-group:*\u0026#34;, \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/*\u0026#34; ] } ] } ParseFindingsLambdaServiceRole\nRole name: ParseFindingsLambdaServiceRole Description: Execution role for parsing GuardDuty findings Managed policy: AWSLambdaBasicExecutionRole No inline policy needed IsolateEC2LambdaServiceRole\nRole name: IsolateEC2LambdaServiceRole Description: Execution role for isolating compromised EC2 instances Managed policy: AWSLambdaBasicExecutionRole Inline policy name: IsolateEC2Policy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:DescribeInstances\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } QuarantineIAMLambdaServiceRole\nRole name: QuarantineIAMLambdaServiceRole Description: Execution role for quarantining compromised IAM users Managed policy: AWSLambdaBasicExecutionRole Inline policy name: QuarantineIAMPolicy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iam:AttachUserPolicy\u0026#34;, \u0026#34;iam:ListAttachedUserPolicies\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:iam::ACCOUNT_ID:user/*\u0026#34;, \u0026#34;arn:aws:iam::ACCOUNT_ID:policy/IrQuarantineIAMPolicy\u0026#34; ] } ] } AlertDispatchLambdaServiceRole\nRole name: AlertDispatchLambdaServiceRole Description: Execution role for dispatching alerts via SNS/SES/Slack Managed policy: AWSLambdaBasicExecutionRole Inline policy name: AlertDispatchPolicy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;sns:Publish\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:sns:REGION:ACCOUNT_ID:IncidentResponseAlerts\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ses:SendEmail\u0026#34;, \u0026#34;ses:SendRawEmail\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.7-dashboard-setup/5.7.2-setup-lambda/5.7.2.1-create-iam-role-and-policy-for-lambda/","title":"Lambda IAM Role and Policy setup","tags":[],"description":"","content":"In this guide, you will setup IAM Role and Policy for Lambda.\nCreate IAM Role for Lambda Open the IAM Console\nNavigate to https://console.aws.amazon.com/iam/ Or: AWS Management Console → Services → IAM Create Role:\nChoose the Role option on the left menu panel. Then click Create role. Select trusted entity:\nTrusted entity type: AWS Service Use case: Lambda Click \u0026ldquo;Next\u0026rdquo; Attach permissions policies:\nIn the search box, type AWSLambdaBasicExecutionRole Check the box next to \u0026ldquo;AWSLambdaBasicExecutionRole\u0026rdquo; Click \u0026ldquo;Next\u0026rdquo; Name, review, and create:\nRole name: Enter dashboard-query-role Description: Enter Execution role for Lambda function Click \u0026ldquo;Create role\u0026rdquo; Add inline policy:\nAfter creation, you\u0026rsquo;ll be on the role details page Click on the \u0026ldquo;Permissions\u0026rdquo; tab Click \u0026ldquo;Add permissions\u0026rdquo; → \u0026ldquo;Create inline policy\u0026rdquo; Create inline policy:\nClick on the \u0026ldquo;JSON\u0026rdquo; tab Paste the following policy: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AthenaActions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;athena:StartQueryExecution\u0026#34;, \u0026#34;athena:GetQueryExecution\u0026#34;, \u0026#34;athena:GetQueryResults\u0026#34;, \u0026#34;athena:StopQueryExecution\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;GlueCatalogRead\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;glue:GetDatabase\u0026#34;, \u0026#34;glue:GetDatabases\u0026#34;, \u0026#34;glue:GetTable\u0026#34;, \u0026#34;glue:GetTables\u0026#34;, \u0026#34;glue:GetPartitions\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;S3SourceAndResultAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::vel-athena-results\u0026#34;, \u0026#34;arn:aws:s3:::vel-athena-results/*\u0026#34;, \u0026#34;arn:aws:s3:::vel-processed-cloudtrail-logs\u0026#34;, \u0026#34;arn:aws:s3:::vel-processed-cloudtrail-logs/*\u0026#34;, \u0026#34;arn:aws:s3:::vel-processed-guardduty\u0026#34;, \u0026#34;arn:aws:s3:::vel-processed-guardduty/*\u0026#34;, \u0026#34;arn:aws:s3:::cloudwatch-formatted\u0026#34;, \u0026#34;arn:aws:s3:::cloudwatch-formatted/*\u0026#34; ] } ] } Click \u0026ldquo;Next\u0026rdquo;\nPolicy name:\nPolicy name: Enter lambda-query-policy Click \u0026ldquo;Create policy\u0026rdquo; Verify role creation:\nYou should see the role with both managed and inline policies attached "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.1-workshop-overview/","title":"Overview","tags":[],"description":"","content":"System Components Auto Incident Response and Forensics is an architecture that uses automation services to ingest, process, and automatically respond to security findings, minimizing the time required for human intervention and aids security personel in visualizing and analyzing logs. This system is built around AWS Security Services (CloudTrail, GuardDuty, VPC Flow Logs, CloudWatch) feeding data into a Centralized Data Lake (S3/Glue/Athena) for analysis. The core automation is driven by AWS EventBridge rules triggering AWS Step Functions workflows, which then execute AWS Lambda functions to perform isolation and alerting actions. Workshop overview In this workshop, you will deploy a multi-phase system to achieve end-to-end security automation. This includes:\nFoundation Setup: Creating dedicated S3 buckets and IAM roles to support all services. Monitoring Setup: Enabling and configuring key security logs (CloudTrail, GuardDuty, VPC Flow Logs) to direct data to the central log ingestion point. Processing Setup: Deploying Kinesis Firehose, Lambda ETLs, and Glue/Athena tables to transform raw logs into an easily queryable security data lake. Automation Setup: Creating the Isolation Security Group, SNS Topic, Incident Response Lambda Functions, and the Step Functions State Machine that executes automatic quarantine actions when GuardDuty detects findings. Dashboard Setup: Hosting a secure, S3-based static web interface accelerated by CloudFront and protected by Cognito to provide analysts with real-time visualization of forensic data and direct query capabilities via API Gateway. "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.2-prerequiste/","title":"Prerequisites","tags":[],"description":"","content":"Required Access and Information Before proceeding with the setup of the Automated AWS Incident Response and Forensics System, ensure you have gathered the required access credentials and information below.\n🔑 Access \u0026amp; Identifiers AWS Account with Administrative Access You need full administrative permissions to create resources across multiple AWS services. Access to the AWS Management Console. Your AWS Account ID Format: 12-digit number (e.g., 123456789012). Placeholder: Replace ACCOUNT_ID throughout the guide. Target AWS Region Choose the region where you\u0026rsquo;ll deploy the system (e.g., us-east-1). Placeholder: Replace REGION throughout the guide. VPC ID A VPC with at least one subnet is required for VPC Flow Logs. Placeholder: Replace YOUR_VPC_ID in the guide. Amazon SES Verified Email Address Required for sending and recieving email alerts. Verify this address in the SES Console. Placeholder: Replace YOUR_VERIFIED_EMAIL@example.com. Slack Webhook URL (Optional) If you want Slack notifications, obtain a webhook URL from your Slack workspace. Placeholder: Replace YOUR_SLACK_WEBHOOK_URL. "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.3-foundation-setup/5.3.1-set-up-s3-buckets/","title":"Set up S3 buckets","tags":[],"description":"","content":"In this section, you will create 5 S3 buckets that serve as the foundation for the Auto Incident Response system.\nImportant: Replace ACCOUNT_ID with your AWS Account ID and REGION with your target region (e.g., us-east-1) in all bucket names.\nBucket Names incident-response-log-list-bucket-ACCOUNT_ID-REGION - Primary log collection bucket processed-cloudtrail-logs-ACCOUNT_ID-REGION - Stores processed CloudTrail logs athena-query-results-ACCOUNT_ID-REGION - Stores Athena query results processed-cloudwatch-logs-ACCOUNT_ID-REGION - Stores processed CloudWatch logs processed-guardduty-findings-ACCOUNT_ID-REGION - Stores processed GuardDuty findings Bucket Creation Instructions Open the Amazon S3 Console Navigate to https://console.aws.amazon.com/s3/ Or: AWS Management Console → Services → S3 Click on \u0026ldquo;Create bucket\u0026rdquo; General configuration: Bucket name: Enter incident-response-log-list-bucket-ACCOUNT_ID-REGION Example: incident-response-log-list-bucket-123456789012-us-east-1 AWS Region: Select your target region (e.g., US East (N. Virginia) us-east-1) Object Ownership:\nKeep default: ACLs disabled (recommended) Block Public Access settings for this bucket:\nCheck \u0026ldquo;Block all public access\u0026rdquo; Ensure all 4 sub-options are checked: ✓ Block public access to buckets and objects granted through new access control lists (ACLs) ✓ Block public access to buckets and objects granted through any access control lists (ACLs) ✓ Block public access to buckets and objects granted through new public bucket or access point policies ✓ Block public and cross-account access to buckets and objects through any public bucket or access point policies Bucket Versioning:\nSelect \u0026ldquo;Enable\u0026rdquo; Tags (optional):\nAdd tags if desired Example: Key=Purpose, Value=IncidentResponse Default encryption:\nEncryption type: Select \u0026ldquo;Server-side encryption with Amazon S3 managed keys (SSE-S3)\u0026rdquo; Bucket Key: Keep default (Enabled) Advanced settings:\nKeep all defaults Click \u0026ldquo;Create bucket\u0026rdquo;\nVerify bucket creation:\nYou should see a success message The bucket should appear in your S3 buckets list Repeat steps 2-10 for the remaining 4 buckets:\nprocessed-cloudtrail-logs-ACCOUNT_ID-REGION athena-query-results-ACCOUNT_ID-REGION processed-cloudwatch-logs-ACCOUNT_ID-REGION processed-guardduty-findings-ACCOUNT_ID-REGION Verify all 5 buckets are created:\nNavigate to S3 Console You should see all 5 buckets listed "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.7-dashboard-setup/5.7.1-setup-s3/","title":"Setup S3 Bucket for Dashboard","tags":[],"description":"","content":"In this guide, you will setup a S3 to contain web files and folder. Important: Replace ACCOUNT_ID with your AWS Account ID and REGION with your target region (e.g., us-east-1) in all bucket names.\nBucket Names static-dashboard-bucket-ACCOUNT_ID-REGION - Store builded web files and folder\nBucket Creation Instructions Open the Amazon S3 Console\nNavigate to https://console.aws.amazon.com/s3/ Or: AWS Management Console → Services → S3 Click on \u0026ldquo;Create bucket\u0026rdquo;\nBucket create setting:\nKeep the setting like default: Bucket name: Enter static-dashboard-bucket-ACCOUNT_ID-REGION Example: static-dashboard-bucket-123456789012-us-east-1 Ownership: ACLs disabled Block Public Access: Block all public access Bucket versioning: Disable Tags(Optional): Add if you want Encryption: SSE-S3 Bucket key: Enable Click Create bucket Verify bucket creation:\nYou should see a success message The bucket should appear in your S3 buckets list Upload files and folder:\nGo to Github to get the web content and upload to S3 "},{"uri":"https://beforelights.github.io/AWS-Worklog/2-proposal/","title":"Proposal","tags":[],"description":"","content":"\nAutomated AWS Incident Response and Forensics System Proposal Link (Google Docs): Proposal 1. Executive Summary Our team is building an automated incident response and forensics solution as part of the AWS First Cloud Journey internship program. The idea is straightforward—when a security issue happens in AWS, we want the system to respond automatically without waiting for manual intervention.\nWe\u0026rsquo;re creating a platform that automatically detects security findings from GuardDuty, isolates affected resources, captures forensic evidence through comprehensive data collection, and provides analytics and dashboards where security teams can investigate what happened. Everything is built using Infrastructure-as-Code with AWS CDK, so customers can easily deploy it into their own AWS accounts.\n2. Problem Statement What’s the Problem? The increasing frequency and sophistication of cyber threats pose significant risks to organizations relying on cloud infrastructure. Manual incident response processes are often slow, inconsistent, and prone to human error, which can lead to prolonged system downtime, data breaches, and financial losses. The project aims to address these challenges by developing an automated, reliable, and scalable incident response system that minimizes response time, enhances forensic capabilities, and reduces operational costs.\nThe Solution The main use cases include detecting unauthorized AWS credential use, identifying compromised EC2 instances, and ensuring forensic data is properly collected, processed, and stored for investigation. Our architecture integrates VPC Flow Logs, CloudTrail, CloudWatch, and GuardDuty to detect threats, while Step Functions orchestrates the automated response workflow including EC2 isolation, ASG detachment, Create Snapshot and IAM quarantine. All evidence is collected and processed through custom ETL Lambda and Data Firehose, using Athena for forensic analysis. The system also includes alert dispatching, notification via messaging and email, and provides dashboards and analytics for security teams to investigate what happened.\nBenefits and Return on Investment Rapid threat detection: Automated response reduces the window of vulnerability. Comprehensive evidence gathering: Automated forensic data collection facilitates faster investigations. Cost-effective deployment: Leveraging AWS serverless services minimizes infrastructure expenses. Improved security posture: Continuous monitoring and real-time alerts. Actionable insights: Dashboards and analytics empower security teams. Scalability: Adaptable to organizations of various sizes and incident volumes. 3. Solution Architecture Our solution uses a comprehensive multi-stage architecture for automated incident response and forensics:\nAWS Services Used Amazon GuardDuty: Continuously monitors for security threats and suspicious activity. AWS Step Functions: Orchestrates the incident response workflow. AWS Lambda: Runs automation code for isolation and data processing. Amazon EventBridge: Routes findings from GuardDuty to Step Functions. Amazon S3: Stores forensic evidence and hosts static dashboard. Amazon Athena: Enables SQL queries against forensic datasets. Amazon API Gateway: Facilitates communication between dashboard and backend. Amazon Cognito: Secures access for dashboard users. Amazon CloudFront: Accelerates dashboard delivery across the globe. Amazon SNS \u0026amp; SES: Handles notifications via messaging and email. AWS CloudTrail: Logs all actions for auditing. Amazon CloudWatch: Monitoring and dashboards. Amazon EC2: Optional instances for analysis. AWS KMS: Key management for encryption. Amazon Kinesis Data Firehose: Streams data to S3. Component Design Data Collection \u0026amp; Detection Layer: Collects events from VPC Flow Logs, CloudTrail, CloudWatch, EC2, and GuardDuty. Event Processing Layer: Alert Dispatch, EventBridge routes findings to Step Functions; events are classified by type. Automated Response Orchestration: Step Functions handle parsing, decision making, EC2 isolation, termination protection, ASG detachment, snapshot creation, and IAM quarantine. Alerting \u0026amp; Notification Layer: SNS, Slack \u0026amp; SES handles notifications via messaging and email, Alert Dispatch. Data Processing \u0026amp; Analytics Layer: ETL pipeline with Lambda and Data Firehose processes raw logs into S3; Athena queries the data. Dashboard \u0026amp; Analysis Layer: S3-hosted React dashboard with Cognito auth, consuming data via API Gateway and Athena. 4. Technical Implementation Implementation Phases We use Agile Scrum with 1-week sprints over 6 weeks:\nSprint 1: Foundation \u0026amp; Setup (VPC, Security Groups, Training). Sprint 2: Core Orchestration (Step Functions, Lambda, GuardDuty integration). Sprint 3: Data \u0026amp; Analytics (S3, Athena, ETL pipeline). Sprint 4: Dashboard \u0026amp; UI (Static site, API Gateway, CloudFront). Sprint 5: Testing \u0026amp; Optimization (Cognito, Performance testing, Simulations). Sprint 6: Documentation \u0026amp; Handover (Guides, Demos, Final Polish). Technical Requirements Frontend \u0026amp; Dashboards: Custom HTML/CSS/JS hosted on S3, served via CloudFront. Backend \u0026amp; Processing: Python 3.12 for Lambda, Step Functions for orchestration. Data \u0026amp; Storage: S3 for evidence, Athena for querying, Firehose for streaming. Infrastructure: All defined in AWS CDK (Python). Security: GuardDuty for detection, IAM for least privilege, KMS for encryption. 5. Timeline \u0026amp; Milestones Project Timeline Project Timeline\nWeek 6-7 (Foundation \u0026amp; Setup) Activities: Team training on GuardDuty/Step Functions, architecture design review, VPC and security setup. Deliverables: Architecture document v1, team training completion, GitHub repository established. Week 7-9 (Core Orchestration) Activities: Step Functions workflow development, Lambda function coding for all response actions, EventBridge integration, SNS/SES setup, integration testing. Deliverables: Step Functions state machine definition, 7+ Lambda functions with documentation, GuardDuty integration, notification system, API Gateway. Week 10 (Data \u0026amp; Analytics) Activities: S3 forensic storage setup, Athena table creation, ETL pipeline development, SQL query library. Deliverables: 15+ Athena queries documented, forensic analysis runbooks, processed data storage. Week 11 (Dashboard \u0026amp; UI) Activities: Static dashboard development, Cognito authentication, API Gateway setup, CloudFront CDN configuration, dashboard integration. Deliverables: S3-hosted dashboard, authentication system, query interface, real-time results integration. Week 12 (Testing \u0026amp; Validation \u0026amp; Optimization) Activities: Manual testing, security scanning including simulated incident scenarios (5+ workflows), performance testing, attack simulation. Optimize data with Athena query and Data Firehose. Deliverables: Security scan results, incident simulation videos, data optimization. Week 13 (Documentation \u0026amp; Handover) Activities: Deployment guide, API documentation, knowledge transfer sessions, final demo, GitHub cleanup. Deliverables: Complete GitHub repository (public), deployment guide instructions, live workshop demonstration. 6. Budget Estimation You can find the detailed budget estimation on the AWS Pricing Calculator.\nInfrastructure Costs Typical monthly deployment cost (Free Tier / Low scale): ~$5.01\nGuardDuty: ~$1.80/month S3: ~$1.07/month KMS: ~$1.12/month CloudTrail: ~$0.55/month Athena: ~$0.29/month Amazon Simple Email Service (SES): ~$0.09/month Amazon API Gateway: ~$0.05/month Amazon Data firehose: ~$0.04/month Lambda, Step Functions, SNS: Generally within Free Tier limits for typical usage. Note: Costs assume typical usage of 20-150 incidents per month.\n7. Risk Assessment Risk Matrix Performance Bottlenecks: High data volume slowing down queries. Security Breaches: Compromise of the forensic data itself. Cost Overruns: Unchecked logging or infinite loops. Mitigation Strategies Performance: Monitor Athena/Firehose; optimize queries; dynamic resource adjustment. Security: Encryption (KMS), strict IAM roles, audit logging, compliance checks. Cost: AWS budget alerts, cost anomaly detection, auto-scaling limits. Disaster Recovery: Backups, failover procedures, and redundancy measures. 8. Expected Outcomes Technical Improvements Automated Response: Zero-touch isolation of compromised resources. Speed: Reduction of investigation time from hours to minutes. Reliability: Consistent, repeatable evidence collection without human error. Long-term Value Scalable Architecture: Foundation for future security automation. Knowledge: Team competency in advanced AWS security and serverless concepts. Reusable Asset: A deployable solution for other AWS customers or teams. Status: Ready for Review \u0026amp; Approval Project Code: AWS-FCJ-IR-FORENSICS-2025\n"},{"uri":"https://beforelights.github.io/AWS-Worklog/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: “AWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS” Event Objectives The event aimed to provide a comprehensive introduction to the capabilities of AI, Machine Learning (ML), and Generative AI (GenAI) on AWS. Specific objectives included:\nExploring the foundational concepts of Generative AI and its applications. Demonstrating the use of AWS services and tools for AI/ML workflows. Highlighting best practices for integrating AI/ML into production environments. Speakers The event featured a diverse panel of experienced professionals who shared their expertise:\nLam Tuan Kiet – Senior DevOps Engineer, FPT Software Danh Hoang Hieu Nghi – AI Engineer, Renova Cloud Dinh Le Hoang Anh – Cloud Engineer Trainee, First Cloud AI Journey Van Hoang Kha – Cloud Security Engineer, AWS Community Builder Key Highlights Generative AI with Amazon Bedrock The session provided an in-depth exploration of Amazon Bedrock, a fully managed service for deploying and scaling foundation models. Key topics included:\nFoundation Models: Unlike traditional models, foundation models are pre-trained on vast datasets and can be adapted for a wide range of tasks. Amazon Bedrock offers models from leading AI companies such as OpenAI, Anthropic, and Cohere.\nPrompt Engineering: Techniques for crafting effective prompts to optimize model responses:\nZero-Shot Prompting: Providing no prior context or examples. Few-Shot Prompting: Including a few examples to guide the model. Chain of Thought (CoT): Incorporating reasoning steps to improve the quality of responses. Retrieval Augmented Generation (RAG): A technique to enhance model responses by retrieving relevant information from external data sources:\nRetrieval: Fetching relevant data from a knowledge base. Augmentation: Adding the retrieved data as context to the prompt. Generation: Producing responses based on the augmented prompt. Use Cases: Contextual chatbots, personalized search, real-time data summarization, and improved content generation. Amazon Titan Embedding: A lightweight model designed for high-accuracy retrieval tasks. It translates text into numerical embeddings and supports over 100 languages, making it ideal for multilingual applications.\nPretrained AI Services on AWS AWS offers a suite of ready-to-use AI services to address common business needs:\nAmazon Rekognition: Image and video analysis. Amazon Translate: Text detection and translation. Amazon Textract: Text and layout extraction from documents. Amazon Transcribe: Speech-to-text conversion. Amazon Polly: Text-to-speech synthesis. Amazon Comprehend: Text analysis for insights and relationships. Amazon Kendra: Intelligent search capabilities. Amazon Lookout: Anomaly detection in metrics, equipment, and images. Amazon Personalize: Personalized recommendations for users. Amazon Bedrock AgentCore The session introduced Amazon Bedrock AgentCore, a platform for securely deploying and scaling AI agents in production. Key features include:\nMemory: Enables agents to remember past interactions and learn over time. Identity and Access Controls: Ensures secure execution of agent workflows. Tool Integration: Supports tools like browsers and code interpreters for complex workflows. Observability: Provides insights into agent interactions for auditing and debugging. Frameworks for Building Agents: Includes CrewAI, LangChain, OpenAI Agents SDK, and more. Live Demo: AMZPhoto The demo showcased a face recognition application built using AWS AI services. The application demonstrated the seamless integration of Amazon Rekognition for image analysis.\nKey Takeaways Amazon Bedrock as a GenAI Hub: Bedrock simplifies access to foundation models from top providers, enabling rapid development of AI solutions. Prompt Engineering and RAG: Effective prompting techniques and retrieval-augmented generation can significantly enhance model performance. Titan Embeddings for Search: Amazon Titan Embedding is a powerful tool for high-accuracy information retrieval. Pretrained AI Services: AWS offers a wide range of services to address diverse AI/ML needs, from image analysis to personalized recommendations. AgentCore for Production-Ready AI: Bedrock AgentCore addresses the challenges of deploying AI agents at scale, ensuring security, scalability, and observability. Applying Insights to Work The knowledge gained from this event can be applied in several ways:\nIntegrating Foundation Models: Incorporating foundation models into future projects to enhance AI capabilities. Optimizing Workflows with RAG: Using retrieval-augmented generation to improve the quality of AI-driven solutions. Leveraging Pretrained Services: Utilizing AWS AI services like Rekognition and Textract to automate repetitive tasks and improve efficiency. Building Scalable AI Agents: Exploring Bedrock AgentCore for deploying secure and scalable AI agents in production environments. Event Experience The event was highly engaging and informative, with well-prepared speakers and interactive sessions. Key highlights of the experience included:\nQ\u0026amp;A Session: A team member raised a critical question about handling high volumes of alerts in an architecture using SNS for GuardDuty findings. The solution provided was to integrate SQS to queue events, ensuring no alerts are missed. Kahoot Quiz: Was top 3 until the Ragnarok began\u0026hellip; Networking: Formed an unofficial group, \u0026ldquo;Mèo Cam Đeo Khăn,\u0026rdquo; in collaboration with other attendees, fostering connections and future collaboration opportunities. Event Photos "},{"uri":"https://beforelights.github.io/AWS-Worklog/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Master EC2 instance types, purchasing options, and placement groups. Implement advanced IAM policies for region restriction and MFA enforcement. Host a static website on Amazon S3 with access logging and versioning. Accelerate content delivery using Amazon CloudFront and route domains with Route 53. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Finished restricting an IAM from creating EC2 from another region. (Restricting Service Usage by AWS Region) - Learnt how to analyze compute requirements and chose an appropriate instance family for it. - Launched instances within the same and different families to test the IAM EC2 Policy - Learnt how to restrict certain EBS volumes and the general information about EBS volume types - Tested the policy by launching an EC2 with an EBS volume type that isn\u0026rsquo;t permitted. - Learnt how to limit access to resources by only allowing access to certain IPs - Learnt how to limit permission to delete resources by time period 15/09/2025 15/09/2025 Amazon EC2 instance types Introduction to Amazon EC2 Amazon EC2 instance types IAM policy testing with the IAM policy simulator IAM Policy Simulator 3 - Created an EC2 and SSH\u0026rsquo;ed into it using MobaXterm and created an S3 bucket - Upload files using an accesskey/secretaccesskey: + Generated an IAM user with an access key + Uploaded files to the S3 bucket - Upload files with eligible roles on EC2: + Created a role for the EC2 + Uploaded files to the S3 bucket via EC2 role. 16/09/2025 16/09/2025 Granting authorization for an application to access AWS services with an IAM role. 4 - Looked into Cloud 9 IDE, on what it is about (Can\u0026rsquo;t use it because I don\u0026rsquo;t have access to it.) - Information about S3: + Know the difference between S3 Bucket and S3 Object + Know about the storage classes and others. - Create an S3 bucket and host a static website: + Created an S3 bucket and uploaded the data + Enabled Static website hosting and unchecked Block all public access + Changed the bucket permission settings and made objects public using ACL + Successfully accessed the static website 17/09/2025 17/09/2025 How to migrate from AWS Cloud9 to AWS IDE Toolkits or AWS CloudShell Get started with AWS Cloud 9 Starting with Amazon S3 5 - Accelerate Static Websites with Cloudfront: + Blocked all public access + Created and configured a Cloudfront distribution (The new UI doesn\u0026rsquo;t let you choose the Default root object and Price class on creating, to change this, select your distribution, click on the General tab, find Settings and click Edit, then you can see the missing options there.) + Successfully accessed the website + Checked the loading time by inspecting the website, the flow would be: Inspect -\u0026gt; Network -\u0026gt; Refresh page You can see the time at the far right of the Cloudfront document (for me it\u0026rsquo;s 29ms), when you click on the Cloudfront document, you can see which PoP the website is being returned from It\u0026rsquo;s HKG1-P2 (Hong Kong) for me - Bucket Versioning: + Read and enabled Bucket Versioning + Tested it by editing the index.html file and uploading it onto the S3 bucket and checking Cloudfront - Moving objects from an S3 to another: + Created a new S3 Bucket + Calculated the size of the entire bucket to compare after moving + Successfully moved the bucket + Calculated the size to make sure we didn\u0026rsquo;t lose any data Out of curiosity, I tried changing the Origin domain of the Cloudfront distribution, added a Bucket policy in permissions and enabled Static website hosting on the new S3 Bucket and it works wonder! - Successfully replicated objects inside of a S3 Bucket to another region. Notes and Best Practices 18/09/2025 18/09/2025 Starting with Amazon S3 Restrict access to an Amazon S3 origin AWS Global Infrastructure - Expenses: + Amazon S3 Price +\tAmazon CloudFront Pricing\t+ Amazon Price Week 2 Achievements: Deployed a static website on S3 with public access blocking configured correctly. Created IAM policies that restrict operations to specific regions (e.g., ap-southeast-1) for compliance. Configured a CloudFront distribution with OAC (Origin Access Control) to securely serve private S3 content. Enabled S3 Versioning and replicated data to a secondary region for disaster recovery testing. "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.10-cleanup/5.10.2-cdk-cleanup/","title":"CDK Cleanup","tags":[],"description":"","content":"Clean up (CDK) This guide ensures you correctly decommission all resources provisioned by the AWS CDK stack and clean up manually created data to avoid ongoing charges.\nPhase 1: Manual Data Cleanup (Before CDK Destroy) The CDK automatically deletes most resources failed in deleting content from S3 buckets. You must empty the contents of these buckets before running the cdk destroy command.\nResource Name Purpose Action Required incident-response-log-list-bucket Primary Log Source Empty Contents processed-cloudwatch-logs ETL Destination Empty Contents processed-guardduty-findings ETL Destination Empty Contents processed-cloudtrail-logs ETL Destination Empty Contents athena-query-results Athena Query Results Empty Contents aws-incident-response-automation-dashboard React Dashboard S3 Bucket Empty Contents Instructions for Emptying Buckets:\nOpen the Amazon S3 Console in your browser. For each of the buckets listed above (look for the names based on your AWS Account ID and Region): Click on the bucket name. Navigate to the \u0026ldquo;Objects\u0026rdquo; tab. Click the \u0026ldquo;Empty\u0026rdquo; button. Follow the prompts to confirm the permanent deletion of all objects. Phase 2: CDK Stack Destruction This step uses the CDK CLI to destroy all resources provisioned by the CloudFormation stack.\nEnsure Virtual Environment is Active\nIf you deactivated your Python environment, re-activate it (e.g., source .venv/bin/activate). Navigate to the Project Root\nEnsure you are in the main directory where the cdk.json file is located. Execute the Destroy Command\nRun the command to destroy all deployed stacks. When prompted, type y to approve the deletion. $ cdk destroy --all Phase 3: Post-Destruction Cleanup This step addresses remaining manual cleanup of lingering resources.\nDelete Remaining S3 Buckets\nThe cdk destroy command should remove the empty S3 buckets. If any remain (due to final checks or service protections), delete them now via the S3 Console. Disable Amazon GuardDuty\nGo to GuardDuty Console → Settings → General. Verify the service is disabled to ensure billing stops. Remove Cognito User and Pool\nGo to Cognito Console → User pools. Delete the test user you created. Delete the User Pool created for the dashboard. Remove SES Identity\nGo to Amazon SES Console → Verified Identities. Delete the sender email identity (sender_email) you verified. Deactivate Virtual Environment\nDeactivate the Python virtual environment: $ deactivate "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.5-processing-setup/5.5.2-create-aws-glue-database-and-tables/","title":"Create AWS Glue Database and Tables","tags":[],"description":"","content":"Create AWS Glue Database and Tables Create Database Open Glue Console → Databases → Add database\nDatabase name: security_logs\nCreate database\nCreate Tables (Using Athena DDL) Open Athena Console\nSet query result location: s3://athena-query-results-ACCOUNT_ID-REGION/\nSelect database: security_logs\nCreate processed_cloudtrail Table Run this DDL in Athena (replace ACCOUNT_ID and REGION):\nCREATE EXTERNAL TABLE IF NOT EXISTS security_logs.processed_cloudtrail ( `eventtime` string, `eventname` string, `eventsource` string, `awsregion` string, `sourceipaddress` string, `useragent` string, `useridentity` struct\u0026lt; type:string, invokedby:string, principalid:string, arn:string, accountid:string, accesskeyid:string, username:string, sessioncontext:struct\u0026lt; attributes:map\u0026lt;string,string\u0026gt;, sessionissuer:struct\u0026lt; type:string, principalid:string, arn:string, accountid:string, username:string \u0026gt; \u0026gt;, inscopeof:struct\u0026lt; issuertype:string, credentialsissuedto:string \u0026gt; \u0026gt;, `requestparameters` string, `responseelements` string, `resources` array\u0026lt;struct\u0026lt;arn:string,type:string\u0026gt;\u0026gt;, `recipientaccountid` string, `serviceeventdetails` string, `errorcode` string, `errormessage` string, `hour` string, `usertype` string, `username` string, `isconsolelogin` boolean, `isfailedlogin` boolean, `isrootuser` boolean, `isassumedrole` boolean, `ishighriskevent` boolean, `isprivilegedaction` boolean, `isdataaccess` boolean, `target_bucket` string, `target_key` string, `target_username` string, `target_rolename` string, `target_policyname` string, `new_access_key` string, `new_instance_id` string, `target_group_id` string, `identity_principalid` string ) PARTITIONED BY ( `date` string ) ROW FORMAT SERDE \u0026#39;org.openx.data.jsonserde.JsonSerDe\u0026#39; WITH SERDEPROPERTIES ( \u0026#39;serialization.format\u0026#39; = \u0026#39;1\u0026#39; ) LOCATION \u0026#39;s3://processed-cloudtrail-logs-ACCOUNT_ID-REGION/processed-cloudtrail/\u0026#39; TBLPROPERTIES ( \u0026#39;projection.enabled\u0026#39; = \u0026#39;true\u0026#39;, \u0026#39;projection.date.type\u0026#39; = \u0026#39;date\u0026#39;, \u0026#39;projection.date.format\u0026#39; = \u0026#39;yyyy-MM-dd\u0026#39;, \u0026#39;projection.date.range\u0026#39; = \u0026#39;2025-01-01,NOW\u0026#39;, \u0026#39;projection.date.interval\u0026#39; = \u0026#39;1\u0026#39;, \u0026#39;projection.date.interval.unit\u0026#39; = \u0026#39;DAYS\u0026#39;, \u0026#39;storage.location.template\u0026#39; = \u0026#39;s3://processed-cloudtrail-logs-ACCOUNT_ID-REGION/processed-cloudtrail/date=${date}/\u0026#39;, \u0026#39;classification\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;compressionType\u0026#39; = \u0026#39;gzip\u0026#39; ); Create processed_guardduty Table Run this DDL in Athena:\nCREATE EXTERNAL TABLE IF NOT EXISTS security_logs.processed_guardduty ( `finding_id` string, `finding_type` string, `title` string, `severity` double, `account_id` string, `region` string, `created_at` string, `event_last_seen` string, `remote_ip` string, `remote_port` int, `connection_direction` string, `protocol` string, `dns_domain` string, `dns_protocol` string, `scanned_ip` string, `scanned_port` int, `aws_api_service` string, `aws_api_name` string, `aws_api_caller_type` string, `aws_api_error` string, `aws_api_remote_ip` string, `target_resource_arn` string, `instance_id` string, `instance_type` string, `image_id` string, `instance_tags` string, `resource_region` string, `access_key_id` string, `principal_id` string, `user_name` string, `s3_bucket_name` string, `service_raw` string, `resource_raw` string, `metadata_raw` string ) PARTITIONED BY ( `date` string ) ROW FORMAT SERDE \u0026#39;org.openx.data.jsonserde.JsonSerDe\u0026#39; WITH SERDEPROPERTIES ( \u0026#39;serialization.format\u0026#39; = \u0026#39;1\u0026#39; ) LOCATION \u0026#39;s3://processed-guardduty-findings-ACCOUNT_ID-REGION/processed-guardduty/\u0026#39; TBLPROPERTIES ( \u0026#39;classification\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;compressionType\u0026#39; = \u0026#39;gzip\u0026#39;, \u0026#39;projection.enabled\u0026#39; = \u0026#39;true\u0026#39;, \u0026#39;projection.date.type\u0026#39; = \u0026#39;date\u0026#39;, \u0026#39;projection.date.range\u0026#39; = \u0026#39;2025-01-01,NOW\u0026#39;, \u0026#39;projection.date.format\u0026#39; = \u0026#39;yyyy-MM-dd\u0026#39;, \u0026#39;projection.date.interval\u0026#39; = \u0026#39;1\u0026#39;, \u0026#39;projection.date.interval.unit\u0026#39; = \u0026#39;DAYS\u0026#39;, \u0026#39;storage.location.template\u0026#39; = \u0026#39;s3://processed-guardduty-findings-ACCOUNT_ID-REGION/processed-guardduty/date=${date}/\u0026#39; ); Create vpc_logs Table Run this DDL in Athena:\nCREATE EXTERNAL TABLE IF NOT EXISTS security_logs.vpc_logs ( `version` string, `account_id` string, `region` string, `vpc_id` string, `query_timestamp` string, `query_name` string, `query_type` string, `query_class` string, `rcode` string, `answers` string, `srcaddr` string, `srcport` int, `transport` string, `srcids_instance` string, `timestamp` string ) PARTITIONED BY ( `date` string ) ROW FORMAT SERDE \u0026#39;org.openx.data.jsonserde.JsonSerDe\u0026#39; WITH SERDEPROPERTIES ( \u0026#39;serialization.format\u0026#39; = \u0026#39;1\u0026#39;, \u0026#39;ignore.malformed.json\u0026#39; = \u0026#39;true\u0026#39; ) LOCATION \u0026#39;s3://processed-cloudwatch-logs-ACCOUNT_ID-REGION/vpc-logs/\u0026#39; TBLPROPERTIES ( \u0026#39;projection.enabled\u0026#39; = \u0026#39;true\u0026#39;, \u0026#39;projection.date.type\u0026#39; = \u0026#39;date\u0026#39;, \u0026#39;projection.date.format\u0026#39; = \u0026#39;yyyy-MM-dd\u0026#39;, \u0026#39;projection.date.range\u0026#39; = \u0026#39;2025-01-01,NOW\u0026#39;, \u0026#39;projection.date.interval\u0026#39; = \u0026#39;1\u0026#39;, \u0026#39;projection.date.interval.unit\u0026#39; = \u0026#39;DAYS\u0026#39;, \u0026#39;storage.location.template\u0026#39; = \u0026#39;s3://processed-cloudwatch-logs-ACCOUNT_ID-REGION/vpc-logs/date=${date}/\u0026#39;, \u0026#39;classification\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;compressionType\u0026#39; = \u0026#39;gzip\u0026#39; ); Create eni_flow_logs Table Run this DDL in Athena:\nCREATE EXTERNAL TABLE IF NOT EXISTS security_logs.eni_flow_logs ( `version` int, `account_id` string, `interface_id` string, `srcaddr` string, `dstaddr` string, `srcport` int, `dstport` int, `protocol` int, `packets` bigint, `bytes` bigint, `start_time` bigint, `end_time` bigint, `action` string, `log_status` string, `timestamp_str` string ) PARTITIONED BY ( `date` string ) ROW FORMAT SERDE \u0026#39;org.openx.data.jsonserde.JsonSerDe\u0026#39; WITH SERDEPROPERTIES ( \u0026#39;serialization.format\u0026#39; = \u0026#39;1\u0026#39; ) LOCATION \u0026#39;s3://processed-cloudwatch-logs-ACCOUNT_ID-REGION/eni-flow-logs/\u0026#39; TBLPROPERTIES ( \u0026#39;projection.enabled\u0026#39; = \u0026#39;true\u0026#39;, \u0026#39;projection.date.type\u0026#39; = \u0026#39;date\u0026#39;, \u0026#39;projection.date.format\u0026#39; = \u0026#39;yyyy-MM-dd\u0026#39;, \u0026#39;projection.date.range\u0026#39; = \u0026#39;2025-01-01,NOW\u0026#39;, \u0026#39;projection.date.interval\u0026#39; = \u0026#39;1\u0026#39;, \u0026#39;projection.date.interval.unit\u0026#39; = \u0026#39;DAYS\u0026#39;, \u0026#39;storage.location.template\u0026#39; = \u0026#39;s3://processed-cloudwatch-logs-ACCOUNT_ID-REGION/eni-flow-logs/date=${date}/\u0026#39;, \u0026#39;classification\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;compressionType\u0026#39; = \u0026#39;gzip\u0026#39; ); "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.7-dashboard-setup/5.7.2-setup-lambda/","title":"Create IAM Roles and Policies","tags":[],"description":"","content":"In this section, you will create IAM role and Policy for Lambda. After that you will create Lambda Function to execute query\nContent Create Lambda Execution Roles and Policy Create Lambda Function "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.3-foundation-setup/5.3.3-create-iam-roles-and-policies/5.3.3.2-create-service-roles/","title":"Create Service Roles","tags":[],"description":"","content":"Create Firehose Roles Create CloudTrailFirehoseRole Open IAM Console → Roles → Create role\nSelect trusted entity:\nTrusted entity type: AWS service Use case: Select \u0026ldquo;Kinesis\u0026rdquo; → \u0026ldquo;Kinesis Firehose\u0026rdquo; Click \u0026ldquo;Next\u0026rdquo; Add permissions:\nSkip adding managed policies (we\u0026rsquo;ll add inline policy) Click \u0026ldquo;Next\u0026rdquo; Name and create:\nRole name: CloudTrailFirehoseRole Description: Allows Firehose to write CloudTrail logs to S3 Click \u0026ldquo;Create role\u0026rdquo; Add inline policy:\nPolicy name: FirehosePolicy Policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:PutObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::processed-cloudtrail-logs-ACCOUNT_ID-REGION\u0026#34;, \u0026#34;arn:aws:s3:::processed-cloudtrail-logs-ACCOUNT_ID-REGION/*\u0026#34; ] } ] } Create CloudWatchFirehoseRole Role name: CloudWatchFirehoseRole Description: Allows Firehose to write CloudWatch logs to S3 Trusted entity: Kinesis Firehose Inline policy name: FirehosePolicy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:PutObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::processed-cloudwatch-logs-ACCOUNT_ID-REGION\u0026#34;, \u0026#34;arn:aws:s3:::processed-cloudwatch-logs-ACCOUNT_ID-REGION/*\u0026#34; ] } ] } Create Step Functions Role Create StepFunctionsRole Create role:\nTrusted entity: Step Functions Role name: StepFunctionsRole Description: Execution role for Incident Response Step Functions Add TWO inline policies:\nPolicy 1: LambdaInvokePolicy\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:lambda:REGION:ACCOUNT_ID:function:ir-isolate-ec2-lambda\u0026#34;, \u0026#34;arn:aws:lambda:REGION:ACCOUNT_ID:function:ir-parse-findings-lambda\u0026#34;, \u0026#34;arn:aws:lambda:REGION:ACCOUNT_ID:function:ir-quarantine-iam-lambda\u0026#34; ] } ] } Policy 2: EC2AutoScalingPolicy\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;autoscaling:DescribeAutoScalingInstances\u0026#34;, \u0026#34;autoscaling:DetachInstances\u0026#34;, \u0026#34;autoscaling:UpdateAutoScalingGroup\u0026#34;, \u0026#34;ec2:CreateSnapshot\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:DescribeVolumes\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Create EventBridge Role Create IncidentResponseStepFunctionsEventRole Role name: IncidentResponseStepFunctionsEventRole Description: Allows EventBridge to trigger Step Functions Trusted entity: EventBridge Inline policy name: StartStepFunctionsPolicy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;states:StartExecution\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:REGION:ACCOUNT_ID:stateMachine:IncidentResponseStepFunctions\u0026#34; } ] } Create VPC Flow Logs Role Create FlowLogsIAMRole Create role:\nTrusted entity: EC2 (will edit trust policy) Role name: FlowLogsIAMRole Edit trust relationship to:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;vpc-flow-logs.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } Add inline policy: Policy name: FlowLogsPolicy Policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:DescribeLogStreams\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Create Glue Role Create GlueCloudWatchRole Role name: GlueCloudWatchRole Description: Allows Glue to access S3 and CloudWatch Logs Trusted entity: Glue Managed policies (attach 3): AWSGlueServiceRole CloudWatchLogsReadOnlyAccess AmazonS3FullAccess No inline policies needed "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.11-appendices/5.11.2-guardduty-etl/","title":"GuardDuty ETL Code","tags":[],"description":"","content":" import json import boto3 import gzip import os from datetime import datetime from urllib.parse import unquote_plus s3_client = boto3.client(\u0026#39;s3\u0026#39;) DATABASE_NAME = os.environ.get(\u0026#34;DATABASE_NAME\u0026#34;, \u0026#34;security_logs\u0026#34;) TABLE_NAME_GUARDDUTY = os.environ.get(\u0026#34;TABLE_NAME_GUARDDUTY\u0026#34;, \u0026#34;processed_guardduty\u0026#34;) S3_LOCATION_GUARDDUTY = os.environ.get(\u0026#34;S3_LOCATION_GUARDDUTY\u0026#34;, \u0026#34;s3://vel-processed-guardduty/processed-guardduty/\u0026#34;) DESTINATION_BUCKET = os.environ.get(\u0026#34;DESTINATION_BUCKET\u0026#34;, \u0026#34;vel-processed-guardduty\u0026#34;) def promote_network_details(finding_service): if not finding_service: return {} action = finding_service.get(\u0026#39;action\u0026#39;, {}) net_conn_action = action.get(\u0026#39;networkConnectionAction\u0026#39;, {}) if net_conn_action: remote_ip = net_conn_action.get(\u0026#39;remoteIpDetails\u0026#39;, {}).get(\u0026#39;ipAddressV4\u0026#39;) or \\ net_conn_action.get(\u0026#39;remoteIpDetails\u0026#39;, {}).get(\u0026#39;ipAddressV6\u0026#39;) return { \u0026#39;remote_ip\u0026#39;: remote_ip, \u0026#39;remote_port\u0026#39;: net_conn_action.get(\u0026#39;remotePortDetails\u0026#39;, {}).get(\u0026#39;port\u0026#39;), \u0026#39;connection_direction\u0026#39;: net_conn_action.get(\u0026#39;connectionDirection\u0026#39;), \u0026#39;protocol\u0026#39;: net_conn_action.get(\u0026#39;protocol\u0026#39;), } dns_action = action.get(\u0026#39;dnsRequestAction\u0026#39;, {}) if dns_action: return {\u0026#39;dns_domain\u0026#39;: dns_action.get(\u0026#39;domain\u0026#39;), \u0026#39;dns_protocol\u0026#39;: dns_action.get(\u0026#39;protocol\u0026#39;)} port_probe_action = action.get(\u0026#39;portProbeAction\u0026#39;, {}) if port_probe_action and port_probe_action.get(\u0026#39;portProbeDetails\u0026#39;): detail = port_probe_action[\u0026#39;portProbeDetails\u0026#39;][0] return { \u0026#39;scanned_ip\u0026#39;: detail.get(\u0026#39;remoteIpDetails\u0026#39;, {}).get(\u0026#39;ipAddressV4\u0026#39;), \u0026#39;scanned_port\u0026#39;: detail.get(\u0026#39;localPortDetails\u0026#39;, {}).get(\u0026#39;port\u0026#39;), } return {} def promote_api_details(finding_service): if not finding_service: return {} action = finding_service.get(\u0026#39;action\u0026#39;, {}) aws_api_action = action.get(\u0026#39;awsApiCallAction\u0026#39;, {}) if aws_api_action: return { \u0026#39;aws_api_service\u0026#39;: aws_api_action.get(\u0026#39;serviceName\u0026#39;), \u0026#39;aws_api_name\u0026#39;: aws_api_action.get(\u0026#39;api\u0026#39;), \u0026#39;aws_api_caller_type\u0026#39;: aws_api_action.get(\u0026#39;callerType\u0026#39;), \u0026#39;aws_api_error\u0026#39;: aws_api_action.get(\u0026#39;errorCode\u0026#39;), \u0026#39;aws_api_remote_ip\u0026#39;: aws_api_action.get(\u0026#39;remoteIpDetails\u0026#39;, {}).get(\u0026#39;ipAddressV4\u0026#39;), } return {} def promote_resource_details(finding_resource): if not finding_resource: return {} instance_details = finding_resource.get(\u0026#39;instanceDetails\u0026#39;, {}) if instance_details: return { \u0026#39;target_resource_arn\u0026#39;: instance_details.get(\u0026#39;arn\u0026#39;), \u0026#39;instance_id\u0026#39;: instance_details.get(\u0026#39;instanceId\u0026#39;), \u0026#39;resource_region\u0026#39;: instance_details.get(\u0026#39;awsRegion\u0026#39;), \u0026#39;instance_type\u0026#39;: instance_details.get(\u0026#39;instanceType\u0026#39;), \u0026#39;image_id\u0026#39;: instance_details.get(\u0026#39;imageId\u0026#39;), \u0026#39;instance_tags\u0026#39;: instance_details.get(\u0026#39;tags\u0026#39;) } access_key_details = finding_resource.get(\u0026#39;accessKeyDetails\u0026#39;, {}) if access_key_details: return { \u0026#39;access_key_id\u0026#39;: access_key_details.get(\u0026#39;accessKeyId\u0026#39;), \u0026#39;principal_id\u0026#39;: access_key_details.get(\u0026#39;principalId\u0026#39;), \u0026#39;user_name\u0026#39;: access_key_details.get(\u0026#39;userName\u0026#39;), } s3_details = finding_resource.get(\u0026#39;s3BucketDetails\u0026#39;, []) if s3_details: return { \u0026#39;target_resource_arn\u0026#39;: s3_details[0].get(\u0026#39;arn\u0026#39;), \u0026#39;s3_bucket_name\u0026#39;: s3_details[0].get(\u0026#39;name\u0026#39;), } return {} def process_guardduty_log(bucket, key): response = s3_client.get_object(Bucket=bucket, Key=key) if key.endswith(\u0026#39;.gz\u0026#39;): content = gzip.decompress(response[\u0026#39;Body\u0026#39;].read()).decode(\u0026#39;utf-8\u0026#39;) else: content = response[\u0026#39;Body\u0026#39;].read().decode(\u0026#39;utf-8\u0026#39;) processed_findings = [] for line in content.splitlines(): if not line: continue try: finding = json.loads(line) except json.JSONDecodeError: print(f\u0026#34;Skipping malformed JSON line in {key}\u0026#34;); continue finding_type = finding.get(\u0026#39;type\u0026#39;, \u0026#39;UNKNOWN\u0026#39;) finding_service = finding.get(\u0026#39;service\u0026#39;, {}) network_fields = promote_network_details(finding_service) api_fields = promote_api_details(finding_service) resource_fields = promote_resource_details(finding.get(\u0026#39;resource\u0026#39;, {})) created_at_str = finding.get(\u0026#39;createdAt\u0026#39;) event_last_seen_str = finding_service.get(\u0026#39;eventLastSeen\u0026#39;) dt_obj = datetime.now() if event_last_seen_str: try: dt_obj = datetime.strptime(event_last_seen_str, \u0026#39;%Y-%m-%dT%H:%M:%S.%fZ\u0026#39;) except ValueError: try: dt_obj = datetime.strptime(event_last_seen_str, \u0026#39;%Y-%m-%dT%H:%M:%SZ\u0026#39;) except ValueError: pass elif created_at_str: try: dt_obj = datetime.strptime(created_at_str, \u0026#39;%Y-%m-%dT%H:%M:%S.%fZ\u0026#39;) except ValueError: try: dt_obj = datetime.strptime(created_at_str, \u0026#39;%Y-%m-%dT%H:%M:%SZ\u0026#39;) except ValueError: pass processed_record = { \u0026#39;finding_id\u0026#39;: finding.get(\u0026#39;id\u0026#39;), \u0026#39;finding_type\u0026#39;: finding_type, \u0026#39;title\u0026#39;: finding.get(\u0026#39;title\u0026#39;), \u0026#39;severity\u0026#39;: finding.get(\u0026#39;severity\u0026#39;), \u0026#39;account_id\u0026#39;: finding.get(\u0026#39;accountId\u0026#39;), \u0026#39;region\u0026#39;: finding.get(\u0026#39;region\u0026#39;), \u0026#39;created_at\u0026#39;: created_at_str, \u0026#39;event_last_seen\u0026#39;: event_last_seen_str, **network_fields, **api_fields, **resource_fields, \u0026#39;date\u0026#39;: dt_obj.strftime(\u0026#39;%Y-%m-%d\u0026#39;), \u0026#39;service_raw\u0026#39;: json.dumps(finding_service), \u0026#39;resource_raw\u0026#39;: json.dumps(finding.get(\u0026#39;resource\u0026#39;, {})), \u0026#39;metadata_raw\u0026#39;: json.dumps(finding.get(\u0026#39;metadata\u0026#39;, {})), } processed_findings.append(processed_record) return processed_findings def save_processed_data(processed_events, source_key): if not processed_events: return first_event = processed_events[0] date_str = first_event.get(\u0026#39;date\u0026#39;, datetime.now().strftime(\u0026#39;%Y-%m-%d\u0026#39;)) original_filename = source_key.split(\u0026#39;/\u0026#39;)[-1].replace(\u0026#39;.gz\u0026#39;, \u0026#39;\u0026#39;).replace(\u0026#39;.json\u0026#39;, \u0026#39;\u0026#39;) output_key = f\u0026#34;processed-guardduty/date={date_str}/{original_filename}_processed.jsonl.gz\u0026#34; json_lines = \u0026#34;\u0026#34; for event in processed_events: event_to_dump = event.copy() json_lines += json.dumps(event_to_dump) + \u0026#34;\\n\u0026#34; compressed_data = gzip.compress(json_lines.encode(\u0026#39;utf-8\u0026#39;)) s3_client.put_object( Bucket=DESTINATION_BUCKET, Key=output_key, Body=compressed_data, ContentType=\u0026#39;application/jsonl\u0026#39;, ContentEncoding=\u0026#39;gzip\u0026#39; ) print(f\u0026#34;Saved processed data to: s3://{DESTINATION_BUCKET}/{output_key}\u0026#34;) def lambda_handler(event, context): for record in event[\u0026#39;Records\u0026#39;]: bucket = record[\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] key = unquote_plus(record[\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;]) print(f\u0026#34;Processing GuardDuty finding file: s3://{bucket}/{key}\u0026#34;) try: processed_findings = process_guardduty_log(bucket, key) save_processed_data(processed_findings, key) print(f\u0026#34;Successfully processed {len(processed_findings)} findings from {key}\u0026#34;) except Exception as e: print(f\u0026#34;Error processing {key}: {str(e)}\u0026#34;) raise e return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;GuardDuty findings processed successfully\u0026#39;) } "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.7-dashboard-setup/5.7.2-setup-lambda/5.7.2.2-create-lambda-function/","title":"Lambda setup","tags":[],"description":"","content":"In this guide, you will setup a Lambda using Python to execute query using Athena service.\nCreate Lambda Function Open the Lambda Console\nNavigate to https://console.aws.amazon.com/lambda/ Or: AWS Management Console → Services → Lambda Create Function:\nClick the Create Function In the create setting use the following setting: Choose Author from scratch Name: dashboard-query Runtime: Python 3.12 Architecture: x86_64 Change default execution role: Use an existing role Choose dashboard-query-role Click Create Add code:\nIn the code editor copy and paste the codes below then click Deply: import boto3 import time import os import json athena = boto3.client(\u0026#39;athena\u0026#39;) RESOURCE_MAP = { \u0026#39;/logs/cloudtrail\u0026#39;: { \u0026#39;db\u0026#39;: \u0026#39;security_logs\u0026#39;, \u0026#39;table\u0026#39;: \u0026#39;processed_cloudtrail\u0026#39; }, \u0026#39;/logs/guardduty\u0026#39;: { \u0026#39;db\u0026#39;: \u0026#39;security_logs\u0026#39;, \u0026#39;table\u0026#39;: \u0026#39;processed_guardduty\u0026#39; }, \u0026#39;/logs/vpc\u0026#39;: { \u0026#39;db\u0026#39;: \u0026#39;security_logs\u0026#39;, \u0026#39;table\u0026#39;: \u0026#39;vpc_logs\u0026#39; }, \u0026#39;/logs/eni_logs\u0026#39;:{ \u0026#39;db\u0026#39;: \u0026#39;security_logs\u0026#39;, \u0026#39;table\u0026#39;: \u0026#39;eni_flow_logs\u0026#39; } } OUTPUT_BUCKET_NAME = os.environ.get(\u0026#34;ATHENA_OUTPUT_BUCKET\u0026#34;) REGION = os.environ.get(\u0026#34;REGION\u0026#34;) OUTPUT_BUCKET = f\u0026#39;s3://{OUTPUT_BUCKET_NAME}/\u0026#39; def lambda_handler(event, context): print(\u0026#34;Received event:\u0026#34;, json.dumps(event)) resource_path = event.get(\u0026#39;resource\u0026#39;) config = RESOURCE_MAP.get(resource_path) if not config: return api_response(400, {\u0026#39;error\u0026#39;: f\u0026#39;Unknown resource path: {resource_path}\u0026#39;}) database_name = config[\u0026#39;db\u0026#39;] table_name = config[\u0026#39;table\u0026#39;] query_params = event.get(\u0026#39;queryStringParameters\u0026#39;, {}) or {} if config[\u0026#39;table\u0026#39;] == \u0026#39;processed_cloudtrail\u0026#39;: query_string = f\u0026#34;\u0026#34;\u0026#34;SELECT * FROM {table_name} where \u0026#34;date\u0026#34; \u0026gt;= cast((current_date - interval \u0026#39;3\u0026#39; day) as varchar) order by eventtime desc\u0026#34;\u0026#34;\u0026#34; elif config[\u0026#39;table\u0026#39;] == \u0026#39;processed_guardduty\u0026#39;: query_string = f\u0026#34;\u0026#34;\u0026#34;SELECT * FROM {table_name} where \u0026#34;date\u0026#34; \u0026gt;= cast((current_date - interval \u0026#39;3\u0026#39; day) as varchar) order by date desc\u0026#34;\u0026#34;\u0026#34; elif config[\u0026#39;table\u0026#39;] == \u0026#39;vpc_logs\u0026#39;: query_string = f\u0026#34;\u0026#34;\u0026#34;SELECT * FROM {table_name} where \u0026#34;date\u0026#34; \u0026gt;= cast((current_date - interval \u0026#39;3\u0026#39; day) as varchar) order by timestamp desc\u0026#34;\u0026#34;\u0026#34; elif config[\u0026#39;table\u0026#39;] == \u0026#39;eni_flow_logs\u0026#39;: query_string = f\u0026#34;\u0026#34;\u0026#34;SELECT * FROM {table_name} where \u0026#34;date\u0026#34; \u0026gt;= cast((current_date - interval \u0026#39;3\u0026#39; day) as varchar) order by timestamp_str desc\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;Querying DB: {database_name}, Table: {table_name}, Output: {OUTPUT_BUCKET}\u0026#34;) try: response = athena.start_query_execution( QueryString=query_string, QueryExecutionContext={\u0026#39;Database\u0026#39;: database_name}, ResultConfiguration={\u0026#39;OutputLocation\u0026#39;: OUTPUT_BUCKET} ) query_execution_id = response[\u0026#39;QueryExecutionId\u0026#39;] status = \u0026#39;RUNNING\u0026#39; while status in [\u0026#39;RUNNING\u0026#39;, \u0026#39;QUEUED\u0026#39;]: response = athena.get_query_execution(QueryExecutionId=query_execution_id) status = response[\u0026#39;QueryExecution\u0026#39;][\u0026#39;Status\u0026#39;][\u0026#39;State\u0026#39;] if status in [\u0026#39;FAILED\u0026#39;, \u0026#39;CANCELLED\u0026#39;]: reason = response[\u0026#39;QueryExecution\u0026#39;][\u0026#39;Status\u0026#39;].get(\u0026#39;StateChangeReason\u0026#39;, \u0026#39;Unknown\u0026#39;) return api_response(500, {\u0026#39;error\u0026#39;: f\u0026#39;Query Failed: {reason}\u0026#39;}) time.sleep(1) results = athena.get_query_results(QueryExecutionId=query_execution_id) return api_response(200, results) except Exception as e: print(f\u0026#34;Error: {str(e)}\u0026#34;) return api_response(500, {\u0026#39;error\u0026#39;: str(e)}) def api_response(code, body): return { \u0026#34;statusCode\u0026#34;: code, \u0026#34;headers\u0026#34;: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Access-Control-Allow-Origin\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Access-Control-Allow-Methods\u0026#34;: \u0026#34;GET, OPTIONS\u0026#34; }, \u0026#34;body\u0026#34;: json.dumps(body) } "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.3-foundation-setup/5.3.2-set-up-s3-buckets-policies/","title":"Set up S3 buckets policies","tags":[],"description":"","content":"In this section, you will configure the bucket policy for the primary log bucket to allow CloudTrail, GuardDuty, and CloudWatch Logs to write logs.\nConfigure Bucket Policy Navigate to the primary log bucket: In S3 Console, click on incident-response-log-list-bucket-ACCOUNT_ID-REGION Open the Permissions tab:\nClick on the \u0026ldquo;Permissions\u0026rdquo; tab Scroll to Bucket policy:\nScroll down to the \u0026ldquo;Bucket policy\u0026rdquo; section Click \u0026ldquo;Edit\u0026rdquo; Paste the bucket policy: Copy the following JSON policy Important: Replace ACCOUNT_ID and REGION with your actual values in the policy { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowGuardDutyPutObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;guardduty.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:PutObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:SourceAccount\u0026#34;: \u0026#34;ACCOUNT_ID\u0026#34; }, \u0026#34;ArnLike\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;arn:aws:guardduty:REGION:ACCOUNT_ID:detector/*\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowGuardDutyGetBucketLocation\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;guardduty.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:SourceAccount\u0026#34;: \u0026#34;ACCOUNT_ID\u0026#34; }, \u0026#34;ArnLike\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;arn:aws:guardduty:REGION:ACCOUNT_ID:detector/*\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCloudWatchLogsGetBucketAcl\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;logs.REGION.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:SourceAccount\u0026#34;: \u0026#34;ACCOUNT_ID\u0026#34; }, \u0026#34;ArnLike\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;arn:aws:logs:REGION:ACCOUNT_ID:log-group:*\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCloudWatchLogsPutObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;logs.REGION.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:PutObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:SourceAccount\u0026#34;: \u0026#34;ACCOUNT_ID\u0026#34; }, \u0026#34;ArnLike\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;arn:aws:logs:REGION:ACCOUNT_ID:log-group:*\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCloudTrailAclCheck\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudtrail.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:SourceAccount\u0026#34;: \u0026#34;ACCOUNT_ID\u0026#34; }, \u0026#34;ArnLike\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;arn:aws:cloudtrail:REGION:ACCOUNT_ID:trail/*\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCloudTrailWrite\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudtrail.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:PutObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/AWSLogs/ACCOUNT_ID/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;s3:x-amz-acl\u0026#34;: \u0026#34;bucket-owner-full-control\u0026#34;, \u0026#34;aws:SourceAccount\u0026#34;: \u0026#34;ACCOUNT_ID\u0026#34; }, \u0026#34;ArnLike\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;arn:aws:cloudtrail:REGION:ACCOUNT_ID:trail/*\u0026#34; } } } ] } Click \u0026ldquo;Save changes\u0026rdquo;\nVerify policy is saved: You should see the policy displayed in the Bucket policy section\n"},{"uri":"https://beforelights.github.io/AWS-Worklog/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Summary Report: “AWS Cloud Mastery Series #2 - DevOps on AWS” Event Overview The second session of the AWS Cloud Mastery Series focused on DevOps practices and tools available on AWS. The event aimed to provide attendees with a solid understanding of DevOps principles, Infrastructure as Code (IaC), container services, and monitoring solutions to build scalable, reliable, and maintainable cloud-based systems.\nEvent Objectives The primary goals of the event were to:\nIntroduce AWS DevOps services, including CI/CD pipelines. Explore Infrastructure as Code (IaC) concepts and tools such as AWS CloudFormation, AWS CDK, and Terraform. Provide insights into container services on AWS, including Amazon ECS, EKS, and App Runner. Highlight the importance of monitoring and observability using AWS services like CloudWatch and AWS X-Ray. Speakers The event featured a panel of experienced AWS Community Builders and cloud professionals:\nTruong Quang Tinh – Platform Engineer, TymeX Bao Huynh – AWS Community Builder Nguyen Khanh Phuc Thinh – AWS Community Builder Tran Dai Vi – AWS Community Builder Huynh Hoang Long – AWS Community Builder Pham Hoang Quy – AWS Community Builder Nghiem Le – AWS Community Builder Dinh Le Hoang Anh – Cloud Engineer Trainee, First Cloud AI Journey Key Highlights DevOps Mindset The session began with an introduction to the DevOps culture and its core principles:\nCollaboration: Breaking down silos between development and operations teams. Automation: Streamlining repetitive tasks to improve efficiency. Continuous Learning: Encouraging experimentation and learning from failures. Measurement: Using metrics to track progress and optimize processes. Key Roles in DevOps:\nDevOps Engineer Cloud Engineer Platform Engineer Site Reliability Engineer (SRE) Success Metrics:\nDeployment health and frequency System stability and reliability Improved agility and customer experience Justification of technology investments Best Practices:\nDO DON\u0026rsquo;T Start with fundamentals Stay in tutorial hell Learn by building real projects Copy-paste blindly Document everything Compare your progress to others Master one thing at a time Give up after failures Soft skills enhancement - Continuous Integration: Team members integrate their work frequently, aims for continuous Delivery and Deployment\nInfrastructure as Code (IaC) The event provided an in-depth look into Infrastructure as Code (IaC) and its benefits:\nAutomation: Reducing manual intervention and potential errors. Scalability: Easily scaling infrastructure up or down based on demand. Reproducibility: Ensuring consistent environments from development to production. Collaboration: Enhancing teamwork between development and operations. AWS CloudFormation: AWS\u0026rsquo;s native IaC tool, allowing users to define and provision AWS infrastructure using a declarative template format (YAML or JSON).\nStack: A collection of AWS resources managed as a single unit. CloudFormation Template: A file that describes the infrastructure resources needed for a specific application or service. Drift Detection: Identifying and correcting differences between the stack\u0026rsquo;s actual configuration and its expected configuration. AWS Cloud Development Kit (CDK): An open-source software development framework that allows users to define cloud infrastructure using a programming language of their choice.\nConstruct: The basic building block of AWS CDK, representing a single unit of deployment. AWS Amplify: A development platform for building secure, scalable mobile and web applications.\nTerraform: An open-source IaC tool that enables users to define and provision data center infrastructure using a high-level configuration language.\nStrengths: Multi-cloud support, state management, and a large module ecosystem. Criteria for Choosing IaC Tools:\nProject requirements: single cloud vs. multi-cloud Team expertise: developer-focused or operations-focused Ecosystem and community support Container Services on AWS The event covered various container services offered by AWS and how they fit into the DevOps landscape:\nDocker: A platform for developing, shipping, and running applications in containers. Amazon ECR (Elastic Container Registry): A fully managed Docker container registry that makes it easy to store, manage, and deploy Docker container images. Amazon ECS (Elastic Container Service): A fully managed container orchestration service that supports Docker containers. Amazon EKS (Elastic Kubernetes Service): A managed service that simplifies running Kubernetes on AWS without needing to install and operate your own Kubernetes control plane or nodes. AWS App Runner: A service that makes it easy to quickly deploy containerized web applications and APIs. Container Orchestration with ECS and EKS:\nECS: AWS\u0026rsquo;s native container orchestration service, tightly integrated with other AWS services. EKS: A managed Kubernetes service, providing the flexibility and control of Kubernetes without the operational overhead. Monitoring \u0026amp; Observability The final session focused on monitoring and observability solutions available on AWS:\nAmazon CloudWatch: A monitoring and observability service that provides data and actionable insights to monitor applications, respond to system-wide performance changes, and optimize resource utilization. AWS X-Ray: A service that helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. Key Features:\nCloudWatch Metrics: Monitor and collect metrics from AWS resources and applications. CloudWatch Alarms: Automatically react to changes in your AWS resources. CloudWatch Logs: Monitor, store, and access log files from Amazon EC2 instances, AWS CloudTrail, and other sources. X-Ray Tracing: Analyze and debug distributed applications, providing insights into performance bottlenecks and errors. Event Experience This event was very important for our project as it tackle our plan of adding IaC using CDK, instead of using ClickOps for maintainability and reproducibility. Also some more insights on CloudWatch helped greatly with our data monitoring feature\nThe speakers answered our team\u0026rsquo;s question:\nQ: Our project up until now have been purely built with ClickOps, and we are planning to use CDK. Are there any tool that could scan and turn our existing infrastructure into CDK or CloudFormation rather than reproducing the infrastructure from scratch with IaC?\nA: Unfortunately no, there isn\u0026rsquo;t a tool that can assist with that problem yet, your team is going to have to built the infrastructure from scratch again. If there by any chance that you do found a tool that can assist with please share with us too.\nQ: We noticed that AWS X-Ray used with CloudWatch is similar to CloudTrail in its tracing method, can you explain more on what differentiate them?\nA: X-Ray is used for CloudWatch and used to trail the resources and services that the system interacted with, meanwhile CloudTrail is commonly used to trail the AWS user\u0026rsquo;s actions\nQ: Our project is built around Guard Duty Findings, do you have any experiences on how to reliably trigger Findings for a demo scenarios?\nA: In my experience I know that Guard Duty Findings can be triggered by port scanning activities but i\u0026rsquo;m sure there are other ways too\nA: Guard Duty can be configured to have a threat list containing custom rules to trigger findings upon activities relating the configured malicious domains or IPs\nThis event is also the first time some of the speaker\u0026rsquo;s first time presenting a topic:\nThe DevOps and IaC sections was well presented Monitoring \u0026amp; Observability wasn\u0026rsquo;t as great and we can notice the speaker\u0026rsquo;s nervousness but still delivered great values regardless Some event photos "},{"uri":"https://beforelights.github.io/AWS-Worklog/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Learn VM Import/Export workflows to migrate on-premises VMs to AWS. Deploy a multi-tier web application (FCJ Management) with High Availability. Implement Auto Scaling Groups and Application Load Balancers for resilience. Understand AWS CLI advanced usage for import tasks and IAM role configuration. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Completed Understanding Research Methods in Coursera for my other subject (ENW493c) 22/09/2025 22/09/2025 3 - Can\u0026rsquo;t access to Lightsail as of now so I\u0026rsquo;m skipping - Completed Research Methodologies in Coursera (ENW493c) - Completed Giáo dục và Phát triển nguồn nhân lực số MOOC in Coursera (KS57 Program) 23/09/2025 23/09/2025 4 - Completed end-to-end VM Import/Export between VMware Workstation and Amazon EC2. + Created an Ubuntu virtual machine using VMware Workstation and configured OpenSSH for remote access inside the guest OS. + Exported the VM from VMware Workstation as an OVF/VMDK package to prepare it for migration. + Created an S3 import bucket in the AWS Management Console and uploaded the VMDK file from the local machine to Amazon S3. + Installed and configured the AWS CLI on the workstation, then created the vmimport IAM role (trust policy + permissions) using aws iam create-role and aws iam put-role-policy. + Used aws ec2 import-image from AWS CLI to convert the VMDK object in S3 into an AMI (WARNING!!!: Use older 5.x LTS versions.), then launched an EC2 instance from the imported AMI in the EC2 console. + Created an S3 export bucket and configured its bucket ACL in the S3 console so the vm-import-export@amazon.com service account can write exported VM images. + Ran aws ec2 create-instance-export-task from AWS CLI to export an EC2 instance/AMI to the S3 export bucket as a VM image for reuse in VMware or other hypervisors. 24/09/2025 24/09/2025 Installing or updating to the latest version of the AWS CLI Authenticating using IAM user credentials for the AWS CLI VM Import/Export Requirements VIRTUAL MACHINE (VM) IMPORT/EXPORT – AWS Study Group VM Import/Export – What it is and capabilities VM Import/Export Requirements and supported formats Import a VM to Amazon EC2 as an image using VM Import/Export Export an EC2 instance as a VM using VM Import/Export Required permissions for VM Import/Export (vmimport role) 5 Designed and deployed the FCJ Management application stack on AWS using EC2, RDS, and an Application Load Balancer.\n+ Reviewed the FCJ Management with Auto Scaling Group workshop objectives and overall architecture to understand required AWS components and traffic flow.\n+ Built the network infrastructure by creating a dedicated VPC, public subnets, an Internet Gateway, and configuring route tables for internet access to the FCJ application.\n+ Launched a FCJ EC2 application server from an Amazon Linux/Ubuntu AMI, attached a security group with HTTP/SSH rules, and verified basic connectivity via public IP.\n+ Installed and configured Node.js, npm, and PM2 on the EC2 instance, cloned the FCJ Management application source, and set it up as a managed PM2 process for resilience.\n+ Provisioned an Amazon RDS instance for the FCJ database, selected the required engine and instance class, configured networking/security groups, and enabled connectivity from the FCJ EC2 instance.\n+ Initialized the FCJ database schema and loaded sample data by running SQL scripts so that the application could perform end‑to‑end CRUD operations during testing.\n+ Verified full stack connectivity by starting the FCJ application on EC2, connecting to RDS, and confirming that core features worked over HTTP before introducing any load balancing or scaling. 25/09/2025 26/09/2025 Deploying FCJ Management with Auto Scaling Group – AWS Study Group\nPreparation - Deploying FCJ Management with Auto Scaling Group\nDeploy Web Server\nLaunch a Database Instance with RDS\nSetup data for Database 6 Implemented high availability and auto scaling for the FCJ Management application using Launch Templates, ALB, and multiple scaling strategies.\n+ Created an AMI from the configured FCJ EC2 instance and built an EC2 Launch Template capturing AMI ID, instance type, networking, security groups, and user data for repeatable deployments.\n+ Configured an Application Load Balancer and Target Group, registered the FCJ instances as targets, and validated health checks and routing via the ALB DNS name.\n+ Set up an EC2 Auto Scaling Group (ASG) using the Launch Template, attached it to the Target Group, and confirmed that new instances joined the ALB and served traffic correctly.\n+ Implemented manual scaling by adjusting ASG desired capacity, observed new FCJ instances launching, and validated load distribution and health status through the ALB.\n+ Configured scheduled scaling actions to automatically increase capacity during a defined “peak” window and decrease it afterward, then tested that the schedule executed as expected.\n+ Implemented dynamic (target‑tracking) scaling based on metrics such as CPU utilization or request count, ran load tests through the ALB, and observed automatic scale‑out and scale‑in behavior.\n+ Prepared custom CloudWatch metrics for predictive scaling, created a predictive scaling policy on the ASG, and reviewed forecast vs. actual capacity graphs to understand how future scaling decisions are generated.\n+ Performed end‑to‑end testing of all scaling modes (manual, scheduled, dynamic, predictive), monitored instance lifecycle events and metrics, and documented cost considerations for EC2, RDS, and ALB resources. 26/09/2025 26/09/2025 Create Launch Template\nSetting Up Load Balancer\nCreate Auto Scaling Group\nTest - Deploying FCJ Management with Auto Scaling Group\nTest solutions - Deploying FCJ Management with Auto Scaling Group\nRead metrics of predictive scaling solution Week 3 Achievements: Successfully imported a local Ubuntu VM into AWS as an AMI using vmimport roles and AWS CLI. Deployed the FCJ Management App with a secure VPC architecture (Public/Private Subnets). Configured an Auto Scaling Group behind an ALB, proving the ability to scale out during demand spikes. Verified database connectivity between the EC2 instances and the RDS database layer secure within private subnets. "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.7-dashboard-setup/5.7.3-setup-api-gateway/","title":"API Gateway Setup","tags":[],"description":"","content":"In this guide, you will setup an API Gateway to route api call from dashboard to Lambda.\nCreate API Gateway Open the API Gateway Console\nNavigate to https://console.aws.amazon.com/apigateway/ Or: AWS Management Console → Services → API Gateway Create API:\nClick Create API Choose REST API and click Build Use this setting for creation: Choose New API Name: dashboard-api API endpoint type: Regional Security policy: SecurityPolicy_TLS13_1_3_2025_09 Endpoint access mode: Basic IP address type: IPv4 Create Resources:\nEnable CORS for the root resource Click Create resource and name it logs Then click on /logs resource that just created and click Create Resource to create child resource of /logs Name it cloudtrail and enable CORS Repeat this three more times for eni_logs, guardduty and vpc Create methods:\nClick on /cloudtrail that just created and click Cretae method\nIn method creation, use this setting:\nMethod type: GET Intergration type: Lambda function Enable Lambda proxy intergration choose Buffered Lambda function: select your region search for dashboard-query and choose it Timout: 29000 Repeat this three more time for eni_logs, guardduty and vpc\nDeploy API:\nClick the Deploy API on the right corner In deploy API, use this setting: Stage: New stage Name: prod Click Deploy "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.11-appendices/5.11.3-cloudwatch-etl/","title":"CloudWatch ETL Code","tags":[],"description":"","content":"import json import boto3 import gzip import re import os from datetime import datetime, timezone s3 = boto3.client(\u0026#34;s3\u0026#34;) firehose= boto3.client(\u0026#34;firehose\u0026#34;) # -------------------------------------------------- # CONFIG # -------------------------------------------------- SOURCE_PREFIX = \u0026#34;exportedlogs/vpc-dns-logs/\u0026#34; FIREHOSE_STREAM_NAME = os.environ.get(\u0026#34;FIREHOSE_STREAM_NAME\u0026#34;) VPC_RE = re.compile(r\u0026#34;/(vpc-[0-9A-Za-z\\-]+)\u0026#34;) ISO_TS_RE = re.compile(r\u0026#34;^\\d{4}-\\d{2}-\\d{2}T\u0026#34;) def read_gz(bucket, key): obj = s3.get_object(Bucket=bucket, Key=key) with gzip.GzipFile(fileobj=obj[\u0026#34;Body\u0026#34;]) as f: return f.read().decode(\u0026#34;utf-8\u0026#34;, errors=\u0026#34;replace\u0026#34;) def flatten_once(d): out = {} for k, v in (d or {}).items(): if isinstance(v, dict): for k2, v2 in v.items(): out[f\u0026#34;{k}_{k2}\u0026#34;] = v2 else: out[k] = v return out def safe_int(x): try: return int(x) except: return None def parse_dns_line(line): raw = line.strip() if not raw: return None json_part = raw prefix_ts = None if ISO_TS_RE.match(raw): try: prefix_ts, rest = raw.split(\u0026#34; \u0026#34;, 1) json_part = rest except: pass if not json_part.startswith(\u0026#34;{\u0026#34;): idx = json_part.find(\u0026#34;{\u0026#34;) if idx != -1: json_part = json_part[idx:] try: obj = json.loads(json_part) except: return None flat = flatten_once(obj) if prefix_ts: flat[\u0026#34;_prefix_ts\u0026#34;] = prefix_ts return flat def lambda_handler(event, context): print(f\u0026#34;Received S3 Event. Records: {len(event.get(\u0026#39;Records\u0026#39;, []))}\u0026#34;) firehose_records = [] for record in event.get(\u0026#34;Records\u0026#34;, []): if \u0026#34;s3\u0026#34; not in record: continue bucket = record[\u0026#34;s3\u0026#34;][\u0026#34;bucket\u0026#34;][\u0026#34;name\u0026#34;] key = record[\u0026#34;s3\u0026#34;][\u0026#34;object\u0026#34;][\u0026#34;key\u0026#34;] if not key.startswith(SOURCE_PREFIX) or not key.endswith(\u0026#34;.gz\u0026#34;): print(f\u0026#34;Skipping file: {key}\u0026#34;) continue print(f\u0026#34;Processing S3 file: {key}\u0026#34;) # Extract VPC ID from file path vpc_id_match = VPC_RE.search(key) vpc_id = vpc_id_match.group(1) if vpc_id_match else \u0026#34;unknown\u0026#34; # Read and process file content content = read_gz(bucket, key) if not content: continue for line in content.splitlines(): r = parse_dns_line(line) if not r: continue # Create flattened JSON record out = { \u0026#34;version\u0026#34;: r.get(\u0026#34;version\u0026#34;), \u0026#34;account_id\u0026#34;: r.get(\u0026#34;account_id\u0026#34;), \u0026#34;region\u0026#34;: r.get(\u0026#34;region\u0026#34;), \u0026#34;vpc_id\u0026#34;: r.get(\u0026#34;vpc_id\u0026#34;, vpc_id), \u0026#34;query_timestamp\u0026#34;: r.get(\u0026#34;query_timestamp\u0026#34;), \u0026#34;query_name\u0026#34;: r.get(\u0026#34;query_name\u0026#34;), \u0026#34;query_type\u0026#34;: r.get(\u0026#34;query_type\u0026#34;), \u0026#34;query_class\u0026#34;: r.get(\u0026#34;query_class\u0026#34;), \u0026#34;rcode\u0026#34;: r.get(\u0026#34;rcode\u0026#34;), \u0026#34;answers\u0026#34;: json.dumps(r.get(\u0026#34;answers\u0026#34;), ensure_ascii=False), \u0026#34;srcaddr\u0026#34;: r.get(\u0026#34;srcaddr\u0026#34;), \u0026#34;srcport\u0026#34;: safe_int(r.get(\u0026#34;srcport\u0026#34;)), \u0026#34;transport\u0026#34;: r.get(\u0026#34;transport\u0026#34;), \u0026#34;srcids_instance\u0026#34;: r.get(\u0026#34;srcids_instance\u0026#34;), \u0026#34;timestamp\u0026#34;: (r.get(\u0026#34;query_timestamp\u0026#34;) or r.get(\u0026#34;timestamp\u0026#34;) or r.get(\u0026#34;_prefix_ts\u0026#34;)) } # Add newline for JSONL format json_row = json.dumps(out, ensure_ascii=False) + \u0026#34;\\n\u0026#34; firehose_records.append({\u0026#39;Data\u0026#39;: json_row}) # Send to Firehose in batches of 500 if firehose_records: total_records = len(firehose_records) print(f\u0026#34;Sending {total_records} records to Firehose...\u0026#34;) batch_size = 500 for i in range(0, total_records, batch_size): batch = firehose_records[i:i + batch_size] try: response = firehose.put_record_batch( DeliveryStreamName=FIREHOSE_STREAM_NAME, Records=batch ) if response[\u0026#39;FailedPutCount\u0026#39;] \u0026gt; 0: print(f\u0026#34;Warning: {response[\u0026#39;FailedPutCount\u0026#39;]} records failed\u0026#34;) except Exception as e: print(f\u0026#34;Firehose error: {e}\u0026#34;) return {\u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;, \u0026#34;total_records\u0026#34;: len(firehose_records)} "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.3-foundation-setup/5.3.3-create-iam-roles-and-policies/5.3.3.3-create-iam-policy/","title":"Create IAM Policy","tags":[],"description":"","content":"Create IAM Quarantine Policy Create IrQuarantineIAMPolicy Navigate to IAM Console → Policies → Create policy\nPolicy JSON:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Policy name: IrQuarantineIAMPolicy Description: Deny-all policy for quarantining compromised IAM users "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.3-foundation-setup/5.3.3-create-iam-roles-and-policies/","title":"Create IAM Roles and Policies","tags":[],"description":"","content":"In this section, you will create 17 IAM roles with their associated policies for Lambda functions, Firehose streams, Step Functions, and other services.\nOverview of IAM Roles Lambda Execution Roles (9 roles):\nCloudTrailETLLambdaServiceRole GuardDutyETLLambdaServiceRole CloudWatchETLLambdaServiceRole CloudWatchENIETLLambdaServiceRole CloudWatchExportLambdaServiceRole ParseFindingsLambdaServiceRole IsolateEC2LambdaServiceRole QuarantineIAMLambdaServiceRole AlertDispatchLambdaServiceRole Service Roles (6 roles): 10. CloudTrailFirehoseRole 11. CloudWatchFirehoseRole 12. StepFunctionsRole 13. IncidentResponseStepFunctionsEventRole 14. FlowLogsIAMRole 15. GlueCloudWatchRole\nIAM Policy (1 policy): 16. IrQuarantineIAMPolicy\nContent Create Lambda Execution Roles Create Service Roles Create IAM Policy "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.5-processing-setup/5.5.3-create-lambda-function-etl-processing/","title":"Create Lambda Function - ETL Processing","tags":[],"description":"","content":"Create Lambda Functions - ETL Processing In this section, you will create 5 Lambda functions that process logs and send them to Kinesis Firehose or S3.\nincident-response-cloudtrail-etl Runtime: Python 3.12 Handler: CloudTrailETL.lambda_handler Role: CloudTrailETLLambdaServiceRole Timeout: 300s, Memory: 128MB Env: FIREHOSE_STREAM_NAME=cloudtrail-firehose-stream Code: cloudtrail-etl incident-response-guardduty-etl Runtime: Python 3.12 Handler: guardduty_etl.lambda_handler Role: GuardDutyETLLambdaServiceRole Timeout: 300s, Memory: 128MB Env: DESTINATION_BUCKET, S3_LOCATION_GUARDDUTY, DATABASE_NAME, TABLE_NAME_GUARDDUTY Code: guardduty-etl cloudwatch-etl-lambda Runtime: Python 3.12 Handler: cloudwatch_etl.lambda_handler Role: CloudWatchETLLambdaServiceRole Env: FIREHOSE_STREAM_NAME=vpc-dns-firehose-stream Code: cloudwatch-etl cloudwatch-eni-etl-lambda Runtime: Python 3.12 Handler: cloudwatch_eni_etl.lambda_handler Role: CloudWatchENIETLLambdaServiceRole Env: FIREHOSE_STREAM_NAME=vpc-flow-firehose-stream Code: cloudwatch-eni-etl cloudwatch-export-lambda Runtime: Python 3.12 Handler: cloudwatch_autoexport.lambda_handler Role: CloudWatchExportLambdaServiceRole Env: DESTINATION_BUCKET=incident-response-log-list-bucket-ACCOUNT_ID-REGION Code: cloudwatch-autoexport Configure CloudWatch Logs Subscription Filter Configure Subscription Filter Open the CloudWatch Console.\nIn the left navigation pane, select Log Management.\nClick on the centralized log group: /aws/incident-response/centralized-logs.\nCreate Subscription Filter:\nClick the \u0026ldquo;Subscription filters\u0026rdquo; tab. Click \u0026ldquo;Create Lambda subscription filter\u0026rdquo;. Configure Destination:\nDestination Lambda function: Select the function cloudwatch-export-lambda. Log format: Select \u0026ldquo;Other\u0026rdquo;. (This ensures the raw log data is passed efficiently for Lambda processing). Configure Log Format and Filter:\nSubscription filter name: Enter a descriptive name, e.g., VPC-Log-Export-Filter. Filter pattern: Leave this field blank. (Ensures all logs in the group are processed). Click \u0026ldquo;Start streaming\u0026rdquo;.\nConfigure S3 Event Notifications S3 Console → incident-response-log-list-bucket-ACCOUNT_ID-REGION → Properties → Event notifications\nCreate 4 notifications with Event types/Object creation/✅All object create events:\nCloudTrailETLTrigger: Prefix AWSLogs/ACCOUNT_ID/CloudTrail/ → Lambda incident-response-cloudtrail-etl VPCDNSLogsTrigger: Prefix exportedlogs/vpc-dns-logs/ → Lambda cloudwatch-etl-lambda VPCFlowLogsTrigger: Prefix exportedlogs/vpc-flow-logs/ → Lambda cloudwatch-eni-etl-lambda GuardDutyFindingsTrigger: Prefix AWSLogs/ACCOUNT_ID/GuardDuty/ → Lambda incident-response-guardduty-etl "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.3-foundation-setup/","title":"Foundation Setup","tags":[],"description":"","content":"This initial Foundation Setup phase establishes the core prerequisites for the Auto Incident Response System, concentrating on the deployment of dedicated storage and essential security authorization. This mandates the creation of five secure Amazon S3 buckets for centralized log ingestion and processing, applying a necessary Bucket Policy for secure log delivery, and defining 17 IAM roles and a quarantine policy to enforce least-privilege access across all integrated AWS services.\nContent Set up Amazon S3 Bucket Configure S3 Bucket Policy for Primary Log Bucket Create IAM Roles and Policies "},{"uri":"https://beforelights.github.io/AWS-Worklog/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section lists and introduces the blogs that I have translated:\nBlog 1 - Building a High-Performance Exchange Market Data Broadcasting Platform on AWS This blog explores how SMC Global Securities Ltd. modernized its trading infrastructure using AWS. It details the implementation of a robust financial data broadcasting system leveraging AWS Transit Gateway, Fortinet SD-WAN, and multicast technology to handle real-time market data efficiently.\nBlog 2 - Optimize Security Operations with AWS Security Incident Response This blog provides insights into enhancing security operations using AWS Security Incident Response. It covers the integration of services like Amazon GuardDuty and AWS Security Hub to detect, analyze, and respond to security threats effectively.\nBlog 3 - Unlocking the Value of Unstructured Data with Amazon Bedrock Data Automation This blog discusses how Amazon Bedrock Data Automation helps organizations manage unstructured data. It highlights the automation of data storage, governance, and extraction of insights from diverse data formats like images, audio, and video.\n"},{"uri":"https://beforelights.github.io/AWS-Worklog/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"Summary Report: “AWS Cloud Mastery Series #3: AWS Well-Architected – Security Pillar Workshop” Event Overview The third session of the AWS Cloud Mastery Series focused on the AWS Well-Architected Framework, specifically the Security Pillar. This workshop provided attendees with a deep dive into the five key areas of cloud security: Identity and Access Management (IAM), Detection and Continuous Monitoring, Infrastructure Protection, Data Protection, and Incident Response. The event also introduced the AWS Cloud Club initiative, which aims to foster cloud computing skills and build a strong community of cloud professionals.\nEvent Objectives The workshop aimed to achieve the following objectives:\nIntroduce the AWS Cloud Club initiative and its benefits for students and professionals. Provide a detailed understanding of the five pillars of cloud security: Identity and Access Management (IAM) Detection and Continuous Monitoring Infrastructure Protection Data Protection Incident Response Share best practices for implementing security measures on AWS. Highlight real-world use cases and advanced security solutions. Speakers The event featured a diverse panel of AWS Cloud Club Captains, AWS Community Builders, and industry professionals, including:\nLe Vu Xuan An – AWS Cloud Club Captain, HCMUTE Tran Duc Anh – AWS Cloud Club Captain, SGU Tran Doan Cong Ly – AWS Cloud Club Captain, PTIT Danh Hoang Hieu Nghi – AWS Cloud Club Captain, HUFLIT Huynh Hoang Long – AWS Community Builder Dinh Le Hoang Anh – AWS Community Builder Van Hoang Kha – Cloud Security Engineer, AWS Community Builder Mendel Grabski (Long) – Ex-Head of Security \u0026amp; DevOps, Cloud Security Solution Architect Tinh Truong – Platform Engineer, TymeX, AWS Community Builder Key Highlights 1. AWS Cloud Club Initiative The event began with an introduction to the AWS Cloud Club, a program designed to:\nHelp students and professionals explore and grow their cloud computing skills. Develop technical leadership and mentorship opportunities. Build meaningful connections within the global AWS community. Benefits of Joining AWS Cloud Club:\nHands-on AWS experiences through workshops and projects. Mentorship from AWS professionals. Long-term career support and networking opportunities. Participating AWS Cloud Clubs:\nHCMUTE SGU PTIT HUFLIT 2. Identity \u0026amp; Access Management (IAM) IAM is a foundational AWS service for managing secure access to AWS resources. The session covered:\nCore Concepts: Users, Groups, Roles, and Policies. Authentication and Authorization. Best Practices: Apply the Principle of Least Privilege. Delete root access keys after account creation. Avoid using wildcards (*) in policies. Use AWS Single Sign-On (SSO) for centralized access management. Advanced IAM Features:\nService Control Policies (SCPs): Organization-wide policies that define the maximum permissions available to accounts. Permission Boundaries: Limit the maximum permissions a user or role can have. Multi-Factor Authentication (MFA): TOTP (Time-based One-Time Password): Requires manual entry of a 6-digit code. FIDO2 (Fast Identity Online): Uses biometric scans or hardware tokens for authentication. Credential Rotation with AWS Secrets Manager:\nAutomates the rotation of credentials using a 4-step process: Create, Set, Test, and Finish Secret. Integrates with EventBridge for scheduling and automation. 3. Detection \u0026amp; Continuous Monitoring This session emphasized the importance of real-time visibility and proactive threat detection:\nMulti-Layer Security Visibility: Management Events: API calls and console actions. Data Events: S3 object access and Lambda executions. Network Activity Events: VPC Flow Logs for network-level monitoring. Event-Driven Architecture with EventBridge: Real-time event processing for automated alerting and response. Integration with Lambda, SNS, and SQS for security workflows. Detection-as-Code: Use CloudTrail Lake queries for advanced threat hunting. Version-controlled detection rules managed in code repositories. 4. GuardDuty – Intelligent Threat Detection GuardDuty provides continuous threat detection using three key data sources:\nData Source What It Monitors Example CloudTrail Events IAM actions, permission changes Disabling logging to cover tracks. VPC Flow Logs Network traffic EC2 sending data to a botnet C2 server. DNS Logs DNS queries Malware querying cryptomining domains. Advanced Protection Plans:\nS3 Protection: Detects abnormal access patterns and scans for malware. EKS Protection: Monitors Kubernetes audit logs for unauthorized access. Malware Protection: Scans EBS volumes for compromised instances. RDS Protection: Detects brute-force attacks on databases. Lambda Protection: Monitors network traffic from Lambda functions. 5. Infrastructure Protection The session covered key network security controls:\nSecurity Groups (SG): Stateful firewalls at the instance level. Network ACLs (NACLs): Stateless firewalls at the subnet level. AWS Network Firewall: Provides intrusion prevention and egress filtering. 6. Data Protection \u0026amp; Governance The session highlighted AWS services for securing data:\nEncryption with KMS: Protects data using customer-managed keys. AWS Secrets Manager: Automates credential rotation. Certificate Management with ACM: Provides free public certificates with automatic renewal. 7. Incident Response The final session focused on preparing for and responding to security incidents:\nBest Practices: Use temporary credentials. Avoid exposing S3 buckets directly. Manage infrastructure through IaC. Incident Response Process: Preparation Detection \u0026amp; Analysis Containment Eradication \u0026amp; Recovery Post-Incident Review Event Experience This workshop was highly relevant to our project on Automated Incident Response and Forensics. Key takeaways included:\nGuardDuty Latency: Findings take up to 5 minutes due to the large dataset analysis. Third-party tools like Open Clarity can provide near real-time findings. Support from Experts: Mendel Grabski offered valuable insights and expressed interest in supporting our project. Event Photos Group photo with Mendel Grabski and Van Hoang Kha. "},{"uri":"https://beforelights.github.io/AWS-Worklog/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Build custom CloudWatch Dashboards to visualize logs and metrics. Implement cost optimization automation using Lambda and EventBridge. Perform heterogeneous database migration (MSSQL to MySQL) using AWS SCT. Finalize team project proposal and estimate resource costs. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Built a CloudWatch dashboard to visualize key metrics and alarms from the AWS CloudWatch workshop.\n+ Opened the CloudWatch console, navigated to Dashboards, and created a new custom dashboard with a descriptive name.\n+ Added metric widgets displaying the custom error-count metric generated from CloudWatch Logs metric filters, tuning the period and statistic for readability.\n+ Placed alarm widgets on the dashboard to show the real-time state of the log-derived error alarm, linking logs, metrics, and alarms in one view.\n+ Arranged and resized widgets on the 24-cell grid to highlight critical observability components and saved the dashboard for reuse. 29/09/2025 29/09/2025 AWS CloudWatch Workshop :: AWS Account Setup\nCloudWatch Dashboards :: AWS Account Setup\nUsing Amazon CloudWatch dashboards - AWS Documentation\nCreating a customized CloudWatch dashboard - AWS Documentation 3 - Completed Being a researcher (in Information Science and Technology) in Coursera (ENW493c) 30/09/2025 30/09/2025 4 - Completed Advanced Writing in Coursera (ENW493c) 01/10/2025 01/10/2025 5 Automated EC2 cost optimization using Lambda functions, tags, and Slack notifications for start/stop control.\n+ Created a dedicated VPC, subnets, and security group to host a lab EC2 instance targeted for automated scheduling.\n+ Launched and tagged an EC2 instance with a cost-optimization tag (for example, environment_auto) so only specific instances are affected by automation.\n+ Configured a Slack Incoming Webhook and channel to receive real-time notifications from AWS Lambda about EC2 start/stop actions.\n+ Created an IAM execution role for Lambda with permissions to describe, start, and stop tagged EC2 instances and write logs.\n+ Implemented two Lambda functions (start/stop) that filter EC2 instances by tag, call StartInstances/StopInstances APIs, and post structured messages to Slack.[10]\n+ Tested the workflow end-to-end by invoking the functions, validating EC2 state changes in the console, and confirming Slack notifications as the basis for a scheduled solution using EventBridge/CloudWatch Events.\n- Discussed with the team on what to do with our project and how we can implement Slack 02/10/2025 02/10/2025 Optimize EC2 cost with Lambda - AWS Study Group\nDefining Lambda function permissions with an execution role\nHow to Schedule EC2 Instances to Stop/Start Automatically\nUse AWS Lambda to send Slack notifications for running Amazon EC2 instances 6 - Participated in the AI-Driven Development Life Cycle: Reimagining Software Engineering event 03/10/2025 04/10/2025 Week 4 Achievements: Designed and deployed a custom CloudWatch Dashboard to monitor application health and error rates. Developed a serverless cost-optimization solution using Lambda to automatically stop/start EC2 instances, integrated with Slack notifications. Successfully migrated a legacy MSSQL database to Amazon MySQL using the AWS Schema Conversion Tool (SCT), resolving compatibility issues. Collaborated with the team to finalize the capstone project proposal and secured admin access for all members. "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.7-dashboard-setup/5.7.4-setup-cloudfront/","title":"Cloudfront Setup","tags":[],"description":"","content":"In this guide, you will setup a Cloudfront for cache, routing and web accessing.\nCreate Cloudfront Distribution Open the Cloudfront Console\nNavigate to https://console.aws.amazon.com/cloudfront/ Or: AWS Management Console → Services → Cloudfront Create Distribution:\nClick the Create distribution button In distribution creation, use this setting: Choose a plan: Free plan Name: Static Dashboard Website CloudFront Origin type: Amazom S3 S3 Origin: Choose the static-dashboard-bucket Keep the rest like default Enable security: Use this if you choose free plan Review and click Create distribution General setting:\nAfter creation complete, on your Cloudfront General tab click on Edit At the Default root object enter index.html Description: Static Dashboard Distribution Click Save change Create API Gateway origin:\nClick Origins on the menu tabs Then click Create origin In orogin creation, use this setting: Origin domain: choose dashboard-api Protocol: HTTPS only HTTPS port: 443 Minimum Origin SSL protocol: TLSv1.2 Origin path: /prod Click Create origin Create behaviors for API Gateway:\nClick Behaviors on the menu tabs Then click Create behavior In behavior creation, use this setting: Path pattern: /logs/* Origin and origin groups: choose dashboard-api Leave the rest setting like default Click Create behavior Update S3 policy to work with Cloudfront:\nClick Origins on the menu tabs, choose the s3-static-dashboard origin name Click Edit At Origin access controll section press Go to S3 bucket permissions Check if your S3 permission look like this, if don\u0026rsquo;t then copy and paste it to your S3 permission (Change the ACCOUNT_ID, ACCOUNT_REGION and CLOUDFRONT_ID to your): { \u0026#34;Version\u0026#34;: \u0026#34;2008-10-17\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;PolicyForCloudFrontPrivateContent\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCloudFrontServicePrincipal\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudfront.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::s3-static-dashboard-[ACCOUNT_ID]-[ACCOUNT_REGION]/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;ArnLike\u0026#34;: { \u0026#34;AWS:SourceArn\u0026#34;: \u0026#34;arn:aws:cloudfront::[ACCOUNT_ID]:distribution/[CLOUDFRONT_ID]\u0026#34; } } } ] } Click Save change Create error pages:\nClick Error pages on the menu tabs Click Create custom error page In custom error page creation, use this setting: HTTP error code: 403: Forbident Error caching minimum TTL: 300 Customize error response: Yes Response page path: /index.html HTTP Response code: 200: OK Repeat this for 404 code "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.11-appendices/5.11.4-cloudwatch-eni-etl/","title":"CloudWatch ENI ETL Code","tags":[],"description":"","content":" import json import boto3 import gzip import os from datetime import datetime s3 = boto3.client(\u0026#34;s3\u0026#34;) firehose = boto3.client(\u0026#34;firehose\u0026#34;) # -------------------------------------------------- # CONFIGURATION # -------------------------------------------------- FIREHOSE_STREAM_NAME = os.environ.get(\u0026#34;FIREHOSE_STREAM_NAME\u0026#34;) # ----------------------------- UTILS ----------------------------- def read_gz(bucket, key): obj = s3.get_object(Bucket=bucket, Key=key) with gzip.GzipFile(fileobj=obj[\u0026#34;Body\u0026#34;]) as f: return f.read().decode(\u0026#34;utf-8\u0026#34;, errors=\u0026#34;replace\u0026#34;) def safe_int(x): try: return int(x) except: return None def parse_flow_log_line(line): parts = line.strip().split(\u0026#39; \u0026#39;) if len(parts) \u0026lt; 14: return None try: start_timestamp = safe_int(parts[10]) time_str = None if start_timestamp: dt_object = datetime.fromtimestamp(start_timestamp) time_str = dt_object.strftime(\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;) record = { \u0026#34;version\u0026#34;: safe_int(parts[0]), # Cột 1: version (int) \u0026#34;account_id\u0026#34;: parts[1], # Cột 2: account_id (STRING) \u0026#34;interface_id\u0026#34;: parts[2], # Cột 3: eni-... \u0026#34;srcaddr\u0026#34;: parts[3], \u0026#34;dstaddr\u0026#34;: parts[4], \u0026#34;srcport\u0026#34;: safe_int(parts[5]), \u0026#34;dstport\u0026#34;: safe_int(parts[6]), \u0026#34;protocol\u0026#34;: safe_int(parts[7]), \u0026#34;packets\u0026#34;: safe_int(parts[8]), \u0026#34;bytes\u0026#34;: safe_int(parts[9]), \u0026#34;start_time\u0026#34;: start_timestamp, # Cột 11 \u0026#34;end_time\u0026#34;: safe_int(parts[11]), \u0026#34;action\u0026#34;: parts[12], \u0026#34;log_status\u0026#34;: parts[13], \u0026#34;timestamp_str\u0026#34;: time_str } return record except Exception as e: print(f\u0026#34;Error parsing line: {e}\u0026#34;) return None def lambda_handler(event, context): print(f\u0026#34;Received S3 Event. Records: {len(event.get(\u0026#39;Records\u0026#39;, []))}\u0026#34;) firehose_records = [] # Duyệt qua các file S3 gửi về for record in event.get(\u0026#34;Records\u0026#34;, []): if \u0026#34;s3\u0026#34; not in record: continue bucket = record[\u0026#34;s3\u0026#34;][\u0026#34;bucket\u0026#34;][\u0026#34;name\u0026#34;] key = record[\u0026#34;s3\u0026#34;][\u0026#34;object\u0026#34;][\u0026#34;key\u0026#34;] # Chỉ xử lý file .gz if not key.endswith(\u0026#34;.gz\u0026#34;): print(f\u0026#34;Skipping non-gz: {key}\u0026#34;) continue print(f\u0026#34;Processing: {key}\u0026#34;) # Đọc nội dung content = read_gz(bucket, key) if not content: continue # Parse từng dòng log for line in content.splitlines(): rec = parse_flow_log_line(line) if not rec: continue # Chuyển thành JSON string và thêm xuống dòng (\\n) json_row = json.dumps(rec) + \u0026#34;\\n\u0026#34; firehose_records.append({\u0026#39;Data\u0026#39;: json_row}) # Đẩy sang Firehose (Batching 500 dòng) if firehose_records: total = len(firehose_records) print(f\u0026#34;Flushing {total} records to Firehose...\u0026#34;) batch_size = 500 for i in range(0, total, batch_size): batch = firehose_records[i:i + batch_size] try: response = firehose.put_record_batch( DeliveryStreamName=FIREHOSE_STREAM_NAME, Records=batch ) if response[\u0026#39;FailedPutCount\u0026#39;] \u0026gt; 0: print(f\u0026#34;Warning: {response[\u0026#39;FailedPutCount\u0026#39;]} records failed.\u0026#34;) except Exception as e: print(f\u0026#34;Firehose API Error: {e}\u0026#34;) return {\u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;, \u0026#34;count\u0026#34;: len(firehose_records)} "},{"uri":"https://beforelights.github.io/AWS-Worklog/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in several events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AI-Driven Development Life Cycle: Reimagining Software Engineering\nDate \u0026amp; Time: September 18th, 2025\nLocation: Level 26, Bitexco Financial Tower, 2 Hai Trieu Street, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS\nDate \u0026amp; Time: November 15th, 2025\nLocation: Level 26, Bitexco Financial Tower, 2 Hai Trieu Street, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: AWS Cloud Mastery Series #2 - DevOps on AWS\nDate \u0026amp; Time: November 17th, 2025\nLocation: Level 26, Bitexco Financial Tower, 2 Hai Trieu Street, Ho Chi Minh City\nRole: Attendee\nEvent 4 Event Name: AWS Cloud Mastery Series #3: AWS Well-Architected – Security Pillar Workshop\nDate \u0026amp; Time: November 29th, 2025\nLocation: Level 26, Bitexco Financial Tower, 2 Hai Trieu Street, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.4-monitoring-setup/","title":"Monitoring Setup","tags":[],"description":"","content":"This Monitoring Setup phase activates and configures the three core log sources for threat detection. It involves enabling CloudTrail for comprehensive management and data events, activating GuardDuty to export security findings to the primary S3 bucket, and setting up VPC Flow Logs on your network to send all traffic metadata to the dedicated CloudWatch Log Group. This ensures a constant, centralized stream of log data is available for processing and automated response.\nCreate CloudWatch Log Group Open CloudWatch Console → Log Management → Create log group Configure:\nLog group name: /aws/incident-response/centralized-logs Retention: 90 days KMS key: None Click \u0026ldquo;Create\u0026rdquo;\nEnable AWS CloudTrail Open CloudTrail Console → Trail → Create trail Trail attributes:\nTrail name: incident-responses-cloudtrail-ACCOUNT_ID-REGION Storage location: Use existing S3 bucket S3 bucket: Choose your incident-response-log-list-bucket-ACCOUNT_ID-REGION Log file SSE-KMS encryption: Disable Log file validation: Enabled Click next Choose log events:\nEvents Choose Management events, Data events Management events: All (Read + Write) Data events: S3 - Log all events Click next till step 4 and Create Trail Advanced event selectors: Exlcude log buckets:\nClick the Trail then scroll down to Data Event and click Edit Setup like picture with the under format: -arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/\n-arn:aws:s3:::processed-guardduty-findings-ACCOUNT_ID-REGION/\n-arn:aws:s3:::processed-cloudtrail-logs-ACCOUNT_ID-REGION\n-arn:aws:s3:::athena-query-results-ACCOUNT_ID-REGION/\n-arn:aws:s3:::processed-cloudwatch-logs-ACCOUNT_ID-REGION/\nSave change Enable Amazon GuardDuty Open GuardDuty Console → Get Started → Enable GuardDuty\nConfigure settings:\nFinding export frequency: Update CWE and S3 every 15 minutes S3 export: incident-response-log-list-bucket-ACCOUNT_ID-REGION KMS encryption: Choose or create KMS key Enable VPC Flow Logs Open VPC Console → Your VPCs → Select your VPC\nActions → Create flow log\nConfigure:\nFilter: All Aggregation interval: 10 minutes Destination: CloudWatch Logs Log group: /aws/incident-response/centralized-logs IAM role: FlowLogsIAMRole Log format: Default Create flow log\nEnable VPC DNS Query Logging Configure Resolver Query Logging Open the Amazon Route 53 Console.\nIn the left navigation pane, select VPC Resolver -\u0026gt; Query logging.\nClick \u0026ldquo;Configure query logging\u0026rdquo;.\nConfigure:\nName: Enter a descriptive name, e.g., IR-DNS-Query-Log-Config. Destination for query logs: CloudWatch Logs log group Log group: Select \u0026ldquo;Existing log group\u0026rdquo; and choose: /aws/incident-response/centralized-logs Click \u0026ldquo;Configure query logging\u0026rdquo;.\n"},{"uri":"https://beforelights.github.io/AWS-Worklog/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Set up a Hybrid DNS environment using Route 53 Resolver Endpoints (Inbound/Outbound). Integrate AWS Managed Microsoft AD with on-premises simulations via RD Gateway. Gain proficiency in reducing manual toil by performing core tasks via AWS CLI. Translate technical blog posts to deepen understanding of cloud concepts. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Completed end-to-end setup of a hybrid DNS environment using Route 53 Resolver with AWS Managed Microsoft AD and RD Gateway.\n+ Launched the provided CloudFormation template to create a custom VPC, public/private subnets across two Availability Zones, Internet Gateway, and NAT Gateways for outbound access from private subnets.\n+ Configured a Remote Desktop Gateway (RDGW) EC2 instance in the public subnet, adjusted security groups for controlled RDP/ICMP access, and verified remote connectivity from the local workstation.\n+ Deployed AWS Managed Microsoft AD, joined RDGW to the managed domain, and confirmed domain authentication from the RD session host. + Created Route 53 Resolver outbound endpoints in private subnets and forwarding rules so VPC workloads can resolve on-premises-style domain names via the managed AD DNS servers. + Created Route 53 Resolver inbound endpoints and rules to allow external or on-premises DNS forwarders to resolve private AWS hostnames in the VPC, completing the hybrid DNS flow. + Tested DNS name resolution from the RDGW instance to ensure queries for both AWS private records and on-premises-style domains were correctly routed through the configured Resolver endpoints and AD DNS. 06/10/2025 06/10/2025 AWS Managed Microsoft AD - AWS Directory Service\nWhat is Route 53 VPC Resolver?\nNAT Gateways - Amazon VPC 3 Translated Blog 1 and Blog 2 07/10/2025 07/10/2025 Blog 1\nBlog 2 4 Completed end-to-end AWS CLI practice across core services (S3, SNS, IAM, VPC, EC2) using an SSO-authenticated profile.\n+ Used AWS CLI with Amazon S3 to create and inspect S3 buckets and objects instead of doing those actions in the console.\n+ Practiced AWS CLI with Amazon SNS by creating topics, adding email subscriptions, and sending test notifications.\n+ Managed IAM identities via AWS CLI, creating users/groups and handling access keys through the command line.\n+ Built networking with VPC and Internet Gateway using AWS CLI, creating a custom VPC, subnets, and public routing for internet access.\n+ Launched and managed an EC2 instance via AWS CLI, tested SSH connectivity, and terminated the instance after the lab.\n+ Ran all labs using aws login instead of static access keys from aws configure. 08/10/2025 08/10/2025 Getting Started with the AWS CLI\nGetting started with Amazon S3 using the AWS CLI\nAccessing Amazon SNS in the AWS CLI\nIAM examples using AWS CLI\nGetting started with Amazon VPC using the AWS CLI\nUsing Amazon EC2 in the AWS CLI\nConfiguring IAM Identity Center authentication with the AWS CLI 5 Translated Blog 3 09/10/2025 09/10/2025 Blog 3 6 - Completed Introduction to Research for Essay Writing in Coursera (ENW493c) 10/10/2025 10/10/2025 Week 5 Achievements: Successfully implemented a Hybrid DNS architecture enabling seamless resolution between on-premises and AWS (Route 53 Resolver). Gained hands-on experience with AWS Managed Microsoft AD and Remote Desktop Gateway integration. Demonstrated full proficiency in managing core AWS resources (VPC, EC2, S3, IAM) exclusively via CLI. Enhanced technical communication skills by translating three in-depth cloud engineering blog posts. "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.11-appendices/5.11.5-cloudwatch-autoexport/","title":"CloudWatch Autoexport Code","tags":[],"description":"","content":" import json import base64 import gzip from io import BytesIO import boto3 import os import time s3 = boto3.client(\u0026#39;s3\u0026#39;) # --- CONFIGURATION --- RAW_S3_BUCKET = os.environ.get(\u0026#34;DESTINATION_BUCKET\u0026#34;) # The log group pattern constant is no longer used for filtering, but is kept for reference. # VPC_DNS_LOG_PATTERN = \u0026#39;/aws/route53/query/\u0026#39; def is_vpc_dns_log(log_message): try: json_body = json.loads(log_message.strip()) if \u0026#39;query_name\u0026#39; in json_body and \u0026#39;query_type\u0026#39; in json_body: return True return False except Exception: return False def lambda_handler(event, context): try: compressed_payload = base64.b64decode(event[\u0026#39;awslogs\u0026#39;][\u0026#39;data\u0026#39;]) f = BytesIO(compressed_payload) decompressed_data = gzip.GzipFile(fileobj=f).read() log_data = json.loads(decompressed_data.decode(\u0026#39;utf-8\u0026#39;)) log_lines = [] for log_event in log_data.get(\u0026#39;logEvents\u0026#39;, []): log_lines.append(log_event.get(\u0026#39;message\u0026#39;, \u0026#39;\u0026#39;)) if not log_lines: print(f\u0026#34;Batch skipped: No log events found in payload. Log Group: {log_data.get(\u0026#39;logGroup\u0026#39;)}\u0026#34;) return {\u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: \u0026#39;Log batch ignored (No events).\u0026#39;} is_dns_log = is_vpc_dns_log(log_lines[0]) if is_dns_log: key_prefix = \u0026#39;vpc-dns-logs\u0026#39; filename_prefix = \u0026#39;vpc-\u0026#39; # Add vpc- to the filename else: key_prefix = \u0026#39;vpc-flow-logs\u0026#39; filename_prefix = \u0026#39;eni-\u0026#39; # Keep filename blank for other logs output_content = \u0026#39;\\n\u0026#39;.join(log_lines) full_log_group_name = log_data.get(\u0026#39;logGroup\u0026#39;, \u0026#39;unknown-group\u0026#39;) log_group_name_safe = full_log_group_name.strip(\u0026#39;/\u0026#39;).replace(\u0026#39;/\u0026#39;, \u0026#39;_\u0026#39;) final_filename = f\u0026#34;{filename_prefix}{context.aws_request_id}.gz\u0026#34; s3_key = f\u0026#39;exportedlogs/{key_prefix}/{log_group_name_safe}/{final_filename}\u0026#39; buffer = BytesIO() with gzip.GzipFile(fileobj=buffer, mode=\u0026#39;w\u0026#39;) as gz: gz.write(output_content.encode(\u0026#39;utf-8\u0026#39;)) gzipped_data = buffer.getvalue() s3.put_object( Bucket=RAW_S3_BUCKET, Key=s3_key, Body=gzipped_data, ContentType=\u0026#39;application/x-gzip\u0026#39; ) num_logs = len(log_lines) print(f\u0026#34;Exported {num_logs} raw log lines to s3://{RAW_S3_BUCKET}/{s3_key}\u0026#34;) return {\u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: f\u0026#39;Logs exported. {num_logs} events processed. Key Prefix: {key_prefix}\u0026#39;} except Exception as e: print(f\u0026#34;Error in CW Export Lambda: {e}\u0026#34;) raise e "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.7-dashboard-setup/5.7.5-setup-cognito/","title":"Cognito Setup","tags":[],"description":"","content":"In this guide, you will create a Cognito user pool for dashboard login.\nCreate Cognito User Pool Open the Amazon Cognito Console\nNavigate to https://console.aws.amazon.com/cognito/ Or: AWS Management Console → Services → Cognito Create user pool:\nClick Create user pool In user pool creation, use this setting: Application type: Single-page application (SPA) Application name: dashboard-user-pool-client Options for sign-in identifiers: Email and Username Self-registration: Enable self-registration Required attributes for sign-up: email Add a return URL: Go to Cloudfront, choose the one that you just created and copy the Distribution domain name and paste it here (Example: https://d2bvvvpr6s4eyd.cloudfront.net) Click Create user directory After create, scroll down and click Go to overview User pool App clients configuration:\nSelect App clients on the left menu panel Choose dashboard-user-pool-client In App client information section, click Edit Click Save change Managed login pages configuration:\nIn Managed login pages configuration section, click Edit Click Add sign-out URL at Allowed sign-out URLs section Copy the URL on the callbacks URL and paste to Allowed sign-out URLs Scroll down to OpenID Connect scopes add Profile to the scopes Click Save change Create a user:\nOn the left menu panel, select User option Click Create user Enter your user information Click Create user "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.5-processing-setup/","title":"Processing Setup","tags":[],"description":"","content":"This Processing Setup phase establishes the core data pipeline for structuring raw logs and preparing them for queryable analysis. It mandates the deployment of three Kinesis Data Firehose streams for buffering and delivering CloudTrail and VPC logs to target S3 buckets. Concurrently, you will configure the AWS Glue Database and four Athena tables via DDL to make the structured data queryable. This pipeline relies on five ETL Lambda functions triggered by S3 Event Notifications to perform the necessary data transformation upon log arrival.\nContent Create Kinesis Data Firehose Delivery Streams Create AWS Glue Database and Tables Create Lambda Functions - ETL Processing "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/","title":"Workshop","tags":[],"description":"","content":"AWS Auto Incident Response System Setup Overview This guide provides a complete, step-by-step procedure for deploying our automated incident response and forensic system in AWS. This system leverages CloudTrail, GuardDuty, VPC Flow Logs, Kinesis Firehose, Glue, Athena, and Lambda functions orchestrated by AWS Step Functions to automatically detect, analyze, and quarantine compromised resources like EC2 instances and IAM users. Futher log forensics capacity is added by setting up a Security Dashboard hosted on S3 and accessed via CloudFront and Cognito, query log using API Gateway and Lambda.\nContent Overview Prerequisites Phase 1: Foundation Setup Phase 2: Monitoring Setup Phase 3: Processing Setup Phase 4: Automation Setup Phase 5: Dashboard Setup Verify Use CDK Cleanup Appendices "},{"uri":"https://beforelights.github.io/AWS-Worklog/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Define project scope and assign team responsibilities. Estimate and optimize operational costs using AWS Pricing Calculator. Gain insights into DevSecOps and Generative AI applications. Establish a structured documentation system for AWS learning. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Completed Pháp luật và đạo đức trong công nghệ số MOOC in Coursera (KS57 Program) 13/10/2025 13/10/2025 3 - Attended a team meeting to discuss the project scope, clarify each member’s responsibilities, and plan the next tasks 14/10/2025 14/10/2025 4 - Attended a team meeting - Reviewed the estimated costs of running the EC2 instance and Lambda functions using the AWS Pricing Calculator, then adjusted the architecture to optimize expenses. 15/10/2025 15/10/2025 AWS Pricing Calculator\nGenerating Amazon EC2 estimates\nAWS Lambda pricing 5 - Completed Quản trị dự án và duy trì đổi mới trong chuyển đổi số MOOC in Coursera (KS57 Program) - Participated in the online seminar 𝗗𝗫\u0026lt;𝗶𝗻𝗔𝗰𝘁𝗶𝗼𝗻\u0026gt; 𝗧𝗮𝗹𝗸#𝟳: Reinventing DevSecOps with AWS Generative AI 16/10/2025 16/10/2025 6 - I created a dedicated Notion workspace to document what I learn about AWS, along with practical tips and tricks, so I can easily review and refine my understanding over time. 17/10/2025 17/10/2025 Week 6 Achievements: Defined clear project scope and distributed tasks among team members. Optimized infrastructure costs by analyzing estimates with AWS Pricing Calculator. Enhanced knowledge of legal, ethical, and project management aspects in tech. Created a Notion workspace to centralize and refine AWS notes. "},{"uri":"https://beforelights.github.io/AWS-Worklog/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at Amazon Web Services (AWS) from 08/09/2025 to 12/12/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in the AWS First Cloud Journey Internship, through which I improved my skills in Cloud Computing, Infrastructure as Code (IaC), DevOps practices, Security compliance, and Solutions Architecture.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ✅ ☐ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ☐ ✅ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ☐ ✅ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ✅ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ✅ ☐ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ✅ ☐ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ☐ ✅ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Sharing and Feedback My internship at Amazon has been a transformative experience, marking my transition from a student to a cloud professional. The journey began with foundational knowledge of core AWS services like EC2, VPC, and IAM, but quickly escalated to complex, real-world scenarios involving Serverless architectures, Event-Driven designs, and Infrastructure as Code (IaC).\nOne of the most valuable aspects was the shift in mindset, from simply \u0026ldquo;making things work\u0026rdquo; with ClickOps to ensuring they are scalable, reproducible, and secure using tools like CloudFormation and adopting the AWS Well-Architected Framework. Participating in events like the \u0026ldquo;AWS Cloud Mastery Series\u0026rdquo; expanded my horizon beyond technical skills, introducing me to the importance of security pillars, cost optimization, and the power of Generative AI.\nWhile there were challenges, such as the steep learning curve of new services like Step Functions and the discipline required for continuous self-learning, the support from the community and mentors was invaluable. This internship has not only equipped me with technical proficiency but also instilled a culture of customer obsession and ownership that I will carry forward in my career.\nNeeds Improvement Strengthen Discipline and Compliance: Strictly adhere to organizational schedules, rules, and processes to ensure professionalism and reliability (addressing the \u0026ldquo;Average\u0026rdquo; rating). Enhance Proactiveness and Responsibility: Take more initiative in seeking out tasks and responsibilities rather than waiting for instructions, improving from \u0026ldquo;Fair\u0026rdquo; to \u0026ldquo;Good\u0026rdquo;. Improve Team Contribution: Contribute more actively to team discussions and brainstorm innovative ideas to increase my overall impact on the project. Deepen Professional Knowledge: Accelerate the learning curve for new technologies to apply them more proficiently in practical scenarios. "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.6-automation-setup/","title":"Automation Setup","tags":[],"description":"","content":"Phase 4: Automation Setup Create Isolation Security Group EC2 Console → Security Groups → Create security group Name: IR-Isolation-SG Description: Denies all inbound and outbound traffic for compromised instances VPC: Select your VPC Inbound rules: None (deny all) Outbound rules: Remove default (deny all) Create and note Security Group ID (e.g., sg-0078026b70389e7b3) Create SNS Topic SNS Console → Create topic Type: Standard, Name: IncidentResponseAlerts Access policy: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;events.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sns:Publish\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:sns:ap-southeast-1:831981618496:IncidentResponseAlerts\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;AWSEvents_IncidentResponseAlert_Target0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;events.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;SNS:Publish\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:sns:ap-southeast-1:831981618496:IncidentResponseAlerts\u0026#34; } ] } Create Lambda Functions - Incident Response ir-parse-findings-lambda Handler: parse_findings.lambda_handler Role: ParseFindingsLambdaServiceRole Code: parse-findings ir-isolate-ec2-lambda Handler: isolate_ec2.lambda_handler Role: IsolateEC2LambdaServiceRole Env: ISOLATION_SG_ID=sg-XXXXXXX (from step 12) Code: isolate-ec2 ir-quarantine-iam-lambda Handler: quarantine_iam.lambda_handler Role: QuarantineIAMLambdaServiceRole Env: QUARANTINE_POLICY_ARN=arn:aws:iam::ACCOUNT_ID:policy/IrQuarantineIAMPolicy Code: quarantine-iam ir-alert-dispatch Handler: alert_dispatch.lambda_handler Role: AlertDispatchLambdaServiceRole Env: SENDER_EMAIL, RECIPIENT_EMAIL, SLACK_WEBHOOK_URL Add SNS trigger: Topic IncidentResponseAlerts Code: alert-dispatch Update SNS Topic Subscription SNS Console → IncidentResponseAlerts → Subscriptions Verify: Protocol=AWS Lambda, Endpoint=ir-alert-dispatch, Status=Confirmed Create Step Functions State Machine Step Functions Console → Create state machine Type: Standard, Name: IncidentResponseStepFunctions Definition: Step Functions Definition Role: StepFunctionsRole Create Create EventBridge Rule EventBridge Console → Rules → Create rule Name: IncidentResponseAlert Event pattern: { \u0026#34;source\u0026#34;: [\u0026#34;aws.guardduty\u0026#34;], \u0026#34;detail-type\u0026#34;: [\u0026#34;GuardDuty Finding\u0026#34;] } Targets (2): SNS topic: IncidentResponseAlerts Step Functions: IncidentResponseStepFunctions with role IncidentResponseStepFunctionsEventRole Configure Athena Workgroup Athena Console → Workgroups → primary → Edit Query result location: s3://athena-query-results-ACCOUNT_ID-REGION/ Save "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.11-appendices/5.11.6-parse-findings/","title":"Parse Findings Code","tags":[],"description":"","content":" import json import logging logger = logging.getLogger() logger.setLevel(logging.INFO) def lambda_handler(event, context): instance_ids = [] detail = event.get(\u0026#39;detail\u0026#39;, {}) region = event.get(\u0026#39;region\u0026#39;) or detail.get(\u0026#39;region\u0026#39;) or \u0026#39;ap-southeast-1\u0026#39; instance_id_primary = detail.get(\u0026#39;resource\u0026#39;, {}).get(\u0026#39;instanceDetails\u0026#39;, {}).get(\u0026#39;instanceId\u0026#39;) if instance_id_primary: instance_ids.append(instance_id_primary) # --- 2. Extract from the older/secondary \u0026#39;resources\u0026#39; array structure --- for r in detail.get(\u0026#34;resources\u0026#34;, []): if r.get(\u0026#34;type\u0026#34;) == \u0026#34;AwsEc2Instance\u0026#34;: id_from_details = r.get(\u0026#39;details\u0026#39;, {}).get(\u0026#39;instanceId\u0026#39;) if id_from_details: instance_ids.append(id_from_details) else: arn_id = r.get(\u0026#39;id\u0026#39;) if arn_id and arn_id.startswith(\u0026#39;arn:aws:ec2:\u0026#39;): instance_ids.append(arn_id.split(\u0026#39;/\u0026#39;)[-1]) unique_instance_ids = list(set([id for id in instance_ids if id])) return { \u0026#34;InstanceIds\u0026#34;: unique_instance_ids, \u0026#34;Region\u0026#34;: region } "},{"uri":"https://beforelights.github.io/AWS-Worklog/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Deploy and configure Amazon FSx for Windows File Server with AD integration. Perform storage administration tasks: deduplication, quotas, and shadow copies. Benchmark file system performance using DiskSpd and monitor via CloudWatch. Reinforce cloud architecture knowledge through gamified learning (AWS Card Clash). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Busy with outside work - Online team meeting to assign tasks. 20/10/2025 20/10/2025 3 Provisioned and configured an Amazon FSx for Windows File Server lab environment with multiple file systems and SMB shares for later testing.\n+ Set up the FSx lab environment via AWS CloudFormation, creating the required VPC, subnets, security groups, and Windows management instances for the workshop.\n+ Created both SSD and HDD Multi-AZ FSx for Windows file systems, selecting appropriate storage sizes, throughput capacities, private subnets, and AWS Managed Microsoft AD integration.\n+ Used the Windows management instance to configure new SMB file shares on the FSx volumes, assigning application/data folders and permissive lab share permissions for later scenarios.\n+ Verified basic connectivity from the Windows instance to the FSx DNS names and shares, ensuring the environment was ready for performance and data-management exercises. 21/10/2025 21/10/2025 Amazon FSx for Windows File Server - AWS Study Group\nGetting started with Amazon FSx for Windows File Server\nAccessing data using file shares - Amazon FSx for Windows File Server 4 Implemented performance testing, monitoring, and advanced data protection features on Amazon FSx for Windows File Server.\n+ Ran DiskSpd-based performance tests from a Windows instance against the mapped FSx drive to measure read/write throughput under load.\n+ Monitored FSx CloudWatch metrics (throughput, IOPS, latency, connections) from the FSx console to correlate observed performance with the configured capacity.\n+ Enabled data deduplication on the FSx file system via the remote PowerShell FSx CLI, created a daily optimization schedule, and validated dedup status and savings statistics.\n+ Turned on shadow copies for the FSx volume, increased the storage limit, created an on-demand snapshot, and confirmed that file “Previous Versions” could be restored from Windows Explorer.\n+ Practiced admin control over user sessions and open files using Shared Folders and FSx PowerShell cmdlets, including force-closing an active test file to interrupt client I/O. 22/10/2025 22/10/2025 FSx for Windows File Server performance\nProtecting your data with shadow copies\nAdministering FSx for Windows file systems 5 - Played AWS Card Clash matches with teammates to place the right AWS service cards into sample cloud architecture diagrams and see how services connect.\n- Talked through each round to explain in simple terms what the main services do (for example, which ones handle compute, storage, or networking) so everyone could remember them more easily.\n- Loaded lecture notes and AWS study links into NotebookLM and used it to highlight key ideas and generate short explanations for topics that still felt confusing.\n- Asked NotebookLM to create study questions and short summaries from those notes so the mid-term review felt more like a guided Q\u0026amp;A instead of just re-reading everything.\n- Used Gemini to turn mid-term topics into small practice quizzes and flashcards, then answered them to check which AWS areas still need more revision.\n- Re-checked any unclear answers using official AWS learning materials to be sure the explanations and quiz responses matched how the services actually work. 23/10/2025 23/10/2025 AWS Card Clash: Learn Cloud Architecture\nIntroducing AWS Card Clash mobile 6 Configured quotas, high-availability shares, capacity scaling, cleanup, and reviewed AWS CLI workflows for FSx for Windows.\n+ Enabled per-user storage quotas on the FSx file system using the FSx remote PowerShell interface, set default limits and warnings, and enforced a custom quota for a specific user.\n+ Created a continuously available, encrypted SMB share for SQL workloads on FSx, using credentials from AWS Secrets Manager and the FSx PowerShell CLI to enable Continuous Availability.\n+ Scaled throughput and storage capacity of the FSx file system from the console, increasing MB/s and total GiB while allowing FSx to perform online optimization in the background.\n+ Deleted the entire FSx lab environment by removing the original CloudFormation stack and confirming that all associated resources were terminated to avoid ongoing costs.\n+ Read the “Using the AWS CLI (reference)” section to understand how to automate FSx, AD, and related tasks with AWS CLI instead of the console. 24/10/2025 24/10/2025 Managing storage quotas - Amazon FSx for Windows File Server\nManaging throughput and storage capacity\nOne-time file system setup tasks using the Amazon FSx CLI Week 7 Achievements: Successfully provisioned and managed a high-performance Windows file server environment on AWS using FSx. Executed essential storage tasks including Data Deduplication (saving space) and Shadow Copies (enabling user-driven recovery). Enforced governance through user storage quotas and monitored file system health via CloudWatch. Enhanced team knowledge of AWS services and architecture through interactive learning sessions. "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.7-dashboard-setup/","title":"Dashboard Setup","tags":[],"description":"","content":"This guide will show you how to setup the security dashboard. The security dashboard will be using S3 to contain the web files and folder, Lambda to query data using Athena, API Gateway to routing api to Lambda and Cloudfront to caching and access to the web using it\u0026rsquo;s URL.\nContent Setup S3 Setup Lambda Setup API Gateway Setup Cloudfront Setup Cognito "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.11-appendices/5.11.7-isolate-ec2/","title":"Isolate EC2 Code","tags":[],"description":"","content":" import json import boto3 import os from botocore.exceptions import ClientError ISOLATION_SG_ID = os.getenv(\u0026#39;ISOLATION_SG_ID\u0026#39;) def lambda_handler(event, context): print(\u0026#34;=== ISOLATE EVENT RECEIVED ===\u0026#34;) print(json.dumps(event, indent=2)) instance_id = event.get(\u0026#39;InstanceId\u0026#39;) region = event.get(\u0026#39;Region\u0026#39;, \u0026#39;ap-southeast-1\u0026#39;) if not instance_id or not ISOLATION_SG_ID: print(\u0026#34;[ERROR] Missing InstanceId or IsolationSGId in input. Cannot isolate.\u0026#34;) return {\u0026#34;status\u0026#34;: \u0026#34;isolation_failed\u0026#34;, \u0026#34;InstanceId\u0026#34;: instance_id, \u0026#34;error\u0026#34;: \u0026#34;Missing input data\u0026#34;} try: ec2 = boto3.client(\u0026#39;ec2\u0026#39;, region_name=region) response = ec2.describe_instances(InstanceIds=[instance_id]) instance = response[\u0026#39;Reservations\u0026#39;][0][\u0026#39;Instances\u0026#39;][0] current_sgs = [sg[\u0026#39;GroupId\u0026#39;] for sg in instance.get(\u0026#39;SecurityGroups\u0026#39;, [])] if ISOLATION_SG_ID in current_sgs: print(f\u0026#34;[INFO] {instance_id} already has isolation SG {ISOLATION_SG_ID}\u0026#34;) return { **event, \u0026#34;status\u0026#34;: \u0026#34;already_isolated\u0026#34;, \u0026#34;InstanceId\u0026#34;: instance_id, \u0026#34;Region\u0026#34;: region, \u0026#34;IsolationSG\u0026#34;: None } print(f\u0026#34;[ACTION] Isolating {instance_id} in {region} with SG {ISOLATION_SG_ID}\u0026#34;) ec2.modify_instance_attribute( InstanceId=instance_id, Groups=[ISOLATION_SG_ID] ) print(f\u0026#34;[SUCCESS] {instance_id} isolated with SG {ISOLATION_SG_ID}\u0026#34;) return { **event, \u0026#34;status\u0026#34;: \u0026#34;isolation_complete\u0026#34;, \u0026#34;InstanceId\u0026#34;: instance_id, \u0026#34;Region\u0026#34;: region, \u0026#34;IsolationSG\u0026#34;: ISOLATION_SG_ID } except ClientError as e: error_code = e.response.get(\u0026#39;Error\u0026#39;, {}).get(\u0026#39;Code\u0026#39;) print(f\u0026#34;[ERROR] Isolation FAILED for {instance_id} ({error_code}): {str(e)}\u0026#34;) return { \u0026#34;status\u0026#34;: \u0026#34;isolation_failed\u0026#34;, \u0026#34;InstanceId\u0026#34;: instance_id, \u0026#34;error\u0026#34;: str(e) } except Exception as e: print(f\u0026#34;[ERROR] Isolation FAILED (General) for {instance_id}: {str(e)}\u0026#34;) raise e "},{"uri":"https://beforelights.github.io/AWS-Worklog/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. We also have frequent social gatherings and team bonding activities which help strengthen our relationships and make the atmosphere even more enjoyable.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. They also diligently share links to free Udemy courses and other high-quality learning resources to help us improve. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nWhile the specific tasks—focused on software engineering, web, and app development—do not directly align with my university\u0026rsquo;s current curriculum, I find this to be a significant advantage. It has pushed me to venture into new areas I hadn\u0026rsquo;t encountered before, allowing me to build valuable new skills effectively.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. Most importantly, I gained deep hands-on experience with AWS services and cloud architecture. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive, welcoming, and supportive. Everyone respects each other, works seriously but still keeps things enjoyable. For projects, each team works cohesively, and when there are urgent matters, everyone supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company offers highly flexible working hours which is very convenient. The true value lies in the immense knowledge gained regarding cloud technologies and the professional connections established during the internship, which are invaluable for my future career.\nAdditional Questions What did you find most satisfying during your internship?\nThe most satisfying aspect was successfully setting up complex integrations like Telegram and Cognito. Seeing the authentication flow work seamlessly and being able to deploy the entire infrastructure using AWS CDK gave me a strong sense of accomplishment in mastering these technical challenges.\nWhat do you think the company should improve for future interns?\nI believe the timeline of the workshops could be improved. Specifically, scheduling the Mastery Series earlier, before the project phase begins, would provide interns with the necessary foundational knowledge to tackle the project more effectively from the start.\nIf recommending to a friend, would you suggest they intern here? Why or why not?\nYes, I would definitely recommend it. The community is incredibly welcoming, and the mentors are genuinely supportive. I particularly appreciate the culture of mutual support and the shared mindset of continuous self-improvement and helping one another grow.\nSuggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience?\nAs mentioned, aligning the workshop schedule to ensure technical training (like the Mastery Series) precedes the project kick-off would be a significant improvement for future cohorts.\nWould you like to continue this program in the future?\nYes, I would be eager to continue with the program.\nAny other comments (free sharing): I would like to express my sincere thanks for this internship opportunity. It has been a valuable learning experience.\n"},{"uri":"https://beforelights.github.io/AWS-Worklog/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Learn the fundamentals of 3D web development using Three.js and WebGL. Prepare rigorously for the AWS Midterm Exam using practice tests and flashcards. Coordinate with the team to identify knowledge gaps and strengthen weak areas. Explore AWS Certified Solutions Architect question banks for deeper understanding. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Started learning Three.js fundamentals using the Three.js Journey course to prepare for building interactive 3D web experiences.\n+ Reviewed the Three.js Journey introduction to understand how WebGL works under the hood and how Three.js simplifies 3D rendering in the browser.\n+ Set up a minimal Three.js project with an HTML canvas, imported the library, and initialized a basic scene, camera, and WebGL renderer to display content on the page.\n+ Created a simple 3D cube mesh using BoxGeometry and a basic material, then rendered it on screen to verify that the environment was configured correctly.\n+ Experimented with camera position and object rotation in the animation loop to see how changes affect the view and interaction feel in real time.\n+ Took notes on how these core Three.js concepts could later integrate with existing JavaScript and frontend skills for portfolio-style projects. 27/10/2025 27/10/2025 Three.js Journey — Learn WebGL with Three.js\nthree.js manual – Getting started 3 - Created 500 AWS Flashcards together with team members for learning on Quizlet 28/10/2025 28/10/2025 4 - Found 2 GitHub Repo that has questions and answers that I think is somewhat related to the Midterm test 29/10/2025 29/10/2025 AWS Certified Solutions Architect Exam\nAWS Certified Cloud Practitioner Notes 5 Practiced AWS exam questions using the GitHub resources from Day 4 and basic summaries from NotebookLM.\n+ Used AWS Certified Cloud Practitioner notes from other learners online to complete practice tests and check overall readiness for the midterm.\n+ Practiced around 40 questions from AWS Certified Solutions Architect Associate–style question sets to get used to harder scenario-based items.\n+ Opened the GitHub repos found on Day 4 during review to quickly look up explanations when an answer was unclear.\n+ Used Gemini with selected notes to simplify definitions of core AWS services and concepts that appeared frequently in the practice questions.\n+ Used the old NotebookLM to generate quizzes. 30/10/2025 30/10/2025 6 - FCJ Midterm Exam day (300/650) 31/10/2025 31/10/2025 Week 8 Achievements: Successfully set up a development environment for Three.js and rendered initial 3D scenes. Collaborated to create a comprehensive study deck of 500 AWS Flashcards. Completed the Midterm Exam, identifying personal strengths and areas for improvement. Gained exposure to real-world solution architect scenarios through practice questions. "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.11-appendices/5.11.8-quarantine-iam/","title":"Quarantine IAM Code","tags":[],"description":"","content":" import json import boto3 import os QUARANTINE_POLICY_ARN = os.environ.get(\u0026#34;QUARANTINE_POLICY_ARN\u0026#34;) def lambda_handler(event, context): print(\u0026#34;=== EVENT RECEIVED ===\u0026#34;) print(json.dumps(event, indent=2)) try: finding = event.get(\u0026#39;detail\u0026#39;, {}) user_name = ( finding.get(\u0026#39;resource\u0026#39;, {}) .get(\u0026#39;accessKeyDetails\u0026#39;, {}) .get(\u0026#39;userName\u0026#39;) ) if not user_name: print(\u0026#34;[WARNING] No IAM user found in this finding. Skipping.\u0026#34;) return {\u0026#34;status\u0026#34;: \u0026#34;no_user\u0026#34;} print(f\u0026#34;[ACTION] Quarantining IAM User \u0026#39;{user_name}\u0026#39;...\u0026#34;) iam = boto3.client(\u0026#39;iam\u0026#39;) # Kiểm tra nếu policy đã được gán attached_policies = iam.list_attached_user_policies(UserName=user_name)[\u0026#39;AttachedPolicies\u0026#39;] policy_arns = [p[\u0026#39;PolicyArn\u0026#39;] for p in attached_policies] if QUARANTINE_POLICY_ARN in policy_arns: print(f\u0026#34;[INFO] Policy {QUARANTINE_POLICY_ARN} is already attached to user {user_name}.\u0026#34;) else: iam.attach_user_policy( UserName=user_name, PolicyArn=QUARANTINE_POLICY_ARN ) print(f\u0026#34;[SUCCESS] Policy attached. User {user_name} is now quarantined.\u0026#34;) except Exception as e: print(f\u0026#34;[ERROR] Failed to quarantine user: {str(e)}\u0026#34;) raise e return {\u0026#34;status\u0026#34;: \u0026#34;processed\u0026#34;, \u0026#34;action\u0026#34;: \u0026#34;iam_quarantined\u0026#34;} "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.8-verify-setup/","title":"Verify Setup","tags":[],"description":"","content":"After all the setup phase, please refer to the checklist to ensure complete resources creation\nVerify Setup Complete Verification Checklist:\nIncident Response and Forensics:\n✅ S3 Buckets: All 5 created with versioning/encryption ✅ IAM Roles: All 17 roles with correct policies ✅ CloudTrail: Logging enabled ✅ GuardDuty: Enabled with S3 export ✅ VPC Flow Logs: Active ✅ Lambda Functions: All 9 deployed ✅ Firehose Streams: All 3 active ✅ Glue Tables: All 4 created ✅ S3 Events: All 4 triggers configured ✅ SNS Topic: Created with subscription ✅ Step Functions: Active ✅ EventBridge Rule: Enabled with 2 targets Security Dashboard:\n✅ S3 Buckets: Bucket is created with dashboard file stored and enabled hosting ✅ Query Lambda: Lambda is created with the appropriate roles ✅ API Gateway: API Gateway is created with the correct API and resources ✅ CloudFront: Distribution is created with API and S3 origins configured ✅ Cognito: Linked to CloudFront distribution and created user in user pool End-to-End Test\nGenerate sample GuardDuty findings: 1.1 GuardDuty Console → Settings → Generate sample findings (200+ findings) or 1.2 Trigger single finding via CloudShell (Dectector Id is in GuardDuty Console → Settings ) aws guardduty create-sample-findings --detector-id [$dectector-id] --finding-types \u0026#34;Recon:EC2/PortProbeUnprotectedPort\u0026#34; Monitor workflow: Check EventBridge, SNS, Step Functions, Lambda logs Verify alerts: Check email and Slack Query data in Athena: "},{"uri":"https://beforelights.github.io/AWS-Worklog/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Analyze cloud spending and usage patterns using AWS Glue and Amazon Athena. Monitor EC2 metrics in real-time by creating custom Grafana dashboards via CloudWatch. Automate business workflows using AWS Step Functions and AWS SAM. Deploy infrastructure as code using AWS CDK in a local VS Code environment. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Completed Quản trị dữ liệu và an toàn thông tin MOOC in Coursera (KS57 Program) 03/11/2025 03/11/2025 3 Used AWS Glue and Amazon Athena to explore AWS Cost \u0026amp; Usage Reports and understand where cloud spending comes from.\n+ Ran simple Athena queries on the Glue-created CUR tables to view example rows, check billing periods, and get familiar with the structure of the cost data.\n+ Summarized costs by account and service in Athena to see which accounts and AWS services are responsible for the highest spend, then focused on key Amazon EC2 on-demand usage lines for more detail.\n+ Checked cost allocation tags (such as a cost-center tag) in the CUR data to compare how much spend is correctly tagged to teams versus how much is still untagged.\n+ Compared discounted usage (Savings Plans/Reserved Instances) with estimated on-demand prices in Athena queries to see how much savings the discounts provide for specific EC2 usage types.\n+ Used the query results to build a clearer picture of who is spending, on which services, and how efficiently, as preparation for future cost optimization work. 04/11/2025 04/11/2025 Cost and performance analysis with AWS Glue and Amazon Athena\nQuerying Cost and Usage Reports using Amazon Athena\nAWS Cost and Usage Reports overview\nCreating tables for CUR data using AWS Glue and Athena\nOrganizing and tracking costs with AWS cost allocation tags\nAmazon Athena pricing 4 Set up Grafana to monitor my EC2 instance using CloudWatch and built a simple live dashboard.\n+ Connected Grafana to Amazon CloudWatch as a data source so it can read EC2 metrics from my AWS account.\n+ Tested the data source and confirmed Grafana could successfully query CloudWatch metrics for my instance.\n+ Created a Grafana dashboard with panels showing key EC2 metrics like CPU usage over time.\n+ Saved the dashboard and used the refresh and share options so it can be reused to watch instance health in real time. 05/11/2025 05/11/2025 Getting started with Grafana basic\nUsing Amazon CloudWatch metrics\nGrafana CloudWatch data source configuration\nBuild your first Grafana dashboard 5 Completed the end-to-end CDK-based setup for an EC2 web server environment used in the AWS Storage Gateway lab, using local VS Code instead of Cloud9.\n+ Finished the introduction and preparation steps, including creating the required IAM role that lets Storage Gateway access S3 securely. + Set up a local AWS CDK development environment in VS Code with the AWS CLI configured for the correct account and region. + Initialized a new AWS CDK project, defined a VPC with public subnets, and checked the synthesized CloudFormation template for errors from the local machine. + Updated the CDK stack to add an EC2 instance, security group, and user data script cài đặt Apache, then deployed the stack and validated the web server from the instance’s public IP. 06/11/2025 06/11/2025 CDK Basic 6 Completed the core AWS Step Functions workshop: modeled the business flow, wired sample services, and deployed a SAM-managed state machine with proper IAM permissions.\n+ Reviewed the business domain and workflow details for the account application process, identifying each business step to later map into Step Functions states.\n+ Set up a local VS Code + AWS Toolkit environment and used it (instead of Cloud9 since I can\u0026rsquo;t use it) to work with the workshop’s sample backend services and infrastructure in the target AWS region.\n+ Built the first AWS Step Functions state machine to orchestrate account creation and data checks, then used the execution graph/history to verify each Task state’s input and output.\n+ Refined Task states and state input/output handling with InputPath and ResultPath so application data and check results are preserved cleanly through the workflow.\n+ Migrated the state machine into a SAM template with a dedicated execution role, letting CloudFormation manage deployments and IAM for the workflow end-to-end. 07/11/2025 07/11/2025 Get Started with AWS Step Functions\nUsing AWS SAM to build Step Functions workflows\nWorking with AWS CDK and Toolkit in VS Code\nProcessing input and output in Step Functions\nService-level permissions for Step Functions Week 9 Achievements: Performed granular billing analysis using Amazon Athena on Data Glue Catalog tables. Established a real-time observability solution for EC2 using Grafana and CloudWatch. Successfully orchestrated a serverless workflow using Step Functions and defined infrastructure with SAM. Transitioned infrastructure provisioning from manual steps to automated CDK deployments in VS Code. "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.11-appendices/5.11.9-alert-dispatch/","title":"Alert Dispatch Code","tags":[],"description":"","content":" import os import json import logging import urllib.request import boto3 from botocore.exceptions import ClientError import html # --- Telegram ENV --- # BOT_TOKEN = os.environ.get(\u0026#39;BOT_TOKEN\u0026#39;) # CHAT_ID = os.environ.get(\u0026#39;CHAT_ID\u0026#39;) # MESSAGE_THREAD_ID = os.environ.get(\u0026#39;MESSAGE_THREAD_ID\u0026#39;) # --- Slack ENV --- SLACK_WEBHOOK_URL = os.environ.get(\u0026#34;SLACK_WEBHOOK_URL\u0026#34;) # --- SES ENV --- SENDER_EMAIL = os.environ.get(\u0026#39;SENDER_EMAIL\u0026#39;) RECIPIENT_EMAIL = os.environ.get(\u0026#39;RECIPIENT_EMAIL\u0026#39;) # Can now be \u0026#34;a@b.com, c@d.com\u0026#34; AWS_REGION = os.environ.get(\u0026#39;AWS_REGION\u0026#39;, \u0026#39;ap-southeast-1\u0026#39;) # --- Setup --- # TELEGRAM_URL = f\u0026#34;https://api.telegram.org/bot{BOT_TOKEN}/sendMessage\u0026#34; if BOT_TOKEN else None logger = logging.getLogger() logger.setLevel(logging.INFO) # Initialize SES Client ses_client = boto3.client(\u0026#39;ses\u0026#39;, region_name=AWS_REGION) # ==================================================================== # SEND TO TELEGRAM # ==================================================================== # def send_to_telegram(finding, chat_id, thread_id): # logger.info(\u0026#34;Formatting message for Telegram...\u0026#34;) # severity_num = finding.get(\u0026#39;severity\u0026#39;, 0) # if severity_num \u0026gt;= 7.0: # severity = \u0026#34;🔴 HIGH\u0026#34; # elif severity_num \u0026gt;= 4.0: # severity = \u0026#34;🟠 MEDIUM\u0026#34; # else: # severity = \u0026#34;🔵 LOW\u0026#34; # title = finding.get(\u0026#39;title\u0026#39;, \u0026#39;N/A\u0026#39;) # description = finding.get(\u0026#39;description\u0026#39;, \u0026#39;N/A\u0026#39;) # account_id = finding.get(\u0026#39;accountId\u0026#39;, \u0026#39;N/A\u0026#39;) # region = finding.get(\u0026#39;region\u0026#39;, \u0026#39;N/A\u0026#39;) # finding_type = finding.get(\u0026#39;type\u0026#39;, \u0026#39;N/A\u0026#39;) # message_text = ( # f\u0026#34;🚨 *GuardDuty Finding* 🚨\\n\\n\u0026#34; # f\u0026#34;*Severity:* {severity}\\n\u0026#34; # f\u0026#34;*Account:* {account_id}\\n\u0026#34; # f\u0026#34;*Region:* {region}\\n\u0026#34; # f\u0026#34;*Title:* {title}\\n\u0026#34; # f\u0026#34;*Description:* {description}\\n\\n\u0026#34; # f\u0026#34;*Finding Type:* `{finding_type}`\u0026#34; # ) # payload = {\u0026#39;chat_id\u0026#39;: chat_id, \u0026#39;text\u0026#39;: message_text, \u0026#39;parse_mode\u0026#39;: \u0026#39;Markdown\u0026#39;} # if thread_id: # payload[\u0026#39;message_thread_id\u0026#39;] = thread_id # try: # req = urllib.request.Request( # TELEGRAM_URL, # data=json.dumps(payload).encode(\u0026#39;utf-8\u0026#39;), # headers={\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;} # ) # with urllib.request.urlopen(req) as response: # logger.info(\u0026#34;Telegram response: \u0026#34; + response.read().decode(\u0026#39;utf-8\u0026#39;)) # except Exception as e: # logger.error(f\u0026#34;TELEGRAM FAILED: {e}\u0026#34;) # ==================================================================== # SEND TO SLACK # ==================================================================== def send_to_slack(finding): if not SLACK_WEBHOOK_URL: logger.warning(\u0026#34;Slack ENV missing. Skipping.\u0026#34;) return severity_num = finding.get(\u0026#34;severity\u0026#34;, 0) title = finding.get(\u0026#34;title\u0026#34;, \u0026#34;No Title\u0026#34;) description = finding.get(\u0026#34;description\u0026#34;, \u0026#34;No Description\u0026#34;) region = finding.get(\u0026#34;region\u0026#34;, \u0026#34;N/A\u0026#34;) account_id = finding.get(\u0026#34;accountId\u0026#34;, \u0026#34;N/A\u0026#34;) finding_type = finding.get(\u0026#34;type\u0026#34;, \u0026#34;N/A\u0026#34;) if severity_num \u0026gt;= 7: color = \u0026#34;#ff0000\u0026#34; sev = \u0026#34;🔴 HIGH\u0026#34; elif severity_num \u0026gt;= 4: color = \u0026#34;#ffa500\u0026#34; sev = \u0026#34;🟠 MEDIUM\u0026#34; else: color = \u0026#34;#007bff\u0026#34; sev = \u0026#34;🔵 LOW\u0026#34; payload = { \u0026#34;text\u0026#34;: f\u0026#34;🚨 {sev} – {title}\u0026#34;, \u0026#34;attachments\u0026#34;: [{ \u0026#34;color\u0026#34;: color, \u0026#34;blocks\u0026#34;: [ {\u0026#34;type\u0026#34;: \u0026#34;header\u0026#34;, \u0026#34;text\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;plain_text\u0026#34;, \u0026#34;text\u0026#34;: f\u0026#34;🚨 GuardDuty Finding: {title}\u0026#34;}}, {\u0026#34;type\u0026#34;: \u0026#34;section\u0026#34;, \u0026#34;fields\u0026#34;: [ {\u0026#34;type\u0026#34;: \u0026#34;mrkdwn\u0026#34;, \u0026#34;text\u0026#34;: f\u0026#34;*Severity:*\\n{sev}\u0026#34;}, {\u0026#34;type\u0026#34;: \u0026#34;mrkdwn\u0026#34;, \u0026#34;text\u0026#34;: f\u0026#34;*Region:*\\n{region}\u0026#34;} ]}, {\u0026#34;type\u0026#34;: \u0026#34;section\u0026#34;, \u0026#34;text\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;mrkdwn\u0026#34;, \u0026#34;text\u0026#34;: f\u0026#34;*Description:*\\n{description}\u0026#34;}}, {\u0026#34;type\u0026#34;: \u0026#34;divider\u0026#34;}, {\u0026#34;type\u0026#34;: \u0026#34;context\u0026#34;, \u0026#34;elements\u0026#34;: [ {\u0026#34;type\u0026#34;: \u0026#34;mrkdwn\u0026#34;, \u0026#34;text\u0026#34;: f\u0026#34;*Account:* `{account_id}`\u0026#34;}, {\u0026#34;type\u0026#34;: \u0026#34;mrkdwn\u0026#34;, \u0026#34;text\u0026#34;: f\u0026#34;*Type:* `{finding_type}`\u0026#34;} ]} ] }] } try: req = urllib.request.Request( SLACK_WEBHOOK_URL, data=json.dumps(payload).encode(\u0026#34;utf-8\u0026#34;), headers={\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;} ) with urllib.request.urlopen(req) as response: logger.info(\u0026#34;Slack response: \u0026#34; + response.read().decode(\u0026#34;utf-8\u0026#34;)) except Exception as e: logger.error(f\u0026#34;SLACK FAILED: {e}\u0026#34;) # ==================================================================== # SEND TO SES EMAIL (UPDATED FOR MULTIPLE RECIPIENTS) # ==================================================================== def send_to_ses(finding): if not SENDER_EMAIL or not RECIPIENT_EMAIL: logger.warning(\u0026#34;SES Env vars missing. Skipping Email.\u0026#34;) return logger.info(\u0026#34;Formatting message for SES Email...\u0026#34;) recipient_list = [email.strip() for email in RECIPIENT_EMAIL.split(\u0026#39;,\u0026#39;)] severity_num = finding.get(\u0026#34;severity\u0026#34;, 0) title = finding.get(\u0026#34;title\u0026#34;, \u0026#34;No Title\u0026#34;) description = finding.get(\u0026#34;description\u0026#34;, \u0026#34;No Description\u0026#34;) region = finding.get(\u0026#34;region\u0026#34;, \u0026#34;N/A\u0026#34;) account_id = finding.get(\u0026#34;accountId\u0026#34;, \u0026#34;N/A\u0026#34;) finding_type = finding.get(\u0026#34;type\u0026#34;, \u0026#34;N/A\u0026#34;) finding_id = finding.get(\u0026#34;id\u0026#34;, \u0026#34;N/A\u0026#34;) if severity_num \u0026gt;= 7: color = \u0026#34;#ff0000\u0026#34; sev = \u0026#34;HIGH\u0026#34; elif severity_num \u0026gt;= 4: color = \u0026#34;#ffa500\u0026#34; sev = \u0026#34;MEDIUM\u0026#34; else: color = \u0026#34;#007bff\u0026#34; sev = \u0026#34;LOW\u0026#34; html_body = f\u0026#34;\u0026#34;\u0026#34; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;style\u0026gt; body {{ font-family: Arial, sans-serif; line-height: 1.6; color: #333; }} .container {{ width: 100%; max-width: 600px; margin: 0 auto; border: 1px solid #ddd; border-radius: 8px; overflow: hidden; }} .header {{ background-color: {color}; color: white; padding: 15px; text-align: center; }} .content {{ padding: 20px; }} .footer {{ background-color: #f4f4f4; padding: 10px; text-align: center; font-size: 12px; color: #666; }} .label {{ font-weight: bold; color: #555; }} \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;header\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;🚨 GuardDuty Alert: {sev}\u0026lt;/h2\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;content\u0026#34;\u0026gt; \u0026lt;h3\u0026gt;{title}\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;{description}\u0026lt;/p\u0026gt; \u0026lt;hr\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span class=\u0026#34;label\u0026#34;\u0026gt;Account ID:\u0026lt;/span\u0026gt; {account_id}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span class=\u0026#34;label\u0026#34;\u0026gt;Region:\u0026lt;/span\u0026gt; {region}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span class=\u0026#34;label\u0026#34;\u0026gt;Type:\u0026lt;/span\u0026gt; {finding_type}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span class=\u0026#34;label\u0026#34;\u0026gt;Finding ID:\u0026lt;/span\u0026gt; {finding_id}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;footer\u0026#34;\u0026gt; Generated by AWS Lambda Alert Dispatch \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; \u0026#34;\u0026#34;\u0026#34; try: response = ses_client.send_email( Source=SENDER_EMAIL, Destination={\u0026#39;ToAddresses\u0026#39;: recipient_list}, # Uses the list now Message={ \u0026#39;Subject\u0026#39;: {\u0026#39;Data\u0026#39;: f\u0026#34;GuardDuty Alert [{sev}]: {title}\u0026#34;, \u0026#39;Charset\u0026#39;: \u0026#39;UTF-8\u0026#39;}, \u0026#39;Body\u0026#39;: {\u0026#39;Html\u0026#39;: {\u0026#39;Data\u0026#39;: html_body, \u0026#39;Charset\u0026#39;: \u0026#39;UTF-8\u0026#39;}} } ) logger.info(f\u0026#34;SES Email sent to {len(recipient_list)} recipients! MessageId: {response[\u0026#39;MessageId\u0026#39;]}\u0026#34;) except ClientError as e: logger.error(f\u0026#34;SES FAILED: {e.response[\u0026#39;Error\u0026#39;][\u0026#39;Message\u0026#39;]}\u0026#34;) # ==================================================================== # MAIN HANDLER # ==================================================================== def lambda_handler(event, context): logger.info(f\u0026#34;Event received: {json.dumps(event)}\u0026#34;) try: sns_message_raw = event[\u0026#34;Records\u0026#34;][0][\u0026#34;Sns\u0026#34;][\u0026#34;Message\u0026#34;] message_data = json.loads(sns_message_raw) # Normalization Logic finding = {} if \u0026#34;detail-type\u0026#34; in message_data and message_data[\u0026#34;detail-type\u0026#34;] == \u0026#34;GuardDuty Finding\u0026#34;: detail = message_data[\u0026#34;detail\u0026#34;] finding = { \u0026#34;severity\u0026#34;: detail.get(\u0026#34;severity\u0026#34;, 0), \u0026#34;title\u0026#34;: detail.get(\u0026#34;title\u0026#34;, \u0026#34;GuardDuty Finding\u0026#34;), \u0026#34;description\u0026#34;: detail.get(\u0026#34;description\u0026#34;, \u0026#34;No description provided\u0026#34;), \u0026#34;accountId\u0026#34;: detail.get(\u0026#34;accountId\u0026#34;, \u0026#34;N/A\u0026#34;), \u0026#34;region\u0026#34;: detail.get(\u0026#34;region\u0026#34;, \u0026#34;N/A\u0026#34;), \u0026#34;type\u0026#34;: detail.get(\u0026#34;type\u0026#34;, \u0026#34;N/A\u0026#34;), \u0026#34;id\u0026#34;: detail.get(\u0026#34;id\u0026#34;, \u0026#34;N/A\u0026#34;) } elif \u0026#34;AlarmName\u0026#34; in message_data: state = message_data.get(\u0026#34;NewStateValue\u0026#34;) severity = 8 if state == \u0026#34;ALARM\u0026#34; else 0 finding = { \u0026#34;severity\u0026#34;: severity, \u0026#34;title\u0026#34;: f\u0026#34;CloudWatch Alarm: {message_data.get(\u0026#39;AlarmName\u0026#39;)}\u0026#34;, \u0026#34;description\u0026#34;: message_data.get(\u0026#34;NewStateReason\u0026#34;, \u0026#34;State change detected\u0026#34;), \u0026#34;accountId\u0026#34;: message_data.get(\u0026#34;AWSAccountId\u0026#34;, \u0026#34;N/A\u0026#34;), \u0026#34;region\u0026#34;: message_data.get(\u0026#34;Region\u0026#34;, \u0026#34;N/A\u0026#34;), \u0026#34;type\u0026#34;: \u0026#34;CloudWatch Alarm\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;N/A\u0026#34; } else: finding = { \u0026#34;severity\u0026#34;: 0, \u0026#34;title\u0026#34;: \u0026#34;Unknown Alert\u0026#34;, \u0026#34;description\u0026#34;: f\u0026#34;Raw Payload: {json.dumps(message_data)}\u0026#34;, \u0026#34;accountId\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Unknown\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;N/A\u0026#34; } except Exception as e: logger.error(f\u0026#34;FATAL: Could not parse incoming SNS event: {e}\u0026#34;) return {\u0026#34;statusCode\u0026#34;: 500} # --- Send Telegram --- # if BOT_TOKEN and CHAT_ID: # send_to_telegram(finding, CHAT_ID, MESSAGE_THREAD_ID) # --- Send Slack --- if SLACK_WEBHOOK_URL: send_to_slack(finding) # --- Send SES Email --- if SENDER_EMAIL and RECIPIENT_EMAIL: send_to_ses(finding) return {\u0026#34;statusCode\u0026#34;: 200, \u0026#34;body\u0026#34;: \u0026#34;Dispatch complete\u0026#34;} "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.9-use-cdk/","title":"Use CDK","tags":[],"description":"","content":"Overview We have provided CDK stack to create all of the infrastructure required for this workshop.\nTo get the files please go to this Github Link and clone or download all the files to a folder\nSetup Guide Before deploying the CDK stack, you must configure your local environment to authenticate with your AWS account using the AWS Command Line Interface (CLI).\nInstall the AWS CLI.\nObtain Credentials: You need an Access Key ID and a Secret Access Key from an IAM user with deployment permissions.\nRun the Configuration Command: Open your terminal and execute aws configure.\n$ aws configure When prompted, enter your credentials and desired settings. The Default region name should match the region where you plan to deploy the stack (e.g., ap-southeast-1):\nPrompt Example Value AWS Access Key ID AKIA... AWS Secret Access Key wJalr... Default region name ap-southeast-1 Default output format json Verify Configuration: Test your setup by fetching your user identity. A successful output confirms you are authenticated.\n$ aws sts get-caller-identity Prerequisites Ensure the following tools and services are installed and configured on your system:\nPython 3.8+ and pip: Required for executing the CDK application and building Lambda function assets. Node.js and npm: Required for running the AWS CDK CLI and building the React dashboard. AWS CDK Toolkit: Install the CDK CLI globally: $ npm install -g aws-cdk Set Up Python Environment The infrastructure definition is written in Python. A dedicated virtual environment is used to manage project dependencies.\nCreate the Virtual Environment:\n$ python -m venv .venv Activate the Virtual Environment:\nOperating System Command macOS / Linux source .venv/bin/activate Windows (Command Prompt) .venv\\Scripts\\activate.bat Windows (PowerShell) .venv\\Scripts\\Activate.ps1 Install Python Dependencies:\n$ pip install -r requirements.txt Step to build the dashboard In the project folder location, check inside the react folder. If the dist folder already exists, you do not need to build. Otherwise, please follow the steps below. If you are on cmd use this command to move to react folder:\n$ cd react And use this command to list all content in react:\n$ ls Prerequisites Ensure you have Node.js and npm installed. You can check the current version by running:\n$ npm --version If the command is not recognized, please download and install Node.js from nodejs.org\nInstall dependencies Run the following command to install all necessary libraries:\n$ npm install Build the Project After the installation is complete, run the build command:\n$ npm run build Upon completion, a dist folder will be generated containing index.html and the assets folder.\nConfigure Deployment Context The stack utilizes context variables. These variables are read from cdk.context.json or provided via command-line flags.\nVariable Name Description Required if functionality is desired Default Value (in cdk.context.json) vpc_ids A list of VPC IDs for Flow Logs and DNS Query Logging. Yes [] alert_email A list of email addresses for alert notifications (requires SES). Yes [] sender_email The verified SES sender email address. Yes (if alert_email is set) \u0026quot;\u0026quot; slack_webhook_url The Slack webhook URL for sending alerts. No \u0026quot;\u0026quot; Example\n{ \u0026#34;vpc_ids\u0026#34;: [ \u0026#34;vpc-a1b2c3d4e5f6g7h8i\u0026#34; ], \u0026#34;alert_email\u0026#34;: [ \u0026#34;admin@example.com\u0026#34; ], \u0026#34;sender_email\u0026#34;: \u0026#34;alerts@your-domain.com\u0026#34;, \u0026#34;slack_webhook_url\u0026#34;: \u0026#34;\u0026#34; } Deploy the Stacks Before processing further, if inside the /react folder, enter this command to go back to the main folder:\n$ cd.. CDK Bootstrapping: If you have not used the AWS CDK in your target AWS account and region previously, run the bootstrap command once to provision necessary resources (e.g., S3 deployment bucket).\n$ cdk bootstrap (Optional) Synthesize and Diff: Review the proposed CloudFormation changes before deployment:\n$ cdk synth --all $ cdk diff --all Execute Deployment: Run the deployment command and approve any requested IAM security changes when prompted.\n$ cdk deploy --all The deployment is complete when the CDK CLI reports success for the stack: AwsIncidentResponseAutomationCdkStack and DashboardCdkStack\nIMPORTANT NOTE: After the deployment is complete, you should verify the email in SES. Create a user in Cognito to be able to log in to the Dashboard. Access the Security Group and remove the default outbound rule from the QuarantineSecurityGroup "},{"uri":"https://beforelights.github.io/AWS-Worklog/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Harden Step Functions workflows with callbacks, error handling, and parallel states. Establish a multi-channel Incident Response alerting system (Telegram, Email). Structure the Incident Response project in Jira with clear Epics and ownership. Optimize alerting costs by migrating from SNS Email JSON to Lambda-SES. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Extended and hardened the Step Functions workflow with activation, pause/resume via callbacks, robust error handling, and parallelized checks.\n+ Updated the SubmitApplication Lambda to start state machine executions automatically with the right payload, turning manual tests into an event-driven workflow entry point.\n+ Implemented a callback-based Pending Review step using task tokens so flagged applications pause in Step Functions until a separate review Lambda resumes them with an approve/reject decision.\n+ Added Retry and Catch blocks to critical Task states so transient Lambda/API failures are retried with backoff while unrecoverable errors flow into explicit error-handling paths.\n+ Converted sequential checks into a Parallel state so name and address validations run concurrently, then adjusted the Choice logic to interpret the combined parallel output correctly.\n+ Wrapped up by exploring advanced Step Functions resources (long-running workflows, Map/Wait states, and official samples) to plan next steps for production-ready orchestration patterns. 10/11/2025 10/11/2025 StartExecution – AWS Step Functions\nCallback patterns with task tokens\nHandling errors in Step Functions workflows\nParallel workflow state\nTutorials and workshops for learning Step Functions 3 Built the foundational SNS alert dispatch system: Email/EmailJSON for inbox delivery, custom Lambda code for Telegram with bot setup and topic-based group chat, and generated GuardDuty test findings.\n+ Created an SNS topic (SNSToTelegram) and subscribed Email and EmailJSON directly so security alerts automatically route to your email inbox without any Lambda processing.\n+ Set up a Telegram bot via BotFather, retrieved the bot token, created a group chat, enabled Topics feature, and extracted the chat ID and message thread ID for targeted alert routing.\n+ Deployed a Telegram Lambda function with the bot token, chat ID, and message thread ID stored as environment variables, then coded the function to parse SNS messages and forward them to the Telegram topic.\n+ Configured the SNS topic to trigger the Telegram Lambda so incoming security events automatically send notifications to both your email and the Telegram group chat topic simultaneously.\n+ Generated AWS GuardDuty sample findings (more than 1000 emails\u0026hellip; This was a mistake). 11/11/2025 11/11/2025 4 Structured the AWS Incident Response System project in Jira by defining core epics, assigning ownership across multiple teams, and establishing the foundation for incident handling workflows.\n+ Created 5 key epics in Jira to map the incident response workflow: Threat Detection, Alerting, Response Workflow, Data Pipeline, and Dashboard components.\n+ Assigned epic ownership to individual team members so each area has a clear lead responsible for its scope and delivery.\n+ Organized team structure and responsibilities by aligning each epic with the appropriate skill set (threat detection, alerting infrastructure, workflow orchestration, data ingestion, dashboarding).\n+ Configured epic-to-task rolldown in Jira so that sub-tasks and issues are tied to their parent epics, enabling team visibility and burndown tracking across the incident response system. 12/11/2025 12/11/2025 5 Optimized the alert system by replacing SNS Email subscription with Lambda→SES to reduce costs, enable custom email formatting, and leverage SES\u0026rsquo;s higher sending limits.\n+ Analyzed SNS pricing impact from GuardDuty sample findings and identified that direct Email subscriptions were generating unexpected charges on SNS delivery costs.\n+ Researched AWS SES capabilities and discovered it offers higher sending limits and full control over email formatting compared to SNS Email subscriptions.\n+ Created a new Lambda function to replace the SNS Email subscription that receives SNS messages, parses GuardDuty findings, and sends formatted HTML emails via SES.\n+ Implemented custom email templates in the SES Lambda with styled HTML, embedded alert metadata, severity indicators, and GuardDuty finding details for better readability and actionability.\n+ Updated the SNS topic configuration to trigger both the Telegram Lambda and the new SES Lambda, eliminating the Email subscription and reducing SNS charges while maintaining dual-channel (Email + Telegram) alert delivery. 13/11/2025 13/11/2025 6 - Freelance work 14/11/2025 14/11/2025 Week 10 Achievements: Delivered a robust Step Functions state machine with callback patterns, parallel processing, and error resilience. Architected and implemented a cost-effective, dual-channel (Telegram + SES Email) alert dispatch system. Established a structured Project Management framework in Jira, aligning team roles with technical Epics. Successfully mitigated unexpected SNS costs by refactoring the email delivery mechanism to use AWS SES. "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.10-cleanup/","title":"Cleanup","tags":[],"description":"","content":"Congratulations on completing this workshop! In this workshop, you have created an Automated Incident Response and Forensics System and familiarized with Lambda, Step Functions, EventBridge, Glue, Athena, CloudFront, Cognito, S3 Buckets\nCleanup Guide: Cleanup Guide for Manual Infrastructure Setup Clean Guide for CDK Setup "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.10-cleanup/5.10.1-manual-cleanup/","title":"Manual Cleanup","tags":[],"description":"","content":"Clean up (Manual Infrastructure Setup) Phase 1: Automation and Monitoring Cleanup The goal here is to stop all active processes and delete the monitoring and core automation resources (EventBridge, Step Functions, SNS, GuardDuty, Flow Logs, CloudTrail).\n1. Delete Incident Response Automation 1.1 Delete EventBridge Rule\nGo to EventBridge Console → Rules. Select the rule: IncidentResponseAlert. Click \u0026ldquo;Delete\u0026rdquo;. 1.2 Delete Step Functions State Machine\nGo to Step Functions Console → State Machines. Select the State Machine: IncidentResponseStepFunctions. Click \u0026ldquo;Delete\u0026rdquo;. 1.3 Delete SNS Topic and Subscription\nGo to SNS Console → Topics → IncidentResponseAlerts. First, delete the subscription associated with ir-alert-dispatch. Then, delete the topic itself by clicking \u0026ldquo;Delete topic\u0026rdquo;. 1.4 Delete GuardDuty Detector\nGo to GuardDuty Console → Settings → General. Click \u0026ldquo;Suspend\u0026rdquo; to stop processing, then click \u0026ldquo;Disable GuardDuty\u0026rdquo; (or \u0026ldquo;Delete detector\u0026rdquo;). 1.5 Disable VPC Flow Logs\nGo to VPC Console → VPC Flow Logs. Select the flow log created (associated with YOUR_VPC_ID). Click \u0026ldquo;Delete flow log\u0026rdquo;. 1.6 Delete CloudTrail Trail\nGo to CloudTrail Console → Trails. Select the trail: incident-responses-cloudtrail-ACCOUNT_ID-REGION. Click \u0026ldquo;Delete\u0026rdquo;. Phase 2: Lambda and Compute Cleanup 2. Delete All Lambda Functions (9 Functions) Go to the Lambda Console and delete the following functions:\nincident-response-cloudtrail-etl incident-response-guardduty-etl cloudwatch-etl-lambda cloudwatch-eni-etl-lambda cloudwatch-export-lambda ir-parse-findings-lambda ir-isolate-ec2-lambda ir-quarantine-iam-lambda ir-alert-dispatch 3. Delete Isolation Security Group Go to EC2 Console → Security Groups. Find and select the Security Group: IR-Isolation-SG (using ID sg-XXXXXXX). Click \u0026ldquo;Delete security group\u0026rdquo;. 4. Delete CloudWatch Log Groups Go to the CloudWatch Console → Log Groups and delete:\nThe centralized log group: /aws/incident-response/centralized-logs. Any associated Lambda log groups for the 9 deleted functions (e.g., /aws/lambda/ir-parse-findings-lambda). Phase 3: Processing and Data Lake Cleanup 5. Delete Kinesis Data Firehose Streams Go to the Kinesis Console → Delivery Streams and delete:\ncloudtrail-firehose-stream vpc-dns-firehose-stream vpc-flow-firehose-stream 6. Delete AWS Glue Tables and Database 6.1 Delete Glue Tables\nGo to Glue Console → Tables. Select and delete: security_logs.processed_cloudtrail, security_logs.processed_guardduty, security_logs.vpc_logs, and security_logs.eni_flow_logs. 6.2 Delete Glue Database\nGo to Glue Console → Databases. Select the database: security_logs and click \u0026ldquo;Delete\u0026rdquo;. 7. Delete IAM Roles and Policies 7.1 Delete IAM Policies\nGo to IAM Console → Policies. Delete the custom managed policy: IrQuarantineIAMPolicy. Note: Inline policies created in the setup will be deleted automatically when the corresponding role is deleted. 7.2 Delete IAM Roles\nGo to IAM Console → Roles. Delete the following 17 roles: Lambda Execution Roles: CloudTrailETLLambdaServiceRole, GuardDutyETLLambdaServiceRole, CloudWatchETLLambdaServiceRole, CloudWatchENIETLLambdaServiceRole, CloudWatchExportLambdaServiceRole, ParseFindingsLambdaServiceRole, IsolateEC2LambdaServiceRole, QuarantineIAMLambdaServiceRole, AlertDispatchLambdaServiceRole. Service Roles: CloudTrailFirehoseRole, CloudWatchFirehoseRole, StepFunctionsRole, IncidentResponseStepFunctionsEventRole, FlowLogsIAMRole, GlueCloudWatchRole. Phase 4: S3 Bucket Cleanup (Data Deletion) 8. Empty and Delete S3 Buckets This is the final step to ensure all storage charges are stopped.\nBucket Name Purpose incident-response-log-list-bucket-ACCOUNT_ID-REGION Primary Log Source (CloudTrail/GuardDuty/Exported CW) processed-cloudtrail-logs-ACCOUNT_ID-REGION Firehose Destination for CloudTrail logs processed-cloudwatch-logs-ACCOUNT_ID-REGION Firehose Destination for VPC DNS/Flow logs processed-guardduty-findings-ACCOUNT_ID-REGION ETL Destination for GuardDuty logs athena-query-results-ACCOUNT_ID-REGION Athena Query Results Storage Go to the S3 Console. For each of the 5 buckets: Click on the bucket name. Go to the \u0026ldquo;Objects\u0026rdquo; tab. Click \u0026ldquo;Empty\u0026rdquo; to clear all data. You must confirm the permanent delete by typing permanently delete. Go back to the S3 bucket list, select the bucket, and click \u0026ldquo;Delete\u0026rdquo;. "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.11-appendices/5.11.10-step-functions-state-machine-definition/","title":"Steps Functions Definition ASL Code","tags":[],"description":"","content":" { \u0026#34;Comment\u0026#34;: \u0026#34;Guardduty Incident Response Automation\u0026#34;, \u0026#34;StartAt\u0026#34;: \u0026#34;CheckFindingType\u0026#34;, \u0026#34;States\u0026#34;: { \u0026#34;CheckFindingType\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Choice\u0026#34;, \u0026#34;Choices\u0026#34;: [ { \u0026#34;Comment\u0026#34;: \u0026#34;Check if EC2\u0026#34;, \u0026#34;Variable\u0026#34;: \u0026#34;$.detail.resource.resourceType\u0026#34;, \u0026#34;StringEquals\u0026#34;: \u0026#34;Instance\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;ParseFindings\u0026#34; }, { \u0026#34;Comment\u0026#34;: \u0026#34;Check if IAM\u0026#34;, \u0026#34;Variable\u0026#34;: \u0026#34;$.detail.resource.resourceType\u0026#34;, \u0026#34;StringEquals\u0026#34;: \u0026#34;AccessKey\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;Quarantine_IAM_User\u0026#34; } ], \u0026#34;Default\u0026#34;: \u0026#34;NoActionNeeded\u0026#34; }, \u0026#34;ParseFindings\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::lambda:invoke\u0026#34;, \u0026#34;OutputPath\u0026#34;: \u0026#34;$.Payload\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;Payload.$\u0026#34;: \u0026#34;$\u0026#34;, \u0026#34;FunctionName\u0026#34;: \u0026#34;arn:aws:lambda:ap-southeast-1:831981618496:function:ir-parse-findings-lambda\u0026#34; }, \u0026#34;Retry\u0026#34;: [ { \u0026#34;ErrorEquals\u0026#34;: [ \u0026#34;Lambda.ServiceException\u0026#34;, \u0026#34;Lambda.AWSLambdaException\u0026#34;, \u0026#34;Lambda.SdkClientException\u0026#34;, \u0026#34;Lambda.TooManyRequestsException\u0026#34; ], \u0026#34;IntervalSeconds\u0026#34;: 1, \u0026#34;MaxAttempts\u0026#34;: 3, \u0026#34;BackoffRate\u0026#34;: 2, \u0026#34;JitterStrategy\u0026#34;: \u0026#34;FULL\u0026#34; } ], \u0026#34;Next\u0026#34;: \u0026#34;Isolate_EC2_Instance\u0026#34; }, \u0026#34;Isolate_EC2_Instance\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::lambda:invoke\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;FunctionName\u0026#34;: \u0026#34;arn:aws:lambda:ap-southeast-1:831981618496:function:ir-isolate-ec2-lambda\u0026#34;, \u0026#34;Payload\u0026#34;: { \u0026#34;InstanceId.$\u0026#34;: \u0026#34;$.InstanceIds[0]\u0026#34;, \u0026#34;Region.$\u0026#34;: \u0026#34;$.Region\u0026#34; } }, \u0026#34;Retry\u0026#34;: [ { \u0026#34;ErrorEquals\u0026#34;: [ \u0026#34;Lambda.TooManyRequestsException\u0026#34;, \u0026#34;Lambda.ServiceException\u0026#34;, \u0026#34;Lambda.AWSLambdaException\u0026#34;, \u0026#34;Lambda.SdkClientException\u0026#34; ], \u0026#34;IntervalSeconds\u0026#34;: 2, \u0026#34;MaxAttempts\u0026#34;: 3, \u0026#34;BackoffRate\u0026#34;: 2 } ], \u0026#34;Next\u0026#34;: \u0026#34;CheckIsolationStatus\u0026#34;, \u0026#34;OutputPath\u0026#34;: \u0026#34;$.Payload\u0026#34; }, \u0026#34;CheckIsolationStatus\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Choice\u0026#34;, \u0026#34;Choices\u0026#34;: [ { \u0026#34;Variable\u0026#34;: \u0026#34;$.IsolationSG\u0026#34;, \u0026#34;IsNull\u0026#34;: true, \u0026#34;Next\u0026#34;: \u0026#34;AlreadyIsolated\u0026#34; } ], \u0026#34;Default\u0026#34;: \u0026#34;EnableTerminationProtection\u0026#34; }, \u0026#34;AlreadyIsolated\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Succeed\u0026#34; }, \u0026#34;EnableTerminationProtection\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::aws-sdk:ec2:modifyInstanceAttribute\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;InstanceId.$\u0026#34;: \u0026#34;$.InstanceId\u0026#34;, \u0026#34;DisableApiTermination\u0026#34;: { \u0026#34;Value\u0026#34;: true } }, \u0026#34;Next\u0026#34;: \u0026#34;CreateQuarantineTag\u0026#34;, \u0026#34;ResultPath\u0026#34;: null }, \u0026#34;CreateQuarantineTag\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::aws-sdk:ec2:createTags\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;Resources.$\u0026#34;: \u0026#34;States.Array($.InstanceId)\u0026#34;, \u0026#34;Tags\u0026#34;: [ { \u0026#34;Key\u0026#34;: \u0026#34;Quarantine\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;True\u0026#34; }, { \u0026#34;Key\u0026#34;: \u0026#34;Security Group\u0026#34;, \u0026#34;Value.$\u0026#34;: \u0026#34;$.IsolationSG\u0026#34; } ] }, \u0026#34;Next\u0026#34;: \u0026#34;DescribeInstanceASG\u0026#34;, \u0026#34;ResultPath\u0026#34;: null }, \u0026#34;DescribeInstanceASG\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::aws-sdk:autoscaling:describeAutoScalingInstances\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;InstanceIds.$\u0026#34;: \u0026#34;States.Array($.InstanceId)\u0026#34; }, \u0026#34;ResultPath\u0026#34;: \u0026#34;$.ASGInfo\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;CheckIfASGExists\u0026#34; }, \u0026#34;CheckIfASGExists\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Choice\u0026#34;, \u0026#34;Choices\u0026#34;: [ { \u0026#34;Variable\u0026#34;: \u0026#34;$.ASGInfo.AutoScalingInstances[0]\u0026#34;, \u0026#34;IsPresent\u0026#34;: true, \u0026#34;Next\u0026#34;: \u0026#34;UpdateASGConfiguration\u0026#34; } ], \u0026#34;Default\u0026#34;: \u0026#34;DescribeVolumes\u0026#34; }, \u0026#34;UpdateASGConfiguration\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::aws-sdk:autoscaling:updateAutoScalingGroup\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;AutoScalingGroupName.$\u0026#34;: \u0026#34;$.ASGInfo.AutoScalingInstances[0].AutoScalingGroupName\u0026#34;, \u0026#34;MinSize\u0026#34;: 0 }, \u0026#34;ResultPath\u0026#34;: null, \u0026#34;Next\u0026#34;: \u0026#34;Wait for ASG\u0026#34; }, \u0026#34;Wait for ASG\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Wait\u0026#34;, \u0026#34;Seconds\u0026#34;: 10, \u0026#34;Next\u0026#34;: \u0026#34;DetachFromASG\u0026#34; }, \u0026#34;DetachFromASG\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::aws-sdk:autoscaling:detachInstances\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;AutoScalingGroupName.$\u0026#34;: \u0026#34;$.ASGInfo.AutoScalingInstances[0].AutoScalingGroupName\u0026#34;, \u0026#34;InstanceIds.$\u0026#34;: \u0026#34;States.Array($.InstanceId)\u0026#34;, \u0026#34;ShouldDecrementDesiredCapacity\u0026#34;: false }, \u0026#34;Retry\u0026#34;: [ { \u0026#34;ErrorEquals\u0026#34;: [ \u0026#34;AutoScaling.ValidationException\u0026#34; ], \u0026#34;IntervalSeconds\u0026#34;: 15, \u0026#34;MaxAttempts\u0026#34;: 3, \u0026#34;BackoffRate\u0026#34;: 2 } ], \u0026#34;ResultPath\u0026#34;: null, \u0026#34;Next\u0026#34;: \u0026#34;DescribeVolumes\u0026#34; }, \u0026#34;DescribeVolumes\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::aws-sdk:ec2:describeVolumes\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;Filters\u0026#34;: [ { \u0026#34;Name\u0026#34;: \u0026#34;attachment.instance-id\u0026#34;, \u0026#34;Values.$\u0026#34;: \u0026#34;States.Array($.InstanceId)\u0026#34; } ] }, \u0026#34;ResultPath\u0026#34;: \u0026#34;$.VolumeInfo\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;CreateSnapshots\u0026#34; }, \u0026#34;CreateSnapshots\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Map\u0026#34;, \u0026#34;ItemsPath\u0026#34;: \u0026#34;$.VolumeInfo.Volumes\u0026#34;, \u0026#34;MaxConcurrency\u0026#34;: 1, \u0026#34;Iterator\u0026#34;: { \u0026#34;StartAt\u0026#34;: \u0026#34;Wait before calling CreateSnapshot API\u0026#34;, \u0026#34;States\u0026#34;: { \u0026#34;Wait before calling CreateSnapshot API\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Wait\u0026#34;, \u0026#34;Seconds\u0026#34;: 15, \u0026#34;Next\u0026#34;: \u0026#34;CreateSnapshot\u0026#34; }, \u0026#34;CreateSnapshot\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::aws-sdk:ec2:createSnapshot\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;VolumeId.$\u0026#34;: \u0026#34;$.VolumeId\u0026#34;, \u0026#34;Description.$\u0026#34;: \u0026#34;States.Format(\u0026#39;IR Snapshot for {} - {}\u0026#39;, $.Attachments[0].InstanceId, $.VolumeId)\u0026#34;, \u0026#34;TagSpecifications\u0026#34;: [ { \u0026#34;ResourceType\u0026#34;: \u0026#34;snapshot\u0026#34;, \u0026#34;Tags\u0026#34;: [ { \u0026#34;Key\u0026#34;: \u0026#34;Quarantine\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;True\u0026#34; } ] } ] }, \u0026#34;Retry\u0026#34;: [ { \u0026#34;ErrorEquals\u0026#34;: [ \u0026#34;Ec2.RequestLimitExceeded\u0026#34; ], \u0026#34;IntervalSeconds\u0026#34;: 60, \u0026#34;MaxAttempts\u0026#34;: 3, \u0026#34;BackoffRate\u0026#34;: 2 } ], \u0026#34;End\u0026#34;: true } } }, \u0026#34;End\u0026#34;: true }, \u0026#34;Quarantine_IAM_User\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Choice\u0026#34;, \u0026#34;Choices\u0026#34;: [ { \u0026#34;Variable\u0026#34;: \u0026#34;$.detail.resource.accessKeyDetails.userType\u0026#34;, \u0026#34;StringEquals\u0026#34;: \u0026#34;Root\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;RootUserDetected\u0026#34; } ], \u0026#34;Default\u0026#34;: \u0026#34;ExecuteIAMQuarantine\u0026#34; }, \u0026#34;RootUserDetected\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Succeed\u0026#34;, \u0026#34;Comment\u0026#34;: \u0026#34;Cannot quarantine root user\u0026#34; }, \u0026#34;ExecuteIAMQuarantine\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::lambda:invoke\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;FunctionName\u0026#34;: \u0026#34;arn:aws:lambda:ap-southeast-1:831981618496:function:ir-quarantine-iam-lambda\u0026#34;, \u0026#34;Payload.$\u0026#34;: \u0026#34;$\u0026#34; }, \u0026#34;Retry\u0026#34;: [ { \u0026#34;ErrorEquals\u0026#34;: [ \u0026#34;Lambda.TooManyRequestsException\u0026#34;, \u0026#34;Lambda.ServiceException\u0026#34;, \u0026#34;Lambda.AWSLambdaException\u0026#34;, \u0026#34;Lambda.SdkClientException\u0026#34; ], \u0026#34;IntervalSeconds\u0026#34;: 2, \u0026#34;MaxAttempts\u0026#34;: 3, \u0026#34;BackoffRate\u0026#34;: 2 } ], \u0026#34;End\u0026#34;: true }, \u0026#34;NoActionNeeded\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Succeed\u0026#34; } } } "},{"uri":"https://beforelights.github.io/AWS-Worklog/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Refactor Alerting Lambda functions into a unified, modular \u0026ldquo;Alert Dispatch\u0026rdquo; service. Integrate Slack webhook capabilities to expand the alerting notification channels. Centralize credential management using Lambda Environment Variables. Continue refining the Incident Response System project plan in Jira. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Attended the AWS Cloud Mastery Series #2 – DevOps on AWS 17/11/2025 17/11/2025 3 Consolidated the Telegram and SES Lambdas into one unified Alert Dispatch function with structured code, integrated a friend\u0026rsquo;s Slack implementation, and validated everything with GuardDuty.\n+ Merged the separate Telegram and SES Lambda functions into a single Alert Dispatch Lambda with clean, modular code structure and dedicated helper functions for each channel.\n+ Integrated Slack webhook delivery from my friend\u0026rsquo;s code—now all three channels (Telegram, SES, Slack) are handled by one function with shared parsing logic.\n+ Structured the Lambda code properly with clear separation: SNS parsing → alert enrichment → channel-specific formatters (Slack blocks, Telegram messages, SES HTML) → error handling.\n+ Set up environment variables to centrally manage all credentials (Slack webhook URLs, Telegram bot token/chat ID/thread ID, SES sender address)—way cleaner than hardcoding.\n+ Tested the complete multi-channel pipeline with GuardDuty sample findings—verified alerts arrive correctly formatted in Slack, Telegram, and email simultaneously, and everything works reliably. 18/11/2025 18/11/2025 4 Structured the AWS Incident Response System project in Jira by defining core epics, assigning ownership across multiple teams, and establishing the foundation for incident handling workflows.\n+ Created 5 key epics in Jira to map the incident response workflow: Threat Detection, Alerting, Response Workflow, Data Pipeline, and Dashboard components.\n+ Assigned epic ownership to individual team members so each area has a clear lead responsible for its scope and delivery.\n+ Organized team structure and responsibilities by aligning each epic with the appropriate skill set (threat detection, alerting infrastructure, workflow orchestration, data ingestion, dashboarding).\n+ Configured epic-to-task rolldown in Jira so that sub-tasks and issues are tied to their parent epics, enabling team visibility and burndown tracking across the incident response system. 19/11/2025 19/11/2025 5 Went back Bùi Thị Xuân High school for Teacher\u0026rsquo;s Day. 20/11/2025 20/11/2025 6 I was invited to the graduation ceremony of a Brosis in FPT University. 21/11/2025 21/11/2025 Week 11 Achievements: Significantly improved code maintainability by merging disparate Lambda functions into a single \u0026ldquo;Alert Dispatcher\u0026rdquo;. Expanded the incident alerting system to support Telegram, Slack, and SES Email simultaneously. Implemented secure configuration practices by moving hardcoded credentials to Environment Variables. Enforced project clarity by defining Epics and assigning granular responsibilities in Jira. "},{"uri":"https://beforelights.github.io/AWS-Worklog/5-workshop/5.11-appendices/","title":"Appendices","tags":[],"description":"","content":"Appendices Lambda Codes: CloudTrail ETL GuardDuty ETL CloudWatch ETL CloudWatch ENI ETL CloudWatch Auto Export Parse Findings Isolate EC2 Instance Quarantine IAM Alert Dispatch Step Functions ASL Code: Step Functions ASL Code "},{"uri":"https://beforelights.github.io/AWS-Worklog/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Design and optimize the UX for the S3 Incident Response Dashboard. Secure the public dashboard by implementing Cognito Authentication with React. Prevent unauthorized Athena query costs by implementing session-based access control. Explore alternative Linux setup (Arch) and WireGuard VPN networking. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Designed the S3 dashboard with Kiệt to define key metrics and UX improvements, then fixed bugs and optimized dashboard performance.\n+ Brainstormed S3 dashboard layout with Kiệt, alert summary cards, timeline, severity distribution, and real-time metrics for alert volume and delivery success.\n+ Identified UX issues: missing filters, unclear alert grouping, and missing detail.\n+ Fixed bugs: Query errors and UI bugs\n+ Optimized dashboard queries for faster aggregation, added real-time refresh, and improved visual hierarchy so critical findings stand out, tested with GuardDuty events. 24/11/2025 24/11/2025 3 Identified a critical security and cost issue: unauthenticated dashboard access causing unlimited Athena queries on page refresh, then explored authentication solutions.\n+ Discovered the S3 dashboard had no login page, so anyone could refresh and trigger expensive Athena queries without authorization.\n+ Attempted Cognito + Lambda@Edge integration to add authentication at the edge, but hit a blocker: my CloudFront distribution used flat pricing (not pay-as-you-go) and couldn\u0026rsquo;t be switched, making Lambda@Edge unusable.\n+ Tried CloudFront Functions as an alternative, combining it with Lambda to implement request filtering and basic auth without Lambda@Edge. 25/11/2025 25/11/2025 4 Debugged CloudFront/Lambda session issues, evaluated AWS Amplify but pivoted to React-based authentication to meet the deadline.\n+ Spent hours troubleshooting CloudFront Functions + Lambda setup, the following could be the reasons: sessions weren\u0026rsquo;t persisting and cross-domain access from S3 was blocked, no viable solution found.\n+ Explored AWS Amplify as an alternative for managed authentication, but realized the current dashboard structure wasn\u0026rsquo;t compatible without significant refactoring and time wasted.\n+ Decided to implement authentication directly in the React dashboard given the tight deadline.\n+ Integrated Cognito login directly into React components with token-based session management, protecting dashboard access and Athena query execution.\n+ Tested the React auth flow end-to-end users now must authenticate before viewing the dashboard, and only authorized sessions can trigger Athena queries, solving both security and cost issues. 26/11/2025 26/11/2025 5 This is a side project: Installed Arch Linux on a friend\u0026rsquo;s prebuilt PC and began WireGuard VPN setup, learning pacman package management along the way.\n+ Installed Arch Linux on a prebuilt system as an alternative to the default OS, now I can legitimately say \u0026ldquo;I use Arch btw\u0026rdquo;.\n+ Researched network security for the PC and identified WireGuard as a lightweight, modern VPN solution to secure group traffic.\n+ Discovered pacman (Arch\u0026rsquo;s package manager) works differently from apt, spent time hunting for WireGuard packages and understanding Arch\u0026rsquo;s repository structure (core, extra, community).\n+ Began configuring WireGuard on the Arch system but realized the setup complexity required more time than available in one day.\n+ Documented all package names and configuration steps for continuation the next day. 27/11/2025 27/11/2025 6 Completed WireGuard VPN installation and configuration on Arch Linux, resolving pacman dependency issues and testing peer connectivity.\n+ Finished WireGuard installation using pacman, resolving dependency conflicts and finding correct package versions for Arch.\n+ Generated WireGuard keys (public/private) and configured peer connections for the group\u0026rsquo;s high school project network.\n+ Set up network interfaces and IP routing on Arch with proper firewall rules to enable secure VPN access between group members.\n+ Tested WireGuard connectivity end-to-end, verified peers could connect and communicate through the encrypted tunnel reliably.\n+ Documented the complete setup process for the group so others could replicate the WireGuard configuration on their systems. 28/11/2025 28/11/2025 Week 12 Achievements: Overhauled the S3 Dashboard UX, resolving bugs and improving data visualization performance. Successfully implemented Client-Side Authentication using Cognito and React to secure sensitive metrics. Eliminated the risk of unlimited Athena billing spikes by enforcing session-based query execution. Configured a secure WireGuard VPN on Arch Linux, demonstrating versatility in system administration. "},{"uri":"https://beforelights.github.io/AWS-Worklog/1-worklog/1.13-week13/","title":"Week 13 Worklog","tags":[],"description":"","content":"Week 13 Objectives: Finalize project tracking and management on Jira. Format and structure all project documentation within Hugo. Translate workshop materials and the proposal document into Vietnamese. Deploy the static documentation site to GitHub Pages. Support team members with their GitHub Pages deployments and debug issues. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Finalized Jira board setup. Translated the Google Doc Proposal into Vietnamese and integrated it into the proposal section. 01/12/2025 01/12/2025 3 Started formatting the documentation and organizing the Hugo directory structure. 02/12/2025 02/12/2025 4 Continued formatting documentation, fixing markdown issues, and ensuring consistent styling. 03/12/2025 03/12/2025 5 Finished formatting documentation. Translated the entire Hugo workshop content into Vietnamese. 04/12/2025 04/12/2025 6 Deployed the Hugo site to GitHub Pages. Helped friends and team members deploy their sites to GitHub and debugged their build errors. 05/12/2025 05/12/2025 Week 13 Achievements: Successfully deployed the fully formatted and bilingual (English/Vietnamese) documentation site. Completed ensuring all project tracking was up to date on Jira. Assisted multiple team members in successfully deploying their worklogs and resolving bugs. Integrated the Vietnamese translation of the proposal document. "},{"uri":"https://beforelights.github.io/AWS-Worklog/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://beforelights.github.io/AWS-Worklog/tags/","title":"Tags","tags":[],"description":"","content":""}]